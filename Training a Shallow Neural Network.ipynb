{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Shallow Neural Network\n",
    "The following code implements a shallow neural network with backpropagation using low-level libraries and compares it with a model generated by Scikit-learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Loading & Cleaning\n",
    "The data set contains credit card debt information about 10,000 customers and whether they defaulted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income\n",
       "0      No      No   729.526495  44361.625074\n",
       "1      No     Yes   817.180407  12106.134700\n",
       "2      No      No  1073.549164  31767.138947\n",
       "3      No      No   529.250605  35704.493935\n",
       "4      No      No   785.655883  38463.495879"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('Default.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and converting to NumPy arrays\n",
    "df['default']=df['default'].apply(lambda x: 0 if x=='No' else 1)\n",
    "df['student']=df['student'].apply(lambda x: 0 if x=='No' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "0        0        0   729.526495  44361.625074\n",
       "1        0        1   817.180407  12106.134700\n",
       "2        0        0  1073.549164  31767.138947\n",
       "3        0        0   529.250605  35704.493935\n",
       "4        0        0   785.655883  38463.495879"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   default  10000 non-null  int64  \n",
      " 1   student  10000 non-null  int64  \n",
      " 2   balance  10000 non-null  float64\n",
      " 3   income   10000 non-null  float64\n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 312.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.218835</td>\n",
       "      <td>0.813187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.037616</td>\n",
       "      <td>-1.605496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>-0.131212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.632893</td>\n",
       "      <td>0.164031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>0.370915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.255990</td>\n",
       "      <td>1.460366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.160044</td>\n",
       "      <td>-1.039014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>1.883565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.516742</td>\n",
       "      <td>0.236363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.311691</td>\n",
       "      <td>-1.248805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      default  student   balance    income\n",
       "0           0        0 -0.218835  0.813187\n",
       "1           0        1 -0.037616 -1.605496\n",
       "2           0        0  0.492410 -0.131212\n",
       "3           0        0 -0.632893  0.164031\n",
       "4           0        0 -0.102791  0.370915\n",
       "...       ...      ...       ...       ...\n",
       "9995        0        0 -0.255990  1.460366\n",
       "9996        0        0 -0.160044 -1.039014\n",
       "9997        0        0  0.020751  1.883565\n",
       "9998        0        0  1.516742  0.236363\n",
       "9999        0        1 -1.311691 -1.248805\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['balance','income']] = scaler.fit_transform(df[['balance','income']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['default'].to_numpy().reshape(-1,1)\n",
    "X = df.drop(columns=['default']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (10000, 1)\n",
      "Shape of X: (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 10000)\n",
      "Shape of X: (3, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = X.T\n",
    "Y = Y.T\n",
    "\n",
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training a Shallow Neural Network Using Scikit-learn\n",
    "The following code trains a shallow neural network with 4 neurons in its hidden layer using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1118: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.24796562\n",
      "Iteration 2, loss = 1.23658263\n",
      "Iteration 3, loss = 1.22534232\n",
      "Iteration 4, loss = 1.21424296\n",
      "Iteration 5, loss = 1.20328285\n",
      "Iteration 6, loss = 1.19246025\n",
      "Iteration 7, loss = 1.18177350\n",
      "Iteration 8, loss = 1.17122089\n",
      "Iteration 9, loss = 1.16080077\n",
      "Iteration 10, loss = 1.15051149\n",
      "Iteration 11, loss = 1.14035140\n",
      "Iteration 12, loss = 1.13031889\n",
      "Iteration 13, loss = 1.12041235\n",
      "Iteration 14, loss = 1.11063018\n",
      "Iteration 15, loss = 1.10097080\n",
      "Iteration 16, loss = 1.09143265\n",
      "Iteration 17, loss = 1.08201418\n",
      "Iteration 18, loss = 1.07271385\n",
      "Iteration 19, loss = 1.06353014\n",
      "Iteration 20, loss = 1.05446156\n",
      "Iteration 21, loss = 1.04550660\n",
      "Iteration 22, loss = 1.03666380\n",
      "Iteration 23, loss = 1.02793169\n",
      "Iteration 24, loss = 1.01930883\n",
      "Iteration 25, loss = 1.01079379\n",
      "Iteration 26, loss = 1.00238515\n",
      "Iteration 27, loss = 0.99408151\n",
      "Iteration 28, loss = 0.98588150\n",
      "Iteration 29, loss = 0.97778373\n",
      "Iteration 30, loss = 0.96978686\n",
      "Iteration 31, loss = 0.96188954\n",
      "Iteration 32, loss = 0.95409045\n",
      "Iteration 33, loss = 0.94638828\n",
      "Iteration 34, loss = 0.93878173\n",
      "Iteration 35, loss = 0.93126952\n",
      "Iteration 36, loss = 0.92385038\n",
      "Iteration 37, loss = 0.91652307\n",
      "Iteration 38, loss = 0.90928634\n",
      "Iteration 39, loss = 0.90213898\n",
      "Iteration 40, loss = 0.89507977\n",
      "Iteration 41, loss = 0.88810753\n",
      "Iteration 42, loss = 0.88122107\n",
      "Iteration 43, loss = 0.87441922\n",
      "Iteration 44, loss = 0.86770084\n",
      "Iteration 45, loss = 0.86106479\n",
      "Iteration 46, loss = 0.85450994\n",
      "Iteration 47, loss = 0.84803519\n",
      "Iteration 48, loss = 0.84163944\n",
      "Iteration 49, loss = 0.83532161\n",
      "Iteration 50, loss = 0.82908063\n",
      "Iteration 51, loss = 0.82291544\n",
      "Iteration 52, loss = 0.81682502\n",
      "Iteration 53, loss = 0.81080831\n",
      "Iteration 54, loss = 0.80486432\n",
      "Iteration 55, loss = 0.79899204\n",
      "Iteration 56, loss = 0.79319048\n",
      "Iteration 57, loss = 0.78745867\n",
      "Iteration 58, loss = 0.78179564\n",
      "Iteration 59, loss = 0.77620045\n",
      "Iteration 60, loss = 0.77067215\n",
      "Iteration 61, loss = 0.76520981\n",
      "Iteration 62, loss = 0.75981254\n",
      "Iteration 63, loss = 0.75447942\n",
      "Iteration 64, loss = 0.74920957\n",
      "Iteration 65, loss = 0.74400211\n",
      "Iteration 66, loss = 0.73885617\n",
      "Iteration 67, loss = 0.73377091\n",
      "Iteration 68, loss = 0.72874548\n",
      "Iteration 69, loss = 0.72377905\n",
      "Iteration 70, loss = 0.71887081\n",
      "Iteration 71, loss = 0.71401994\n",
      "Iteration 72, loss = 0.70922565\n",
      "Iteration 73, loss = 0.70448716\n",
      "Iteration 74, loss = 0.69980369\n",
      "Iteration 75, loss = 0.69517448\n",
      "Iteration 76, loss = 0.69059878\n",
      "Iteration 77, loss = 0.68607584\n",
      "Iteration 78, loss = 0.68160493\n",
      "Iteration 79, loss = 0.67718533\n",
      "Iteration 80, loss = 0.67281634\n",
      "Iteration 81, loss = 0.66849725\n",
      "Iteration 82, loss = 0.66422737\n",
      "Iteration 83, loss = 0.66000602\n",
      "Iteration 84, loss = 0.65583252\n",
      "Iteration 85, loss = 0.65170622\n",
      "Iteration 86, loss = 0.64762647\n",
      "Iteration 87, loss = 0.64359262\n",
      "Iteration 88, loss = 0.63960404\n",
      "Iteration 89, loss = 0.63566011\n",
      "Iteration 90, loss = 0.63176021\n",
      "Iteration 91, loss = 0.62790373\n",
      "Iteration 92, loss = 0.62409008\n",
      "Iteration 93, loss = 0.62031866\n",
      "Iteration 94, loss = 0.61658891\n",
      "Iteration 95, loss = 0.61290024\n",
      "Iteration 96, loss = 0.60925210\n",
      "Iteration 97, loss = 0.60564393\n",
      "Iteration 98, loss = 0.60207518\n",
      "Iteration 99, loss = 0.59854531\n",
      "Iteration 100, loss = 0.59505380\n",
      "Iteration 101, loss = 0.59160012\n",
      "Iteration 102, loss = 0.58818375\n",
      "Iteration 103, loss = 0.58480420\n",
      "Iteration 104, loss = 0.58146095\n",
      "Iteration 105, loss = 0.57815352\n",
      "Iteration 106, loss = 0.57488142\n",
      "Iteration 107, loss = 0.57164417\n",
      "Iteration 108, loss = 0.56844130\n",
      "Iteration 109, loss = 0.56527235\n",
      "Iteration 110, loss = 0.56213687\n",
      "Iteration 111, loss = 0.55903439\n",
      "Iteration 112, loss = 0.55596448\n",
      "Iteration 113, loss = 0.55292670\n",
      "Iteration 114, loss = 0.54992062\n",
      "Iteration 115, loss = 0.54694581\n",
      "Iteration 116, loss = 0.54400187\n",
      "Iteration 117, loss = 0.54108837\n",
      "Iteration 118, loss = 0.53820491\n",
      "Iteration 119, loss = 0.53535109\n",
      "Iteration 120, loss = 0.53252652\n",
      "Iteration 121, loss = 0.52973082\n",
      "Iteration 122, loss = 0.52696359\n",
      "Iteration 123, loss = 0.52422447\n",
      "Iteration 124, loss = 0.52151308\n",
      "Iteration 125, loss = 0.51882906\n",
      "Iteration 126, loss = 0.51617205\n",
      "Iteration 127, loss = 0.51354170\n",
      "Iteration 128, loss = 0.51093765\n",
      "Iteration 129, loss = 0.50835957\n",
      "Iteration 130, loss = 0.50580711\n",
      "Iteration 131, loss = 0.50327994\n",
      "Iteration 132, loss = 0.50077774\n",
      "Iteration 133, loss = 0.49830018\n",
      "Iteration 134, loss = 0.49584694\n",
      "Iteration 135, loss = 0.49341771\n",
      "Iteration 136, loss = 0.49101218\n",
      "Iteration 137, loss = 0.48863004\n",
      "Iteration 138, loss = 0.48627100\n",
      "Iteration 139, loss = 0.48393475\n",
      "Iteration 140, loss = 0.48162102\n",
      "Iteration 141, loss = 0.47932950\n",
      "Iteration 142, loss = 0.47705992\n",
      "Iteration 143, loss = 0.47481200\n",
      "Iteration 144, loss = 0.47258547\n",
      "Iteration 145, loss = 0.47038005\n",
      "Iteration 146, loss = 0.46819549\n",
      "Iteration 147, loss = 0.46603150\n",
      "Iteration 148, loss = 0.46388785\n",
      "Iteration 149, loss = 0.46176427\n",
      "Iteration 150, loss = 0.45966052\n",
      "Iteration 151, loss = 0.45757634\n",
      "Iteration 152, loss = 0.45551149\n",
      "Iteration 153, loss = 0.45346573\n",
      "Iteration 154, loss = 0.45143884\n",
      "Iteration 155, loss = 0.44943056\n",
      "Iteration 156, loss = 0.44744068\n",
      "Iteration 157, loss = 0.44546896\n",
      "Iteration 158, loss = 0.44351519\n",
      "Iteration 159, loss = 0.44157915\n",
      "Iteration 160, loss = 0.43966061\n",
      "Iteration 161, loss = 0.43775937\n",
      "Iteration 162, loss = 0.43587522\n",
      "Iteration 163, loss = 0.43400794\n",
      "Iteration 164, loss = 0.43215733\n",
      "Iteration 165, loss = 0.43032320\n",
      "Iteration 166, loss = 0.42850535\n",
      "Iteration 167, loss = 0.42670357\n",
      "Iteration 168, loss = 0.42491768\n",
      "Iteration 169, loss = 0.42314749\n",
      "Iteration 170, loss = 0.42139281\n",
      "Iteration 171, loss = 0.41965345\n",
      "Iteration 172, loss = 0.41792924\n",
      "Iteration 173, loss = 0.41621999\n",
      "Iteration 174, loss = 0.41452553\n",
      "Iteration 175, loss = 0.41284568\n",
      "Iteration 176, loss = 0.41118028\n",
      "Iteration 177, loss = 0.40952915\n",
      "Iteration 178, loss = 0.40789213\n",
      "Iteration 179, loss = 0.40626905\n",
      "Iteration 180, loss = 0.40465975\n",
      "Iteration 181, loss = 0.40306407\n",
      "Iteration 182, loss = 0.40148186\n",
      "Iteration 183, loss = 0.39991295\n",
      "Iteration 184, loss = 0.39835720\n",
      "Iteration 185, loss = 0.39681445\n",
      "Iteration 186, loss = 0.39528456\n",
      "Iteration 187, loss = 0.39376738\n",
      "Iteration 188, loss = 0.39226276\n",
      "Iteration 189, loss = 0.39077057\n",
      "Iteration 190, loss = 0.38929065\n",
      "Iteration 191, loss = 0.38782288\n",
      "Iteration 192, loss = 0.38636711\n",
      "Iteration 193, loss = 0.38492321\n",
      "Iteration 194, loss = 0.38349106\n",
      "Iteration 195, loss = 0.38207050\n",
      "Iteration 196, loss = 0.38066143\n",
      "Iteration 197, loss = 0.37926370\n",
      "Iteration 198, loss = 0.37787720\n",
      "Iteration 199, loss = 0.37650180\n",
      "Iteration 200, loss = 0.37513737\n",
      "Iteration 201, loss = 0.37378381\n",
      "Iteration 202, loss = 0.37244098\n",
      "Iteration 203, loss = 0.37110877\n",
      "Iteration 204, loss = 0.36978706\n",
      "Iteration 205, loss = 0.36847574\n",
      "Iteration 206, loss = 0.36717470\n",
      "Iteration 207, loss = 0.36588383\n",
      "Iteration 208, loss = 0.36460301\n",
      "Iteration 209, loss = 0.36333214\n",
      "Iteration 210, loss = 0.36207111\n",
      "Iteration 211, loss = 0.36081982\n",
      "Iteration 212, loss = 0.35957816\n",
      "Iteration 213, loss = 0.35834603\n",
      "Iteration 214, loss = 0.35712332\n",
      "Iteration 215, loss = 0.35590994\n",
      "Iteration 216, loss = 0.35470580\n",
      "Iteration 217, loss = 0.35351078\n",
      "Iteration 218, loss = 0.35232480\n",
      "Iteration 219, loss = 0.35114776\n",
      "Iteration 220, loss = 0.34997957\n",
      "Iteration 221, loss = 0.34882014\n",
      "Iteration 222, loss = 0.34766937\n",
      "Iteration 223, loss = 0.34652718\n",
      "Iteration 224, loss = 0.34539347\n",
      "Iteration 225, loss = 0.34426816\n",
      "Iteration 226, loss = 0.34315116\n",
      "Iteration 227, loss = 0.34204239\n",
      "Iteration 228, loss = 0.34094176\n",
      "Iteration 229, loss = 0.33984919\n",
      "Iteration 230, loss = 0.33876460\n",
      "Iteration 231, loss = 0.33768791\n",
      "Iteration 232, loss = 0.33661903\n",
      "Iteration 233, loss = 0.33555789\n",
      "Iteration 234, loss = 0.33450441\n",
      "Iteration 235, loss = 0.33345851\n",
      "Iteration 236, loss = 0.33242012\n",
      "Iteration 237, loss = 0.33138916\n",
      "Iteration 238, loss = 0.33036556\n",
      "Iteration 239, loss = 0.32934925\n",
      "Iteration 240, loss = 0.32834015\n",
      "Iteration 241, loss = 0.32733819\n",
      "Iteration 242, loss = 0.32634330\n",
      "Iteration 243, loss = 0.32535541\n",
      "Iteration 244, loss = 0.32437446\n",
      "Iteration 245, loss = 0.32340037\n",
      "Iteration 246, loss = 0.32243308\n",
      "Iteration 247, loss = 0.32147252\n",
      "Iteration 248, loss = 0.32051862\n",
      "Iteration 249, loss = 0.31957133\n",
      "Iteration 250, loss = 0.31863058\n",
      "Iteration 251, loss = 0.31769631\n",
      "Iteration 252, loss = 0.31676845\n",
      "Iteration 253, loss = 0.31584694\n",
      "Iteration 254, loss = 0.31493172\n",
      "Iteration 255, loss = 0.31402273\n",
      "Iteration 256, loss = 0.31311992\n",
      "Iteration 257, loss = 0.31222322\n",
      "Iteration 258, loss = 0.31133258\n",
      "Iteration 259, loss = 0.31044793\n",
      "Iteration 260, loss = 0.30956923\n",
      "Iteration 261, loss = 0.30869642\n",
      "Iteration 262, loss = 0.30782944\n",
      "Iteration 263, loss = 0.30696823\n",
      "Iteration 264, loss = 0.30611275\n",
      "Iteration 265, loss = 0.30526294\n",
      "Iteration 266, loss = 0.30441875\n",
      "Iteration 267, loss = 0.30358013\n",
      "Iteration 268, loss = 0.30274702\n",
      "Iteration 269, loss = 0.30191938\n",
      "Iteration 270, loss = 0.30109715\n",
      "Iteration 271, loss = 0.30028029\n",
      "Iteration 272, loss = 0.29946874\n",
      "Iteration 273, loss = 0.29866246\n",
      "Iteration 274, loss = 0.29786141\n",
      "Iteration 275, loss = 0.29706552\n",
      "Iteration 276, loss = 0.29627476\n",
      "Iteration 277, loss = 0.29548909\n",
      "Iteration 278, loss = 0.29470845\n",
      "Iteration 279, loss = 0.29393280\n",
      "Iteration 280, loss = 0.29316209\n",
      "Iteration 281, loss = 0.29239629\n",
      "Iteration 282, loss = 0.29163534\n",
      "Iteration 283, loss = 0.29087921\n",
      "Iteration 284, loss = 0.29012785\n",
      "Iteration 285, loss = 0.28938122\n",
      "Iteration 286, loss = 0.28863928\n",
      "Iteration 287, loss = 0.28790199\n",
      "Iteration 288, loss = 0.28716931\n",
      "Iteration 289, loss = 0.28644119\n",
      "Iteration 290, loss = 0.28571760\n",
      "Iteration 291, loss = 0.28499850\n",
      "Iteration 292, loss = 0.28428385\n",
      "Iteration 293, loss = 0.28357361\n",
      "Iteration 294, loss = 0.28286774\n",
      "Iteration 295, loss = 0.28216621\n",
      "Iteration 296, loss = 0.28146897\n",
      "Iteration 297, loss = 0.28077600\n",
      "Iteration 298, loss = 0.28008725\n",
      "Iteration 299, loss = 0.27940269\n",
      "Iteration 300, loss = 0.27872229\n",
      "Iteration 301, loss = 0.27804600\n",
      "Iteration 302, loss = 0.27737380\n",
      "Iteration 303, loss = 0.27670565\n",
      "Iteration 304, loss = 0.27604151\n",
      "Iteration 305, loss = 0.27538135\n",
      "Iteration 306, loss = 0.27472515\n",
      "Iteration 307, loss = 0.27407286\n",
      "Iteration 308, loss = 0.27342445\n",
      "Iteration 309, loss = 0.27277989\n",
      "Iteration 310, loss = 0.27213915\n",
      "Iteration 311, loss = 0.27150220\n",
      "Iteration 312, loss = 0.27086900\n",
      "Iteration 313, loss = 0.27023953\n",
      "Iteration 314, loss = 0.26961376\n",
      "Iteration 315, loss = 0.26899165\n",
      "Iteration 316, loss = 0.26837317\n",
      "Iteration 317, loss = 0.26775830\n",
      "Iteration 318, loss = 0.26714701\n",
      "Iteration 319, loss = 0.26653926\n",
      "Iteration 320, loss = 0.26593503\n",
      "Iteration 321, loss = 0.26533429\n",
      "Iteration 322, loss = 0.26473702\n",
      "Iteration 323, loss = 0.26414317\n",
      "Iteration 324, loss = 0.26355274\n",
      "Iteration 325, loss = 0.26296568\n",
      "Iteration 326, loss = 0.26238198\n",
      "Iteration 327, loss = 0.26180160\n",
      "Iteration 328, loss = 0.26122452\n",
      "Iteration 329, loss = 0.26065071\n",
      "Iteration 330, loss = 0.26008015\n",
      "Iteration 331, loss = 0.25951282\n",
      "Iteration 332, loss = 0.25894868\n",
      "Iteration 333, loss = 0.25838771\n",
      "Iteration 334, loss = 0.25782989\n",
      "Iteration 335, loss = 0.25727520\n",
      "Iteration 336, loss = 0.25672360\n",
      "Iteration 337, loss = 0.25617507\n",
      "Iteration 338, loss = 0.25562959\n",
      "Iteration 339, loss = 0.25508715\n",
      "Iteration 340, loss = 0.25454770\n",
      "Iteration 341, loss = 0.25401124\n",
      "Iteration 342, loss = 0.25347773\n",
      "Iteration 343, loss = 0.25294716\n",
      "Iteration 344, loss = 0.25241950\n",
      "Iteration 345, loss = 0.25189472\n",
      "Iteration 346, loss = 0.25137282\n",
      "Iteration 347, loss = 0.25085377\n",
      "Iteration 348, loss = 0.25033754\n",
      "Iteration 349, loss = 0.24982411\n",
      "Iteration 350, loss = 0.24931346\n",
      "Iteration 351, loss = 0.24880558\n",
      "Iteration 352, loss = 0.24830044\n",
      "Iteration 353, loss = 0.24779802\n",
      "Iteration 354, loss = 0.24729829\n",
      "Iteration 355, loss = 0.24680125\n",
      "Iteration 356, loss = 0.24630687\n",
      "Iteration 357, loss = 0.24581513\n",
      "Iteration 358, loss = 0.24532600\n",
      "Iteration 359, loss = 0.24483948\n",
      "Iteration 360, loss = 0.24435555\n",
      "Iteration 361, loss = 0.24387417\n",
      "Iteration 362, loss = 0.24339534\n",
      "Iteration 363, loss = 0.24291903\n",
      "Iteration 364, loss = 0.24244524\n",
      "Iteration 365, loss = 0.24197393\n",
      "Iteration 366, loss = 0.24150509\n",
      "Iteration 367, loss = 0.24103870\n",
      "Iteration 368, loss = 0.24057475\n",
      "Iteration 369, loss = 0.24011322\n",
      "Iteration 370, loss = 0.23965409\n",
      "Iteration 371, loss = 0.23919734\n",
      "Iteration 372, loss = 0.23874295\n",
      "Iteration 373, loss = 0.23829092\n",
      "Iteration 374, loss = 0.23784121\n",
      "Iteration 375, loss = 0.23739383\n",
      "Iteration 376, loss = 0.23694874\n",
      "Iteration 377, loss = 0.23650593\n",
      "Iteration 378, loss = 0.23606539\n",
      "Iteration 379, loss = 0.23562710\n",
      "Iteration 380, loss = 0.23519104\n",
      "Iteration 381, loss = 0.23475720\n",
      "Iteration 382, loss = 0.23432557\n",
      "Iteration 383, loss = 0.23389612\n",
      "Iteration 384, loss = 0.23346885\n",
      "Iteration 385, loss = 0.23304373\n",
      "Iteration 386, loss = 0.23262076\n",
      "Iteration 387, loss = 0.23219991\n",
      "Iteration 388, loss = 0.23178117\n",
      "Iteration 389, loss = 0.23136454\n",
      "Iteration 390, loss = 0.23094998\n",
      "Iteration 391, loss = 0.23053750\n",
      "Iteration 392, loss = 0.23012706\n",
      "Iteration 393, loss = 0.22971867\n",
      "Iteration 394, loss = 0.22931231\n",
      "Iteration 395, loss = 0.22890796\n",
      "Iteration 396, loss = 0.22850560\n",
      "Iteration 397, loss = 0.22810524\n",
      "Iteration 398, loss = 0.22770684\n",
      "Iteration 399, loss = 0.22731040\n",
      "Iteration 400, loss = 0.22691591\n",
      "Iteration 401, loss = 0.22652335\n",
      "Iteration 402, loss = 0.22613271\n",
      "Iteration 403, loss = 0.22574397\n",
      "Iteration 404, loss = 0.22535713\n",
      "Iteration 405, loss = 0.22497216\n",
      "Iteration 406, loss = 0.22458907\n",
      "Iteration 407, loss = 0.22420783\n",
      "Iteration 408, loss = 0.22382843\n",
      "Iteration 409, loss = 0.22345087\n",
      "Iteration 410, loss = 0.22307512\n",
      "Iteration 411, loss = 0.22270118\n",
      "Iteration 412, loss = 0.22232903\n",
      "Iteration 413, loss = 0.22195867\n",
      "Iteration 414, loss = 0.22159007\n",
      "Iteration 415, loss = 0.22122324\n",
      "Iteration 416, loss = 0.22085815\n",
      "Iteration 417, loss = 0.22049480\n",
      "Iteration 418, loss = 0.22013318\n",
      "Iteration 419, loss = 0.21977327\n",
      "Iteration 420, loss = 0.21941506\n",
      "Iteration 421, loss = 0.21905854\n",
      "Iteration 422, loss = 0.21870370\n",
      "Iteration 423, loss = 0.21835053\n",
      "Iteration 424, loss = 0.21799902\n",
      "Iteration 425, loss = 0.21764916\n",
      "Iteration 426, loss = 0.21730093\n",
      "Iteration 427, loss = 0.21695434\n",
      "Iteration 428, loss = 0.21660935\n",
      "Iteration 429, loss = 0.21626598\n",
      "Iteration 430, loss = 0.21592420\n",
      "Iteration 431, loss = 0.21558400\n",
      "Iteration 432, loss = 0.21524538\n",
      "Iteration 433, loss = 0.21490833\n",
      "Iteration 434, loss = 0.21457283\n",
      "Iteration 435, loss = 0.21423888\n",
      "Iteration 436, loss = 0.21390646\n",
      "Iteration 437, loss = 0.21357557\n",
      "Iteration 438, loss = 0.21324619\n",
      "Iteration 439, loss = 0.21291833\n",
      "Iteration 440, loss = 0.21259196\n",
      "Iteration 441, loss = 0.21226707\n",
      "Iteration 442, loss = 0.21194367\n",
      "Iteration 443, loss = 0.21162173\n",
      "Iteration 444, loss = 0.21130126\n",
      "Iteration 445, loss = 0.21098224\n",
      "Iteration 446, loss = 0.21066466\n",
      "Iteration 447, loss = 0.21034851\n",
      "Iteration 448, loss = 0.21003379\n",
      "Iteration 449, loss = 0.20972048\n",
      "Iteration 450, loss = 0.20940858\n",
      "Iteration 451, loss = 0.20909807\n",
      "Iteration 452, loss = 0.20878896\n",
      "Iteration 453, loss = 0.20848123\n",
      "Iteration 454, loss = 0.20817487\n",
      "Iteration 455, loss = 0.20786987\n",
      "Iteration 456, loss = 0.20756623\n",
      "Iteration 457, loss = 0.20726394\n",
      "Iteration 458, loss = 0.20696299\n",
      "Iteration 459, loss = 0.20666337\n",
      "Iteration 460, loss = 0.20636507\n",
      "Iteration 461, loss = 0.20606808\n",
      "Iteration 462, loss = 0.20577241\n",
      "Iteration 463, loss = 0.20547803\n",
      "Iteration 464, loss = 0.20518495\n",
      "Iteration 465, loss = 0.20489314\n",
      "Iteration 466, loss = 0.20460262\n",
      "Iteration 467, loss = 0.20431336\n",
      "Iteration 468, loss = 0.20402536\n",
      "Iteration 469, loss = 0.20373862\n",
      "Iteration 470, loss = 0.20345312\n",
      "Iteration 471, loss = 0.20316886\n",
      "Iteration 472, loss = 0.20288583\n",
      "Iteration 473, loss = 0.20260403\n",
      "Iteration 474, loss = 0.20232344\n",
      "Iteration 475, loss = 0.20204406\n",
      "Iteration 476, loss = 0.20176588\n",
      "Iteration 477, loss = 0.20148890\n",
      "Iteration 478, loss = 0.20121311\n",
      "Iteration 479, loss = 0.20093849\n",
      "Iteration 480, loss = 0.20066505\n",
      "Iteration 481, loss = 0.20039278\n",
      "Iteration 482, loss = 0.20012167\n",
      "Iteration 483, loss = 0.19985171\n",
      "Iteration 484, loss = 0.19958290\n",
      "Iteration 485, loss = 0.19931523\n",
      "Iteration 486, loss = 0.19904869\n",
      "Iteration 487, loss = 0.19878328\n",
      "Iteration 488, loss = 0.19851899\n",
      "Iteration 489, loss = 0.19825581\n",
      "Iteration 490, loss = 0.19799374\n",
      "Iteration 491, loss = 0.19773278\n",
      "Iteration 492, loss = 0.19747290\n",
      "Iteration 493, loss = 0.19721412\n",
      "Iteration 494, loss = 0.19695642\n",
      "Iteration 495, loss = 0.19669979\n",
      "Iteration 496, loss = 0.19644423\n",
      "Iteration 497, loss = 0.19618974\n",
      "Iteration 498, loss = 0.19593631\n",
      "Iteration 499, loss = 0.19568392\n",
      "Iteration 500, loss = 0.19543259\n",
      "Iteration 501, loss = 0.19518229\n",
      "Iteration 502, loss = 0.19493303\n",
      "Iteration 503, loss = 0.19468479\n",
      "Iteration 504, loss = 0.19443758\n",
      "Iteration 505, loss = 0.19419138\n",
      "Iteration 506, loss = 0.19394620\n",
      "Iteration 507, loss = 0.19370201\n",
      "Iteration 508, loss = 0.19345883\n",
      "Iteration 509, loss = 0.19321665\n",
      "Iteration 510, loss = 0.19297545\n",
      "Iteration 511, loss = 0.19273523\n",
      "Iteration 512, loss = 0.19249599\n",
      "Iteration 513, loss = 0.19225772\n",
      "Iteration 514, loss = 0.19202042\n",
      "Iteration 515, loss = 0.19178408\n",
      "Iteration 516, loss = 0.19154870\n",
      "Iteration 517, loss = 0.19131427\n",
      "Iteration 518, loss = 0.19108078\n",
      "Iteration 519, loss = 0.19084823\n",
      "Iteration 520, loss = 0.19061662\n",
      "Iteration 521, loss = 0.19038593\n",
      "Iteration 522, loss = 0.19015617\n",
      "Iteration 523, loss = 0.18992733\n",
      "Iteration 524, loss = 0.18969941\n",
      "Iteration 525, loss = 0.18947239\n",
      "Iteration 526, loss = 0.18924628\n",
      "Iteration 527, loss = 0.18902107\n",
      "Iteration 528, loss = 0.18879675\n",
      "Iteration 529, loss = 0.18857332\n",
      "Iteration 530, loss = 0.18835078\n",
      "Iteration 531, loss = 0.18812911\n",
      "Iteration 532, loss = 0.18790833\n",
      "Iteration 533, loss = 0.18768841\n",
      "Iteration 534, loss = 0.18746936\n",
      "Iteration 535, loss = 0.18725117\n",
      "Iteration 536, loss = 0.18703383\n",
      "Iteration 537, loss = 0.18681735\n",
      "Iteration 538, loss = 0.18660172\n",
      "Iteration 539, loss = 0.18638693\n",
      "Iteration 540, loss = 0.18617297\n",
      "Iteration 541, loss = 0.18595985\n",
      "Iteration 542, loss = 0.18574756\n",
      "Iteration 543, loss = 0.18553610\n",
      "Iteration 544, loss = 0.18532546\n",
      "Iteration 545, loss = 0.18511563\n",
      "Iteration 546, loss = 0.18490661\n",
      "Iteration 547, loss = 0.18469840\n",
      "Iteration 548, loss = 0.18449100\n",
      "Iteration 549, loss = 0.18428439\n",
      "Iteration 550, loss = 0.18407858\n",
      "Iteration 551, loss = 0.18387356\n",
      "Iteration 552, loss = 0.18366932\n",
      "Iteration 553, loss = 0.18346587\n",
      "Iteration 554, loss = 0.18326320\n",
      "Iteration 555, loss = 0.18306130\n",
      "Iteration 556, loss = 0.18286017\n",
      "Iteration 557, loss = 0.18265980\n",
      "Iteration 558, loss = 0.18246020\n",
      "Iteration 559, loss = 0.18226136\n",
      "Iteration 560, loss = 0.18206327\n",
      "Iteration 561, loss = 0.18186593\n",
      "Iteration 562, loss = 0.18166933\n",
      "Iteration 563, loss = 0.18147348\n",
      "Iteration 564, loss = 0.18127837\n",
      "Iteration 565, loss = 0.18108399\n",
      "Iteration 566, loss = 0.18089035\n",
      "Iteration 567, loss = 0.18069743\n",
      "Iteration 568, loss = 0.18050523\n",
      "Iteration 569, loss = 0.18031375\n",
      "Iteration 570, loss = 0.18012299\n",
      "Iteration 571, loss = 0.17993294\n",
      "Iteration 572, loss = 0.17974360\n",
      "Iteration 573, loss = 0.17955497\n",
      "Iteration 574, loss = 0.17936704\n",
      "Iteration 575, loss = 0.17917980\n",
      "Iteration 576, loss = 0.17899326\n",
      "Iteration 577, loss = 0.17880741\n",
      "Iteration 578, loss = 0.17862224\n",
      "Iteration 579, loss = 0.17843776\n",
      "Iteration 580, loss = 0.17825397\n",
      "Iteration 581, loss = 0.17807084\n",
      "Iteration 582, loss = 0.17788839\n",
      "Iteration 583, loss = 0.17770661\n",
      "Iteration 584, loss = 0.17752550\n",
      "Iteration 585, loss = 0.17734505\n",
      "Iteration 586, loss = 0.17716526\n",
      "Iteration 587, loss = 0.17698613\n",
      "Iteration 588, loss = 0.17680765\n",
      "Iteration 589, loss = 0.17662982\n",
      "Iteration 590, loss = 0.17645263\n",
      "Iteration 591, loss = 0.17627609\n",
      "Iteration 592, loss = 0.17610019\n",
      "Iteration 593, loss = 0.17592493\n",
      "Iteration 594, loss = 0.17575030\n",
      "Iteration 595, loss = 0.17557630\n",
      "Iteration 596, loss = 0.17540293\n",
      "Iteration 597, loss = 0.17523018\n",
      "Iteration 598, loss = 0.17505806\n",
      "Iteration 599, loss = 0.17488655\n",
      "Iteration 600, loss = 0.17471566\n",
      "Iteration 601, loss = 0.17454538\n",
      "Iteration 602, loss = 0.17437571\n",
      "Iteration 603, loss = 0.17420665\n",
      "Iteration 604, loss = 0.17403819\n",
      "Iteration 605, loss = 0.17387032\n",
      "Iteration 606, loss = 0.17370306\n",
      "Iteration 607, loss = 0.17353639\n",
      "Iteration 608, loss = 0.17337031\n",
      "Iteration 609, loss = 0.17320482\n",
      "Iteration 610, loss = 0.17303992\n",
      "Iteration 611, loss = 0.17287560\n",
      "Iteration 612, loss = 0.17271186\n",
      "Iteration 613, loss = 0.17254869\n",
      "Iteration 614, loss = 0.17238610\n",
      "Iteration 615, loss = 0.17222408\n",
      "Iteration 616, loss = 0.17206263\n",
      "Iteration 617, loss = 0.17190175\n",
      "Iteration 618, loss = 0.17174143\n",
      "Iteration 619, loss = 0.17158166\n",
      "Iteration 620, loss = 0.17142246\n",
      "Iteration 621, loss = 0.17126381\n",
      "Iteration 622, loss = 0.17110572\n",
      "Iteration 623, loss = 0.17094817\n",
      "Iteration 624, loss = 0.17079117\n",
      "Iteration 625, loss = 0.17063472\n",
      "Iteration 626, loss = 0.17047881\n",
      "Iteration 627, loss = 0.17032343\n",
      "Iteration 628, loss = 0.17016860\n",
      "Iteration 629, loss = 0.17001429\n",
      "Iteration 630, loss = 0.16986052\n",
      "Iteration 631, loss = 0.16970728\n",
      "Iteration 632, loss = 0.16955456\n",
      "Iteration 633, loss = 0.16940237\n",
      "Iteration 634, loss = 0.16925070\n",
      "Iteration 635, loss = 0.16909955\n",
      "Iteration 636, loss = 0.16894891\n",
      "Iteration 637, loss = 0.16879879\n",
      "Iteration 638, loss = 0.16864918\n",
      "Iteration 639, loss = 0.16850008\n",
      "Iteration 640, loss = 0.16835149\n",
      "Iteration 641, loss = 0.16820340\n",
      "Iteration 642, loss = 0.16805581\n",
      "Iteration 643, loss = 0.16790872\n",
      "Iteration 644, loss = 0.16776213\n",
      "Iteration 645, loss = 0.16761604\n",
      "Iteration 646, loss = 0.16747044\n",
      "Iteration 647, loss = 0.16732532\n",
      "Iteration 648, loss = 0.16718070\n",
      "Iteration 649, loss = 0.16703656\n",
      "Iteration 650, loss = 0.16689291\n",
      "Iteration 651, loss = 0.16674973\n",
      "Iteration 652, loss = 0.16660704\n",
      "Iteration 653, loss = 0.16646482\n",
      "Iteration 654, loss = 0.16632307\n",
      "Iteration 655, loss = 0.16618180\n",
      "Iteration 656, loss = 0.16604100\n",
      "Iteration 657, loss = 0.16590067\n",
      "Iteration 658, loss = 0.16576080\n",
      "Iteration 659, loss = 0.16562140\n",
      "Iteration 660, loss = 0.16548245\n",
      "Iteration 661, loss = 0.16534397\n",
      "Iteration 662, loss = 0.16520594\n",
      "Iteration 663, loss = 0.16506837\n",
      "Iteration 664, loss = 0.16493126\n",
      "Iteration 665, loss = 0.16479459\n",
      "Iteration 666, loss = 0.16465837\n",
      "Iteration 667, loss = 0.16452260\n",
      "Iteration 668, loss = 0.16438727\n",
      "Iteration 669, loss = 0.16425239\n",
      "Iteration 670, loss = 0.16411795\n",
      "Iteration 671, loss = 0.16398394\n",
      "Iteration 672, loss = 0.16385037\n",
      "Iteration 673, loss = 0.16371724\n",
      "Iteration 674, loss = 0.16358454\n",
      "Iteration 675, loss = 0.16345227\n",
      "Iteration 676, loss = 0.16332043\n",
      "Iteration 677, loss = 0.16318902\n",
      "Iteration 678, loss = 0.16305803\n",
      "Iteration 679, loss = 0.16292746\n",
      "Iteration 680, loss = 0.16279731\n",
      "Iteration 681, loss = 0.16266759\n",
      "Iteration 682, loss = 0.16253828\n",
      "Iteration 683, loss = 0.16240939\n",
      "Iteration 684, loss = 0.16228091\n",
      "Iteration 685, loss = 0.16215284\n",
      "Iteration 686, loss = 0.16202518\n",
      "Iteration 687, loss = 0.16189793\n",
      "Iteration 688, loss = 0.16177108\n",
      "Iteration 689, loss = 0.16164464\n",
      "Iteration 690, loss = 0.16151861\n",
      "Iteration 691, loss = 0.16139297\n",
      "Iteration 692, loss = 0.16126773\n",
      "Iteration 693, loss = 0.16114289\n",
      "Iteration 694, loss = 0.16101844\n",
      "Iteration 695, loss = 0.16089439\n",
      "Iteration 696, loss = 0.16077073\n",
      "Iteration 697, loss = 0.16064746\n",
      "Iteration 698, loss = 0.16052458\n",
      "Iteration 699, loss = 0.16040208\n",
      "Iteration 700, loss = 0.16027997\n",
      "Iteration 701, loss = 0.16015825\n",
      "Iteration 702, loss = 0.16003690\n",
      "Iteration 703, loss = 0.15991593\n",
      "Iteration 704, loss = 0.15979535\n",
      "Iteration 705, loss = 0.15967513\n",
      "Iteration 706, loss = 0.15955530\n",
      "Iteration 707, loss = 0.15943583\n",
      "Iteration 708, loss = 0.15931674\n",
      "Iteration 709, loss = 0.15919802\n",
      "Iteration 710, loss = 0.15907967\n",
      "Iteration 711, loss = 0.15896168\n",
      "Iteration 712, loss = 0.15884406\n",
      "Iteration 713, loss = 0.15872680\n",
      "Iteration 714, loss = 0.15860990\n",
      "Iteration 715, loss = 0.15849337\n",
      "Iteration 716, loss = 0.15837719\n",
      "Iteration 717, loss = 0.15826137\n",
      "Iteration 718, loss = 0.15814590\n",
      "Iteration 719, loss = 0.15803079\n",
      "Iteration 720, loss = 0.15791603\n",
      "Iteration 721, loss = 0.15780162\n",
      "Iteration 722, loss = 0.15768757\n",
      "Iteration 723, loss = 0.15757385\n",
      "Iteration 724, loss = 0.15746049\n",
      "Iteration 725, loss = 0.15734747\n",
      "Iteration 726, loss = 0.15723480\n",
      "Iteration 727, loss = 0.15712246\n",
      "Iteration 728, loss = 0.15701047\n",
      "Iteration 729, loss = 0.15689881\n",
      "Iteration 730, loss = 0.15678750\n",
      "Iteration 731, loss = 0.15667652\n",
      "Iteration 732, loss = 0.15656587\n",
      "Iteration 733, loss = 0.15645556\n",
      "Iteration 734, loss = 0.15634558\n",
      "Iteration 735, loss = 0.15623593\n",
      "Iteration 736, loss = 0.15612660\n",
      "Iteration 737, loss = 0.15601761\n",
      "Iteration 738, loss = 0.15590894\n",
      "Iteration 739, loss = 0.15580060\n",
      "Iteration 740, loss = 0.15569258\n",
      "Iteration 741, loss = 0.15558488\n",
      "Iteration 742, loss = 0.15547750\n",
      "Iteration 743, loss = 0.15537044\n",
      "Iteration 744, loss = 0.15526370\n",
      "Iteration 745, loss = 0.15515728\n",
      "Iteration 746, loss = 0.15505117\n",
      "Iteration 747, loss = 0.15494538\n",
      "Iteration 748, loss = 0.15483989\n",
      "Iteration 749, loss = 0.15473472\n",
      "Iteration 750, loss = 0.15462986\n",
      "Iteration 751, loss = 0.15452531\n",
      "Iteration 752, loss = 0.15442106\n",
      "Iteration 753, loss = 0.15431712\n",
      "Iteration 754, loss = 0.15421349\n",
      "Iteration 755, loss = 0.15411016\n",
      "Iteration 756, loss = 0.15400713\n",
      "Iteration 757, loss = 0.15390440\n",
      "Iteration 758, loss = 0.15380197\n",
      "Iteration 759, loss = 0.15369984\n",
      "Iteration 760, loss = 0.15359801\n",
      "Iteration 761, loss = 0.15349647\n",
      "Iteration 762, loss = 0.15339523\n",
      "Iteration 763, loss = 0.15329428\n",
      "Iteration 764, loss = 0.15319362\n",
      "Iteration 765, loss = 0.15309326\n",
      "Iteration 766, loss = 0.15299318\n",
      "Iteration 767, loss = 0.15289339\n",
      "Iteration 768, loss = 0.15279389\n",
      "Iteration 769, loss = 0.15269468\n",
      "Iteration 770, loss = 0.15259575\n",
      "Iteration 771, loss = 0.15249710\n",
      "Iteration 772, loss = 0.15239874\n",
      "Iteration 773, loss = 0.15230066\n",
      "Iteration 774, loss = 0.15220285\n",
      "Iteration 775, loss = 0.15210533\n",
      "Iteration 776, loss = 0.15200809\n",
      "Iteration 777, loss = 0.15191112\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=[4], learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=[4], learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=[4], learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Y.shape[1]\n",
    "mlp = MLPClassifier(hidden_layer_sizes=[4],activation='tanh',solver='sgd',alpha=0,learning_rate_init=0.01,max_iter=2000,batch_size=m,shuffle=False,momentum=0,verbose=True)\n",
    "mlp.fit(X.T,Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights and biases\n",
      "W1: [[ 0.70461747 -0.76116886 -0.22569531]\n",
      " [-0.1695387  -0.45054328 -0.26953396]\n",
      " [ 0.04849962  0.16145835 -0.74656021]\n",
      " [ 0.56635342  0.28708787  0.38779696]]\n",
      "b1: [[ 0.61733431]\n",
      " [-0.04359204]\n",
      " [-0.21404018]\n",
      " [-0.13622335]]\n",
      "W2: [[-1.1888251   0.23708056  0.51935621 -0.38401234]]\n",
      "b2: [[-1.61228508]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",mlp.coefs_[0].T)\n",
    "print(\"b1:\",mlp.intercepts_[0].reshape(-1,1))\n",
    "print(\"W2:\",mlp.coefs_[1].T)\n",
    "print(\"b2:\",mlp.intercepts_[1].reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training a Shallow Neural Network Using Backpropagation\n",
    "The following code implements backpropagation to train a shallow neural network with 4 neurons in its hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the paramaters of the neural network\n",
    "W1 = np.random.rand(4,3)\n",
    "b1 = np.zeros((4,1))\n",
    "W2 = np.random.rand(1,4)\n",
    "b2 = np.zeros((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last iteration: 777\n",
      "Losses: [0.16626296 0.16614036 0.16601809 0.16589614 0.16577452 0.16565323\n",
      " 0.16553226 0.16541161 0.16529127 0.16517126 0.16505157]\n",
      "\n",
      "Weights and biases\n",
      "W1: [[0.51426612 0.38412565 0.86649269]\n",
      " [0.66441821 0.2498951  0.76796333]\n",
      " [0.93864872 0.79024567 0.43656679]\n",
      " [0.70329638 0.8004334  0.13971921]]\n",
      "b1: [[-0.1655859 ]\n",
      " [-0.6611979 ]\n",
      " [ 0.20686082]\n",
      " [-0.65955053]]\n",
      "W2: [[ 0.07468513  0.64029642 -0.40869096  0.75230841]]\n",
      "b2: [[-1.59494196]]\n"
     ]
    }
   ],
   "source": [
    "# Updating parameters using gradient descent\n",
    "iter = 777\n",
    "lr = 0.01\n",
    "loss = np.arange(10,21)\n",
    "\n",
    "for i in np.arange(iter):\n",
    "    # Forward propagation\n",
    "    Z1 = W1@X + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = W2@A1 + b2\n",
    "    A2 = 1/(1+np.exp(-Z2))\n",
    "\n",
    "    # Back propagation\n",
    "    dZ2 = A2-Y\n",
    "    dW2 = 1/m*(dZ2@A1.T)\n",
    "    db2 = 1/m*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = (W2.T@dZ2)*(1-np.tanh(Z1)**2)\n",
    "    dW1 = 1/m*(dZ1@X.T)\n",
    "    db1 = 1/m*np.sum(dZ1,axis=1,keepdims=True)\n",
    "\n",
    "    W2 -= lr*dW2\n",
    "    b2 -= lr*db2\n",
    "    W1 -= lr*dW1\n",
    "    b1 -= lr*db1\n",
    "\n",
    "    current_loss = -1/m*(Y@np.log(A2).T+(1-Y)@np.log(1-A2).T)\n",
    "    loss = np.append(loss,current_loss)\n",
    "    loss = np.delete(loss,0)\n",
    "\n",
    "print(\"Last iteration:\",i+1)\n",
    "print(\"Losses:\",loss)\n",
    "\n",
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",W1)\n",
    "print(\"b1:\",b1)\n",
    "print(\"W2:\",W2)\n",
    "print(\"b2:\",b2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusion\n",
    "Similar values of loss from 2 & 3 for the same number of iterations indicates that the custom gradient descent implementation is correct. The weights and biases are different because the 2 models are randomly initialised during training and the loss function of the shallow neural network has multiple maximia and minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
