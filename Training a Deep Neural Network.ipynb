{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Deep Neural Network\n",
    "The following code implements a deep neural network of 2 hidden layers with backpropagation using low-level libraries and compares it with a model generated by Scikit-learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Loading & Cleaning\n",
    "The data set contains credit card debt information about 10,000 customers and whether they defaulted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income\n",
       "0      No      No   729.526495  44361.625074\n",
       "1      No     Yes   817.180407  12106.134700\n",
       "2      No      No  1073.549164  31767.138947\n",
       "3      No      No   529.250605  35704.493935\n",
       "4      No      No   785.655883  38463.495879"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('Default.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and converting to NumPy arrays\n",
    "df['default']=df['default'].apply(lambda x: 0 if x=='No' else 1)\n",
    "df['student']=df['student'].apply(lambda x: 0 if x=='No' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "0        0        0   729.526495  44361.625074\n",
       "1        0        1   817.180407  12106.134700\n",
       "2        0        0  1073.549164  31767.138947\n",
       "3        0        0   529.250605  35704.493935\n",
       "4        0        0   785.655883  38463.495879"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   default  10000 non-null  int64  \n",
      " 1   student  10000 non-null  int64  \n",
      " 2   balance  10000 non-null  float64\n",
      " 3   income   10000 non-null  float64\n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 312.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.218835</td>\n",
       "      <td>0.813187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.037616</td>\n",
       "      <td>-1.605496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>-0.131212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.632893</td>\n",
       "      <td>0.164031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>0.370915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.255990</td>\n",
       "      <td>1.460366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.160044</td>\n",
       "      <td>-1.039014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>1.883565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.516742</td>\n",
       "      <td>0.236363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.311691</td>\n",
       "      <td>-1.248805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      default  student   balance    income\n",
       "0           0        0 -0.218835  0.813187\n",
       "1           0        1 -0.037616 -1.605496\n",
       "2           0        0  0.492410 -0.131212\n",
       "3           0        0 -0.632893  0.164031\n",
       "4           0        0 -0.102791  0.370915\n",
       "...       ...      ...       ...       ...\n",
       "9995        0        0 -0.255990  1.460366\n",
       "9996        0        0 -0.160044 -1.039014\n",
       "9997        0        0  0.020751  1.883565\n",
       "9998        0        0  1.516742  0.236363\n",
       "9999        0        1 -1.311691 -1.248805\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['balance','income']] = scaler.fit_transform(df[['balance','income']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['default'].to_numpy().reshape(-1,1)\n",
    "X = df.drop(columns=['default']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (10000, 1)\n",
      "Shape of X: (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 10000)\n",
      "Shape of X: (3, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = X.T\n",
    "Y = Y.T\n",
    "\n",
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training a Deep Neural Network Using Scikit-learn\n",
    "The following code trains a deep neural network of 2 hidden layers with 4 neurons in each hidden layer using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1118: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79736584\n",
      "Iteration 2, loss = 0.78900528\n",
      "Iteration 3, loss = 0.78079423\n",
      "Iteration 4, loss = 0.77272940\n",
      "Iteration 5, loss = 0.76480754\n",
      "Iteration 6, loss = 0.75702550\n",
      "Iteration 7, loss = 0.74938021\n",
      "Iteration 8, loss = 0.74186866\n",
      "Iteration 9, loss = 0.73448790\n",
      "Iteration 10, loss = 0.72723506\n",
      "Iteration 11, loss = 0.72010733\n",
      "Iteration 12, loss = 0.71310199\n",
      "Iteration 13, loss = 0.70621635\n",
      "Iteration 14, loss = 0.69944780\n",
      "Iteration 15, loss = 0.69279380\n",
      "Iteration 16, loss = 0.68625186\n",
      "Iteration 17, loss = 0.67981956\n",
      "Iteration 18, loss = 0.67349452\n",
      "Iteration 19, loss = 0.66727442\n",
      "Iteration 20, loss = 0.66115702\n",
      "Iteration 21, loss = 0.65514011\n",
      "Iteration 22, loss = 0.64922154\n",
      "Iteration 23, loss = 0.64339922\n",
      "Iteration 24, loss = 0.63767108\n",
      "Iteration 25, loss = 0.63203515\n",
      "Iteration 26, loss = 0.62648947\n",
      "Iteration 27, loss = 0.62103213\n",
      "Iteration 28, loss = 0.61566128\n",
      "Iteration 29, loss = 0.61037511\n",
      "Iteration 30, loss = 0.60517185\n",
      "Iteration 31, loss = 0.60004979\n",
      "Iteration 32, loss = 0.59500722\n",
      "Iteration 33, loss = 0.59004252\n",
      "Iteration 34, loss = 0.58515407\n",
      "Iteration 35, loss = 0.58034032\n",
      "Iteration 36, loss = 0.57559974\n",
      "Iteration 37, loss = 0.57093084\n",
      "Iteration 38, loss = 0.56633217\n",
      "Iteration 39, loss = 0.56180230\n",
      "Iteration 40, loss = 0.55733986\n",
      "Iteration 41, loss = 0.55294348\n",
      "Iteration 42, loss = 0.54861186\n",
      "Iteration 43, loss = 0.54434371\n",
      "Iteration 44, loss = 0.54013776\n",
      "Iteration 45, loss = 0.53599279\n",
      "Iteration 46, loss = 0.53190761\n",
      "Iteration 47, loss = 0.52788104\n",
      "Iteration 48, loss = 0.52391194\n",
      "Iteration 49, loss = 0.51999921\n",
      "Iteration 50, loss = 0.51614174\n",
      "Iteration 51, loss = 0.51233848\n",
      "Iteration 52, loss = 0.50858839\n",
      "Iteration 53, loss = 0.50489046\n",
      "Iteration 54, loss = 0.50124370\n",
      "Iteration 55, loss = 0.49764715\n",
      "Iteration 56, loss = 0.49409985\n",
      "Iteration 57, loss = 0.49060089\n",
      "Iteration 58, loss = 0.48714936\n",
      "Iteration 59, loss = 0.48374439\n",
      "Iteration 60, loss = 0.48038513\n",
      "Iteration 61, loss = 0.47707072\n",
      "Iteration 62, loss = 0.47380035\n",
      "Iteration 63, loss = 0.47057322\n",
      "Iteration 64, loss = 0.46738854\n",
      "Iteration 65, loss = 0.46424556\n",
      "Iteration 66, loss = 0.46114352\n",
      "Iteration 67, loss = 0.45808170\n",
      "Iteration 68, loss = 0.45505937\n",
      "Iteration 69, loss = 0.45207584\n",
      "Iteration 70, loss = 0.44913043\n",
      "Iteration 71, loss = 0.44622248\n",
      "Iteration 72, loss = 0.44335133\n",
      "Iteration 73, loss = 0.44051633\n",
      "Iteration 74, loss = 0.43771688\n",
      "Iteration 75, loss = 0.43495236\n",
      "Iteration 76, loss = 0.43222218\n",
      "Iteration 77, loss = 0.42952574\n",
      "Iteration 78, loss = 0.42686249\n",
      "Iteration 79, loss = 0.42423186\n",
      "Iteration 80, loss = 0.42163332\n",
      "Iteration 81, loss = 0.41906632\n",
      "Iteration 82, loss = 0.41653034\n",
      "Iteration 83, loss = 0.41402488\n",
      "Iteration 84, loss = 0.41154943\n",
      "Iteration 85, loss = 0.40910351\n",
      "Iteration 86, loss = 0.40668664\n",
      "Iteration 87, loss = 0.40429834\n",
      "Iteration 88, loss = 0.40193817\n",
      "Iteration 89, loss = 0.39960567\n",
      "Iteration 90, loss = 0.39730040\n",
      "Iteration 91, loss = 0.39502194\n",
      "Iteration 92, loss = 0.39276986\n",
      "Iteration 93, loss = 0.39054375\n",
      "Iteration 94, loss = 0.38834321\n",
      "Iteration 95, loss = 0.38616785\n",
      "Iteration 96, loss = 0.38401727\n",
      "Iteration 97, loss = 0.38189109\n",
      "Iteration 98, loss = 0.37978896\n",
      "Iteration 99, loss = 0.37771049\n",
      "Iteration 100, loss = 0.37565534\n",
      "Iteration 101, loss = 0.37362316\n",
      "Iteration 102, loss = 0.37161361\n",
      "Iteration 103, loss = 0.36962634\n",
      "Iteration 104, loss = 0.36766104\n",
      "Iteration 105, loss = 0.36571738\n",
      "Iteration 106, loss = 0.36379504\n",
      "Iteration 107, loss = 0.36189371\n",
      "Iteration 108, loss = 0.36001310\n",
      "Iteration 109, loss = 0.35815290\n",
      "Iteration 110, loss = 0.35631282\n",
      "Iteration 111, loss = 0.35449259\n",
      "Iteration 112, loss = 0.35269190\n",
      "Iteration 113, loss = 0.35091050\n",
      "Iteration 114, loss = 0.34914811\n",
      "Iteration 115, loss = 0.34740447\n",
      "Iteration 116, loss = 0.34567932\n",
      "Iteration 117, loss = 0.34397239\n",
      "Iteration 118, loss = 0.34228346\n",
      "Iteration 119, loss = 0.34061225\n",
      "Iteration 120, loss = 0.33895855\n",
      "Iteration 121, loss = 0.33732210\n",
      "Iteration 122, loss = 0.33570269\n",
      "Iteration 123, loss = 0.33410007\n",
      "Iteration 124, loss = 0.33251403\n",
      "Iteration 125, loss = 0.33094435\n",
      "Iteration 126, loss = 0.32939080\n",
      "Iteration 127, loss = 0.32785319\n",
      "Iteration 128, loss = 0.32633130\n",
      "Iteration 129, loss = 0.32482493\n",
      "Iteration 130, loss = 0.32333387\n",
      "Iteration 131, loss = 0.32185793\n",
      "Iteration 132, loss = 0.32039692\n",
      "Iteration 133, loss = 0.31895064\n",
      "Iteration 134, loss = 0.31751891\n",
      "Iteration 135, loss = 0.31610154\n",
      "Iteration 136, loss = 0.31469836\n",
      "Iteration 137, loss = 0.31330918\n",
      "Iteration 138, loss = 0.31193383\n",
      "Iteration 139, loss = 0.31057213\n",
      "Iteration 140, loss = 0.30922393\n",
      "Iteration 141, loss = 0.30788905\n",
      "Iteration 142, loss = 0.30656732\n",
      "Iteration 143, loss = 0.30525860\n",
      "Iteration 144, loss = 0.30396271\n",
      "Iteration 145, loss = 0.30267951\n",
      "Iteration 146, loss = 0.30140884\n",
      "Iteration 147, loss = 0.30015055\n",
      "Iteration 148, loss = 0.29890449\n",
      "Iteration 149, loss = 0.29767052\n",
      "Iteration 150, loss = 0.29644849\n",
      "Iteration 151, loss = 0.29523826\n",
      "Iteration 152, loss = 0.29403969\n",
      "Iteration 153, loss = 0.29285264\n",
      "Iteration 154, loss = 0.29167699\n",
      "Iteration 155, loss = 0.29051258\n",
      "Iteration 156, loss = 0.28935931\n",
      "Iteration 157, loss = 0.28821703\n",
      "Iteration 158, loss = 0.28708561\n",
      "Iteration 159, loss = 0.28596494\n",
      "Iteration 160, loss = 0.28485489\n",
      "Iteration 161, loss = 0.28375534\n",
      "Iteration 162, loss = 0.28266617\n",
      "Iteration 163, loss = 0.28158726\n",
      "Iteration 164, loss = 0.28051849\n",
      "Iteration 165, loss = 0.27945975\n",
      "Iteration 166, loss = 0.27841093\n",
      "Iteration 167, loss = 0.27737191\n",
      "Iteration 168, loss = 0.27634259\n",
      "Iteration 169, loss = 0.27532285\n",
      "Iteration 170, loss = 0.27431259\n",
      "Iteration 171, loss = 0.27331171\n",
      "Iteration 172, loss = 0.27232010\n",
      "Iteration 173, loss = 0.27133766\n",
      "Iteration 174, loss = 0.27036429\n",
      "Iteration 175, loss = 0.26939988\n",
      "Iteration 176, loss = 0.26844435\n",
      "Iteration 177, loss = 0.26749758\n",
      "Iteration 178, loss = 0.26655949\n",
      "Iteration 179, loss = 0.26562999\n",
      "Iteration 180, loss = 0.26470897\n",
      "Iteration 181, loss = 0.26379635\n",
      "Iteration 182, loss = 0.26289204\n",
      "Iteration 183, loss = 0.26199595\n",
      "Iteration 184, loss = 0.26110798\n",
      "Iteration 185, loss = 0.26022806\n",
      "Iteration 186, loss = 0.25935609\n",
      "Iteration 187, loss = 0.25849199\n",
      "Iteration 188, loss = 0.25763567\n",
      "Iteration 189, loss = 0.25678706\n",
      "Iteration 190, loss = 0.25594608\n",
      "Iteration 191, loss = 0.25511263\n",
      "Iteration 192, loss = 0.25428664\n",
      "Iteration 193, loss = 0.25346804\n",
      "Iteration 194, loss = 0.25265674\n",
      "Iteration 195, loss = 0.25185267\n",
      "Iteration 196, loss = 0.25105575\n",
      "Iteration 197, loss = 0.25026591\n",
      "Iteration 198, loss = 0.24948307\n",
      "Iteration 199, loss = 0.24870717\n",
      "Iteration 200, loss = 0.24793811\n",
      "Iteration 201, loss = 0.24717585\n",
      "Iteration 202, loss = 0.24642030\n",
      "Iteration 203, loss = 0.24567140\n",
      "Iteration 204, loss = 0.24492907\n",
      "Iteration 205, loss = 0.24419326\n",
      "Iteration 206, loss = 0.24346388\n",
      "Iteration 207, loss = 0.24274089\n",
      "Iteration 208, loss = 0.24202420\n",
      "Iteration 209, loss = 0.24131375\n",
      "Iteration 210, loss = 0.24060949\n",
      "Iteration 211, loss = 0.23991135\n",
      "Iteration 212, loss = 0.23921926\n",
      "Iteration 213, loss = 0.23853317\n",
      "Iteration 214, loss = 0.23785300\n",
      "Iteration 215, loss = 0.23717871\n",
      "Iteration 216, loss = 0.23651023\n",
      "Iteration 217, loss = 0.23584751\n",
      "Iteration 218, loss = 0.23519048\n",
      "Iteration 219, loss = 0.23453909\n",
      "Iteration 220, loss = 0.23389327\n",
      "Iteration 221, loss = 0.23325298\n",
      "Iteration 222, loss = 0.23261816\n",
      "Iteration 223, loss = 0.23198875\n",
      "Iteration 224, loss = 0.23136470\n",
      "Iteration 225, loss = 0.23074595\n",
      "Iteration 226, loss = 0.23013246\n",
      "Iteration 227, loss = 0.22952416\n",
      "Iteration 228, loss = 0.22892101\n",
      "Iteration 229, loss = 0.22832295\n",
      "Iteration 230, loss = 0.22772994\n",
      "Iteration 231, loss = 0.22714192\n",
      "Iteration 232, loss = 0.22655884\n",
      "Iteration 233, loss = 0.22598066\n",
      "Iteration 234, loss = 0.22540733\n",
      "Iteration 235, loss = 0.22483879\n",
      "Iteration 236, loss = 0.22427500\n",
      "Iteration 237, loss = 0.22371592\n",
      "Iteration 238, loss = 0.22316149\n",
      "Iteration 239, loss = 0.22261167\n",
      "Iteration 240, loss = 0.22206642\n",
      "Iteration 241, loss = 0.22152568\n",
      "Iteration 242, loss = 0.22098942\n",
      "Iteration 243, loss = 0.22045760\n",
      "Iteration 244, loss = 0.21993016\n",
      "Iteration 245, loss = 0.21940706\n",
      "Iteration 246, loss = 0.21888826\n",
      "Iteration 247, loss = 0.21837373\n",
      "Iteration 248, loss = 0.21786341\n",
      "Iteration 249, loss = 0.21735726\n",
      "Iteration 250, loss = 0.21685526\n",
      "Iteration 251, loss = 0.21635734\n",
      "Iteration 252, loss = 0.21586349\n",
      "Iteration 253, loss = 0.21537365\n",
      "Iteration 254, loss = 0.21488778\n",
      "Iteration 255, loss = 0.21440585\n",
      "Iteration 256, loss = 0.21392782\n",
      "Iteration 257, loss = 0.21345366\n",
      "Iteration 258, loss = 0.21298331\n",
      "Iteration 259, loss = 0.21251676\n",
      "Iteration 260, loss = 0.21205395\n",
      "Iteration 261, loss = 0.21159486\n",
      "Iteration 262, loss = 0.21113944\n",
      "Iteration 263, loss = 0.21068767\n",
      "Iteration 264, loss = 0.21023951\n",
      "Iteration 265, loss = 0.20979492\n",
      "Iteration 266, loss = 0.20935386\n",
      "Iteration 267, loss = 0.20891632\n",
      "Iteration 268, loss = 0.20848224\n",
      "Iteration 269, loss = 0.20805160\n",
      "Iteration 270, loss = 0.20762436\n",
      "Iteration 271, loss = 0.20720050\n",
      "Iteration 272, loss = 0.20677997\n",
      "Iteration 273, loss = 0.20636275\n",
      "Iteration 274, loss = 0.20594881\n",
      "Iteration 275, loss = 0.20553812\n",
      "Iteration 276, loss = 0.20513063\n",
      "Iteration 277, loss = 0.20472633\n",
      "Iteration 278, loss = 0.20432519\n",
      "Iteration 279, loss = 0.20392717\n",
      "Iteration 280, loss = 0.20353224\n",
      "Iteration 281, loss = 0.20314037\n",
      "Iteration 282, loss = 0.20275154\n",
      "Iteration 283, loss = 0.20236572\n",
      "Iteration 284, loss = 0.20198287\n",
      "Iteration 285, loss = 0.20160297\n",
      "Iteration 286, loss = 0.20122600\n",
      "Iteration 287, loss = 0.20085192\n",
      "Iteration 288, loss = 0.20048071\n",
      "Iteration 289, loss = 0.20011233\n",
      "Iteration 290, loss = 0.19974678\n",
      "Iteration 291, loss = 0.19938400\n",
      "Iteration 292, loss = 0.19902399\n",
      "Iteration 293, loss = 0.19866672\n",
      "Iteration 294, loss = 0.19831215\n",
      "Iteration 295, loss = 0.19796027\n",
      "Iteration 296, loss = 0.19761105\n",
      "Iteration 297, loss = 0.19726446\n",
      "Iteration 298, loss = 0.19692049\n",
      "Iteration 299, loss = 0.19657909\n",
      "Iteration 300, loss = 0.19624026\n",
      "Iteration 301, loss = 0.19590397\n",
      "Iteration 302, loss = 0.19557019\n",
      "Iteration 303, loss = 0.19523891\n",
      "Iteration 304, loss = 0.19491009\n",
      "Iteration 305, loss = 0.19458371\n",
      "Iteration 306, loss = 0.19425976\n",
      "Iteration 307, loss = 0.19393821\n",
      "Iteration 308, loss = 0.19361903\n",
      "Iteration 309, loss = 0.19330222\n",
      "Iteration 310, loss = 0.19298773\n",
      "Iteration 311, loss = 0.19267556\n",
      "Iteration 312, loss = 0.19236568\n",
      "Iteration 313, loss = 0.19205808\n",
      "Iteration 314, loss = 0.19175272\n",
      "Iteration 315, loss = 0.19144959\n",
      "Iteration 316, loss = 0.19114867\n",
      "Iteration 317, loss = 0.19084994\n",
      "Iteration 318, loss = 0.19055338\n",
      "Iteration 319, loss = 0.19025897\n",
      "Iteration 320, loss = 0.18996668\n",
      "Iteration 321, loss = 0.18967651\n",
      "Iteration 322, loss = 0.18938843\n",
      "Iteration 323, loss = 0.18910242\n",
      "Iteration 324, loss = 0.18881846\n",
      "Iteration 325, loss = 0.18853654\n",
      "Iteration 326, loss = 0.18825663\n",
      "Iteration 327, loss = 0.18797872\n",
      "Iteration 328, loss = 0.18770280\n",
      "Iteration 329, loss = 0.18742883\n",
      "Iteration 330, loss = 0.18715681\n",
      "Iteration 331, loss = 0.18688672\n",
      "Iteration 332, loss = 0.18661853\n",
      "Iteration 333, loss = 0.18635224\n",
      "Iteration 334, loss = 0.18608783\n",
      "Iteration 335, loss = 0.18582527\n",
      "Iteration 336, loss = 0.18556455\n",
      "Iteration 337, loss = 0.18530567\n",
      "Iteration 338, loss = 0.18504859\n",
      "Iteration 339, loss = 0.18479330\n",
      "Iteration 340, loss = 0.18453979\n",
      "Iteration 341, loss = 0.18428804\n",
      "Iteration 342, loss = 0.18403804\n",
      "Iteration 343, loss = 0.18378977\n",
      "Iteration 344, loss = 0.18354322\n",
      "Iteration 345, loss = 0.18329836\n",
      "Iteration 346, loss = 0.18305519\n",
      "Iteration 347, loss = 0.18281369\n",
      "Iteration 348, loss = 0.18257385\n",
      "Iteration 349, loss = 0.18233564\n",
      "Iteration 350, loss = 0.18209906\n",
      "Iteration 351, loss = 0.18186410\n",
      "Iteration 352, loss = 0.18163073\n",
      "Iteration 353, loss = 0.18139894\n",
      "Iteration 354, loss = 0.18116873\n",
      "Iteration 355, loss = 0.18094007\n",
      "Iteration 356, loss = 0.18071296\n",
      "Iteration 357, loss = 0.18048737\n",
      "Iteration 358, loss = 0.18026330\n",
      "Iteration 359, loss = 0.18004073\n",
      "Iteration 360, loss = 0.17981965\n",
      "Iteration 361, loss = 0.17960005\n",
      "Iteration 362, loss = 0.17938191\n",
      "Iteration 363, loss = 0.17916522\n",
      "Iteration 364, loss = 0.17894997\n",
      "Iteration 365, loss = 0.17873615\n",
      "Iteration 366, loss = 0.17852374\n",
      "Iteration 367, loss = 0.17831273\n",
      "Iteration 368, loss = 0.17810311\n",
      "Iteration 369, loss = 0.17789487\n",
      "Iteration 370, loss = 0.17768800\n",
      "Iteration 371, loss = 0.17748247\n",
      "Iteration 372, loss = 0.17727829\n",
      "Iteration 373, loss = 0.17707545\n",
      "Iteration 374, loss = 0.17687392\n",
      "Iteration 375, loss = 0.17667369\n",
      "Iteration 376, loss = 0.17647477\n",
      "Iteration 377, loss = 0.17627713\n",
      "Iteration 378, loss = 0.17608077\n",
      "Iteration 379, loss = 0.17588567\n",
      "Iteration 380, loss = 0.17569182\n",
      "Iteration 381, loss = 0.17549921\n",
      "Iteration 382, loss = 0.17530784\n",
      "Iteration 383, loss = 0.17511769\n",
      "Iteration 384, loss = 0.17492875\n",
      "Iteration 385, loss = 0.17474101\n",
      "Iteration 386, loss = 0.17455446\n",
      "Iteration 387, loss = 0.17436910\n",
      "Iteration 388, loss = 0.17418490\n",
      "Iteration 389, loss = 0.17400186\n",
      "Iteration 390, loss = 0.17381998\n",
      "Iteration 391, loss = 0.17363924\n",
      "Iteration 392, loss = 0.17345962\n",
      "Iteration 393, loss = 0.17328114\n",
      "Iteration 394, loss = 0.17310376\n",
      "Iteration 395, loss = 0.17292749\n",
      "Iteration 396, loss = 0.17275231\n",
      "Iteration 397, loss = 0.17257822\n",
      "Iteration 398, loss = 0.17240520\n",
      "Iteration 399, loss = 0.17223325\n",
      "Iteration 400, loss = 0.17206236\n",
      "Iteration 401, loss = 0.17189252\n",
      "Iteration 402, loss = 0.17172372\n",
      "Iteration 403, loss = 0.17155595\n",
      "Iteration 404, loss = 0.17138921\n",
      "Iteration 405, loss = 0.17122348\n",
      "Iteration 406, loss = 0.17105876\n",
      "Iteration 407, loss = 0.17089504\n",
      "Iteration 408, loss = 0.17073230\n",
      "Iteration 409, loss = 0.17057055\n",
      "Iteration 410, loss = 0.17040978\n",
      "Iteration 411, loss = 0.17024997\n",
      "Iteration 412, loss = 0.17009112\n",
      "Iteration 413, loss = 0.16993321\n",
      "Iteration 414, loss = 0.16977625\n",
      "Iteration 415, loss = 0.16962023\n",
      "Iteration 416, loss = 0.16946513\n",
      "Iteration 417, loss = 0.16931095\n",
      "Iteration 418, loss = 0.16915769\n",
      "Iteration 419, loss = 0.16900533\n",
      "Iteration 420, loss = 0.16885386\n",
      "Iteration 421, loss = 0.16870329\n",
      "Iteration 422, loss = 0.16855360\n",
      "Iteration 423, loss = 0.16840478\n",
      "Iteration 424, loss = 0.16825684\n",
      "Iteration 425, loss = 0.16810975\n",
      "Iteration 426, loss = 0.16796352\n",
      "Iteration 427, loss = 0.16781814\n",
      "Iteration 428, loss = 0.16767359\n",
      "Iteration 429, loss = 0.16752989\n",
      "Iteration 430, loss = 0.16738700\n",
      "Iteration 431, loss = 0.16724494\n",
      "Iteration 432, loss = 0.16710370\n",
      "Iteration 433, loss = 0.16696326\n",
      "Iteration 434, loss = 0.16682362\n",
      "Iteration 435, loss = 0.16668478\n",
      "Iteration 436, loss = 0.16654672\n",
      "Iteration 437, loss = 0.16640945\n",
      "Iteration 438, loss = 0.16627295\n",
      "Iteration 439, loss = 0.16613722\n",
      "Iteration 440, loss = 0.16600225\n",
      "Iteration 441, loss = 0.16586805\n",
      "Iteration 442, loss = 0.16573459\n",
      "Iteration 443, loss = 0.16560188\n",
      "Iteration 444, loss = 0.16546990\n",
      "Iteration 445, loss = 0.16533867\n",
      "Iteration 446, loss = 0.16520815\n",
      "Iteration 447, loss = 0.16507836\n",
      "Iteration 448, loss = 0.16494929\n",
      "Iteration 449, loss = 0.16482093\n",
      "Iteration 450, loss = 0.16469327\n",
      "Iteration 451, loss = 0.16456631\n",
      "Iteration 452, loss = 0.16444005\n",
      "Iteration 453, loss = 0.16431447\n",
      "Iteration 454, loss = 0.16418958\n",
      "Iteration 455, loss = 0.16406536\n",
      "Iteration 456, loss = 0.16394182\n",
      "Iteration 457, loss = 0.16381894\n",
      "Iteration 458, loss = 0.16369673\n",
      "Iteration 459, loss = 0.16357517\n",
      "Iteration 460, loss = 0.16345426\n",
      "Iteration 461, loss = 0.16333400\n",
      "Iteration 462, loss = 0.16321438\n",
      "Iteration 463, loss = 0.16309540\n",
      "Iteration 464, loss = 0.16297705\n",
      "Iteration 465, loss = 0.16285932\n",
      "Iteration 466, loss = 0.16274222\n",
      "Iteration 467, loss = 0.16262573\n",
      "Iteration 468, loss = 0.16250986\n",
      "Iteration 469, loss = 0.16239459\n",
      "Iteration 470, loss = 0.16227992\n",
      "Iteration 471, loss = 0.16216586\n",
      "Iteration 472, loss = 0.16205238\n",
      "Iteration 473, loss = 0.16193950\n",
      "Iteration 474, loss = 0.16182719\n",
      "Iteration 475, loss = 0.16171547\n",
      "Iteration 476, loss = 0.16160433\n",
      "Iteration 477, loss = 0.16149375\n",
      "Iteration 478, loss = 0.16138374\n",
      "Iteration 479, loss = 0.16127429\n",
      "Iteration 480, loss = 0.16116540\n",
      "Iteration 481, loss = 0.16105706\n",
      "Iteration 482, loss = 0.16094927\n",
      "Iteration 483, loss = 0.16084203\n",
      "Iteration 484, loss = 0.16073533\n",
      "Iteration 485, loss = 0.16062916\n",
      "Iteration 486, loss = 0.16052353\n",
      "Iteration 487, loss = 0.16041842\n",
      "Iteration 488, loss = 0.16031384\n",
      "Iteration 489, loss = 0.16020978\n",
      "Iteration 490, loss = 0.16010623\n",
      "Iteration 491, loss = 0.16000320\n",
      "Iteration 492, loss = 0.15990068\n",
      "Iteration 493, loss = 0.15979866\n",
      "Iteration 494, loss = 0.15969714\n",
      "Iteration 495, loss = 0.15959612\n",
      "Iteration 496, loss = 0.15949559\n",
      "Iteration 497, loss = 0.15939555\n",
      "Iteration 498, loss = 0.15929600\n",
      "Iteration 499, loss = 0.15919693\n",
      "Iteration 500, loss = 0.15909833\n",
      "Iteration 501, loss = 0.15900022\n",
      "Iteration 502, loss = 0.15890257\n",
      "Iteration 503, loss = 0.15880539\n",
      "Iteration 504, loss = 0.15870868\n",
      "Iteration 505, loss = 0.15861242\n",
      "Iteration 506, loss = 0.15851663\n",
      "Iteration 507, loss = 0.15842128\n",
      "Iteration 508, loss = 0.15832639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Y.shape[1]\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,4),activation='tanh',solver='sgd',alpha=0,learning_rate_init=0.01,max_iter=2000,batch_size=m,shuffle=False,momentum=0,verbose=True)\n",
    "mlp.fit(X.T,Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights and biases\n",
      "W1: [[-0.48691103  0.91946446 -0.02454228]\n",
      " [ 0.47748217  0.42864578 -0.19787191]\n",
      " [ 0.40134923  0.14995668  0.00688993]\n",
      " [ 0.38780246  0.16899297  0.61408595]]\n",
      "b1: [[ 0.48443609]\n",
      " [ 0.61062761]\n",
      " [-0.91729079]\n",
      " [ 0.68565068]]\n",
      "W2: [[ 0.50797165  0.60230345  0.61599612 -0.87787813]\n",
      " [-0.42840833  0.49984181  0.98151956 -0.04637781]\n",
      " [-0.23230157 -0.6320469   0.35469939 -0.4044402 ]\n",
      " [-0.80300057 -0.32756874  0.62673365 -0.31788066]]\n",
      "b2: [[-0.26892527]\n",
      " [-0.37571438]\n",
      " [-0.25252441]\n",
      " [-0.31019441]]\n",
      "W3: [[ 0.66006332  0.6855996   0.39696308 -0.12668798]]\n",
      "b3: [[-1.45236886]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",mlp.coefs_[0].T)\n",
    "print(\"b1:\",mlp.intercepts_[0].reshape(-1,1))\n",
    "print(\"W2:\",mlp.coefs_[1].T)\n",
    "print(\"b2:\",mlp.intercepts_[1].reshape(-1,1))\n",
    "print(\"W3:\",mlp.coefs_[2].T)\n",
    "print(\"b3:\",mlp.intercepts_[2].reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training a Deep Neural Network Using Backpropagation\n",
    "The following code implements backpropagation to train a deep neural network of 2 hidden layers with 4 neurons in each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the paramaters of the neural network with Xavier initialisation\n",
    "W = [np.random.randn(4,3)*np.sqrt(1/3),np.random.randn(4,4)*np.sqrt(1/4),np.random.randn(1,4)*np.sqrt(1/4)]\n",
    "b = [np.zeros((4,1)),np.zeros((4,1)),np.zeros((1,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last iteration: 508\n",
      "Losses: [0.17210746 0.17192895 0.17175116 0.17157407 0.17139768 0.17122199\n",
      " 0.171047   0.17087269 0.17069907 0.17052612]\n",
      "\n",
      "Weights and biases\n",
      "W1: [[ 0.42274488 -0.45033552 -0.79540808]\n",
      " [ 0.57997555 -0.82170331  0.08461769]\n",
      " [ 0.07099727  1.15501132 -0.25181053]\n",
      " [ 0.00661883 -0.00602965 -0.04531166]]\n",
      "b1: [[ 0.00948902]\n",
      " [ 0.2175426 ]\n",
      " [ 0.09406294]\n",
      " [-0.5058794 ]]\n",
      "W2: [[-0.08367674 -0.90048196 -0.40445887  1.28890518]\n",
      " [ 0.65865158  0.20861881 -0.16428688 -0.08435036]\n",
      " [ 0.31119039  0.41101153 -0.16052644  0.31454617]\n",
      " [ 0.58570103  0.26255889 -0.06516088 -0.98081398]]\n",
      "b2: [[-0.32328211]\n",
      " [-0.2997069 ]\n",
      " [ 0.12666848]\n",
      " [ 0.24372617]]\n",
      "W3: [[ 0.89551584  0.36908528 -0.2221427  -0.5839157 ]]\n",
      "b3: [[-1.00467257]]\n"
     ]
    }
   ],
   "source": [
    "# Updating parameters using gradient descent\n",
    "iter = 508\n",
    "lr = 0.01\n",
    "loss = np.array([])\n",
    "L = 3\n",
    "\n",
    "for i in np.arange(iter):\n",
    "    # Forward propagation\n",
    "    Z = []\n",
    "    A = []\n",
    "    for l in np.arange(L):\n",
    "        Z.append(W[l]@A[l-1]+b[l] if l> 0 else W[l]@X+b[l])\n",
    "        A.append(1/(1+np.exp(-Z[l])) if l==L-1 else np.tanh(Z[l]))\n",
    "\n",
    "    # Back propagation\n",
    "    dZ = [0]*L\n",
    "    dA = [0]*L\n",
    "    dW = [0]*L\n",
    "    db = [0]*L\n",
    "    \n",
    "    for l in np.arange(L-1,-1,-1):\n",
    "        dZ[l] = A[l]-Y if l==L-1 else dA[l]*(1-np.tanh(Z[l])**2)\n",
    "        dA[l-1] = W[l].T@dZ[l] if l> 0 else 0\n",
    "        dW[l] = 1/m*dZ[l]@A[l-1].T if l>0 else 1/m*dZ[l]@X.T\n",
    "        db[l] = 1/m*np.sum(dZ[l],axis=1,keepdims=True)\n",
    "        W[l] -= lr*dW[l]\n",
    "        b[l] -= lr*db[l]\n",
    "\n",
    "    current_loss = -1/m*(Y@np.log(A[L-1]).T+(1-Y)@np.log(1-A[L-1]).T)\n",
    "    loss = np.append(loss,current_loss)\n",
    "\n",
    "print(\"Last iteration:\",i+1)\n",
    "print(\"Losses:\",loss[-10:])\n",
    "\n",
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",W[0])\n",
    "print(\"b1:\",b[0])\n",
    "print(\"W2:\",W[1])\n",
    "print(\"b2:\",b[1])\n",
    "print(\"W3:\",W[2])\n",
    "print(\"b3:\",b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the cost function against the number of iterations\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = plt.figure(figsize=[7,7])\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(np.arange(iter)+1,loss)\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_xlabel('Iterations')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gradient checking to verify the custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00117267,  0.00186388, -0.00370942, -0.00223187, -0.00152015,\n",
       "       -0.0069052 , -0.00088115,  0.00010101, -0.00090408, -0.00014081,\n",
       "       -0.03007766,  0.01428399, -0.00211325, -0.01976677, -0.00516577,\n",
       "        0.02516481,  0.00093391,  0.01261441, -0.00686522, -0.00696084,\n",
       "        0.00382493,  0.01254907, -0.00467193, -0.01068437, -0.00105854,\n",
       "       -0.00712181,  0.0021668 ,  0.00673445,  0.00300364, -0.01143924,\n",
       "        0.00478057,  0.01003912,  0.0146153 ,  0.02300793, -0.01437657,\n",
       "       -0.02082745, -0.06527548, -0.00044569,  0.01929882,  0.05150034,\n",
       "        0.07307083])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the gradient vector of the cost function\n",
    "\n",
    "def calculate_dtheta(X,Y,W,b,L):\n",
    "    dtheta = np.array([])\n",
    "\n",
    "    Z = []\n",
    "    A = []\n",
    "    for l in np.arange(L):\n",
    "        Z.append(W[l]@A[l-1]+b[l] if l> 0 else W[l]@X+b[l])\n",
    "        A.append(1/(1+np.exp(-Z[l])) if l==L-1 else np.tanh(Z[l]))\n",
    "\n",
    "    dZ = [0]*L\n",
    "    dA = [0]*L\n",
    "    dW = [0]*L\n",
    "    db = [0]*L\n",
    "        \n",
    "    for l in np.arange(L-1,-1,-1):\n",
    "        dZ[l] = A[l]-Y if l==L-1 else dA[l]*(1-np.tanh(Z[l])**2)\n",
    "        dA[l-1] = W[l].T@dZ[l] if l> 0 else 0\n",
    "        dW[l] = 1/m*dZ[l]@A[l-1].T if l>0 else 1/m*dZ[l]@X.T\n",
    "        db[l] = 1/m*np.sum(dZ[l],axis=1,keepdims=True)\n",
    "        \n",
    "    for l in range(L):\n",
    "        dtheta = np.append(dtheta,dW[l].flatten())\n",
    "        dtheta = np.append(dtheta,db[l].flatten())\n",
    "    \n",
    "    return dtheta\n",
    "\n",
    "dtheta = calculate_dtheta(X,Y,W,b,L)\n",
    "dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.42274488, -0.45033552, -0.79540808],\n",
      "       [ 0.57997555, -0.82170331,  0.08461769],\n",
      "       [ 0.07099727,  1.15501132, -0.25181053],\n",
      "       [ 0.00661883, -0.00602965, -0.04531166]]), array([[ 0.00948902],\n",
      "       [ 0.2175426 ],\n",
      "       [ 0.09406294],\n",
      "       [-0.5058794 ]]), array([[-0.08367674, -0.90048196, -0.40445887,  1.28890518],\n",
      "       [ 0.65865158,  0.20861881, -0.16428688, -0.08435036],\n",
      "       [ 0.31119039,  0.41101153, -0.16052644,  0.31454617],\n",
      "       [ 0.58570103,  0.26255889, -0.06516088, -0.98081398]]), array([[-0.32328211],\n",
      "       [-0.2997069 ],\n",
      "       [ 0.12666848],\n",
      "       [ 0.24372617]]), array([[ 0.89551584,  0.36908528, -0.2221427 , -0.5839157 ]]), array([[-1.00467257]])]\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of all parameters\n",
    "P = b.copy()\n",
    "for l in range(L):\n",
    "    P.insert(2*l,W[l])\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, (3, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function to index an element in the list of parameters\n",
    "def loc(index,P):\n",
    "    i = 0\n",
    "    for p in P:\n",
    "        if index<p.size:\n",
    "            return i,np.unravel_index(index,p.shape)\n",
    "        else:\n",
    "            index -= p.size\n",
    "            i += 1\n",
    "\n",
    "loc(10,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17035385]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function to calculate the cost function\n",
    "\n",
    "def cost(index,epsilon,X,Y,P,L):\n",
    "    i = loc(index,P)\n",
    "    P[i[0]][i[1]] += epsilon\n",
    "\n",
    "    W = [P[i] for i in range(0,L*2,2)]\n",
    "    b = [P[i] for i in range(1,L*2,2)]\n",
    "\n",
    "    Z = []\n",
    "    A = []\n",
    "    for l in np.arange(L):\n",
    "        Z.append(W[l]@A[l-1]+b[l] if l> 0 else W[l]@X+b[l])\n",
    "        A.append(1/(1+np.exp(-Z[l])) if l==L-1 else np.tanh(Z[l]))\n",
    "    \n",
    "    P[i[0]][i[1]] -= epsilon\n",
    "    return -(Y@np.log(A[L-1]).T+(1-Y)@np.log(1-A[L-1]).T)/m\n",
    "\n",
    "cost(10,1e-7,X,Y,P,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the partial derivative of the cost function with respect to a parameter\n",
    "\n",
    "def partial(index,epsilon,X,Y,P,L):\n",
    "    return (cost(index,epsilon,X,Y,P,L)-cost(index,-1*epsilon,X,Y,P,L))/(2*epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00117267,  0.00186388, -0.00370942, -0.00223187, -0.00152015,\n",
       "       -0.0069052 , -0.00088115,  0.00010101, -0.00090408, -0.00014081,\n",
       "       -0.03007766,  0.01428399, -0.00211325, -0.01976677, -0.00516577,\n",
       "        0.02516481,  0.00093391,  0.01261441, -0.00686522, -0.00696085,\n",
       "        0.00382493,  0.01254907, -0.00467193, -0.01068437, -0.00105854,\n",
       "       -0.00712181,  0.0021668 ,  0.00673445,  0.00300364, -0.01143924,\n",
       "        0.00478057,  0.01003912,  0.0146153 ,  0.02300793, -0.01437657,\n",
       "       -0.02082745, -0.06527548, -0.00044569,  0.01929882,  0.05150034,\n",
       "        0.07307083])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the approximate gradient vector of the cost function\n",
    "\n",
    "dtheta_approx = np.array([])\n",
    "for index in range(41):\n",
    "    dtheta_approx = np.append(dtheta_approx,partial(index,1e-7,X,Y,P,L))\n",
    "\n",
    "dtheta_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0900483840911664e-09"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = np.linalg.norm(dtheta_approx - dtheta)/(np.linalg.norm(dtheta_approx) + np.linalg.norm(dtheta))\n",
    "G"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusion\n",
    "Similar values of loss from 2 & 3 for the same number of iterations and the low value of G from gradient checking indicates that the custom gradient descent implementation is correct. The weights and biases are different because the 2 models are randomly initialised during training and the loss function of the deep neural network has multiple maximia and minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
