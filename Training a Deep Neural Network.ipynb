{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Deep Neural Network\n",
    "The following code implements a deep neural network of 2 hidden layers with backpropagation using low-level libraries and compares it with a model generated by Scikit-learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Loading & Cleaning\n",
    "The data set contains credit card debt information about 10,000 customers and whether they defaulted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income\n",
       "0      No      No   729.526495  44361.625074\n",
       "1      No     Yes   817.180407  12106.134700\n",
       "2      No      No  1073.549164  31767.138947\n",
       "3      No      No   529.250605  35704.493935\n",
       "4      No      No   785.655883  38463.495879"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('Default.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and converting to NumPy arrays\n",
    "df['default']=df['default'].apply(lambda x: 0 if x=='No' else 1)\n",
    "df['student']=df['student'].apply(lambda x: 0 if x=='No' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "0        0        0   729.526495  44361.625074\n",
       "1        0        1   817.180407  12106.134700\n",
       "2        0        0  1073.549164  31767.138947\n",
       "3        0        0   529.250605  35704.493935\n",
       "4        0        0   785.655883  38463.495879"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   default  10000 non-null  int64  \n",
      " 1   student  10000 non-null  int64  \n",
      " 2   balance  10000 non-null  float64\n",
      " 3   income   10000 non-null  float64\n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 312.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.218835</td>\n",
       "      <td>0.813187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.037616</td>\n",
       "      <td>-1.605496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>-0.131212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.632893</td>\n",
       "      <td>0.164031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>0.370915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.255990</td>\n",
       "      <td>1.460366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.160044</td>\n",
       "      <td>-1.039014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>1.883565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.516742</td>\n",
       "      <td>0.236363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.311691</td>\n",
       "      <td>-1.248805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      default  student   balance    income\n",
       "0           0        0 -0.218835  0.813187\n",
       "1           0        1 -0.037616 -1.605496\n",
       "2           0        0  0.492410 -0.131212\n",
       "3           0        0 -0.632893  0.164031\n",
       "4           0        0 -0.102791  0.370915\n",
       "...       ...      ...       ...       ...\n",
       "9995        0        0 -0.255990  1.460366\n",
       "9996        0        0 -0.160044 -1.039014\n",
       "9997        0        0  0.020751  1.883565\n",
       "9998        0        0  1.516742  0.236363\n",
       "9999        0        1 -1.311691 -1.248805\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['balance','income']] = scaler.fit_transform(df[['balance','income']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['default'].to_numpy().reshape(-1,1)\n",
    "X = df.drop(columns=['default']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (10000, 1)\n",
      "Shape of X: (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 10000)\n",
      "Shape of X: (3, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = X.T\n",
    "Y = Y.T\n",
    "\n",
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training a Deep Neural Network Using Scikit-learn\n",
    "The following code trains a deep neural network of 2 hidden layers with 4 neurons in each hidden layer using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1118: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.98469142\n",
      "Iteration 2, loss = 0.97393507\n",
      "Iteration 3, loss = 0.96335254\n",
      "Iteration 4, loss = 0.95294109\n",
      "Iteration 5, loss = 0.94269804\n",
      "Iteration 6, loss = 0.93262069\n",
      "Iteration 7, loss = 0.92270639\n",
      "Iteration 8, loss = 0.91295249\n",
      "Iteration 9, loss = 0.90335639\n",
      "Iteration 10, loss = 0.89391548\n",
      "Iteration 11, loss = 0.88462720\n",
      "Iteration 12, loss = 0.87548900\n",
      "Iteration 13, loss = 0.86649837\n",
      "Iteration 14, loss = 0.85765282\n",
      "Iteration 15, loss = 0.84894988\n",
      "Iteration 16, loss = 0.84038712\n",
      "Iteration 17, loss = 0.83196214\n",
      "Iteration 18, loss = 0.82367257\n",
      "Iteration 19, loss = 0.81551606\n",
      "Iteration 20, loss = 0.80749030\n",
      "Iteration 21, loss = 0.79959301\n",
      "Iteration 22, loss = 0.79182194\n",
      "Iteration 23, loss = 0.78417488\n",
      "Iteration 24, loss = 0.77664963\n",
      "Iteration 25, loss = 0.76924404\n",
      "Iteration 26, loss = 0.76195600\n",
      "Iteration 27, loss = 0.75478341\n",
      "Iteration 28, loss = 0.74772422\n",
      "Iteration 29, loss = 0.74077640\n",
      "Iteration 30, loss = 0.73393796\n",
      "Iteration 31, loss = 0.72720693\n",
      "Iteration 32, loss = 0.72058140\n",
      "Iteration 33, loss = 0.71405947\n",
      "Iteration 34, loss = 0.70763927\n",
      "Iteration 35, loss = 0.70131897\n",
      "Iteration 36, loss = 0.69509677\n",
      "Iteration 37, loss = 0.68897089\n",
      "Iteration 38, loss = 0.68293961\n",
      "Iteration 39, loss = 0.67700121\n",
      "Iteration 40, loss = 0.67115401\n",
      "Iteration 41, loss = 0.66539637\n",
      "Iteration 42, loss = 0.65972667\n",
      "Iteration 43, loss = 0.65414332\n",
      "Iteration 44, loss = 0.64864475\n",
      "Iteration 45, loss = 0.64322945\n",
      "Iteration 46, loss = 0.63789590\n",
      "Iteration 47, loss = 0.63264263\n",
      "Iteration 48, loss = 0.62746819\n",
      "Iteration 49, loss = 0.62237117\n",
      "Iteration 50, loss = 0.61735017\n",
      "Iteration 51, loss = 0.61240382\n",
      "Iteration 52, loss = 0.60753078\n",
      "Iteration 53, loss = 0.60272974\n",
      "Iteration 54, loss = 0.59799941\n",
      "Iteration 55, loss = 0.59333851\n",
      "Iteration 56, loss = 0.58874581\n",
      "Iteration 57, loss = 0.58422010\n",
      "Iteration 58, loss = 0.57976017\n",
      "Iteration 59, loss = 0.57536486\n",
      "Iteration 60, loss = 0.57103302\n",
      "Iteration 61, loss = 0.56676352\n",
      "Iteration 62, loss = 0.56255526\n",
      "Iteration 63, loss = 0.55840716\n",
      "Iteration 64, loss = 0.55431815\n",
      "Iteration 65, loss = 0.55028720\n",
      "Iteration 66, loss = 0.54631328\n",
      "Iteration 67, loss = 0.54239540\n",
      "Iteration 68, loss = 0.53853257\n",
      "Iteration 69, loss = 0.53472383\n",
      "Iteration 70, loss = 0.53096824\n",
      "Iteration 71, loss = 0.52726488\n",
      "Iteration 72, loss = 0.52361284\n",
      "Iteration 73, loss = 0.52001123\n",
      "Iteration 74, loss = 0.51645918\n",
      "Iteration 75, loss = 0.51295583\n",
      "Iteration 76, loss = 0.50950036\n",
      "Iteration 77, loss = 0.50609193\n",
      "Iteration 78, loss = 0.50272975\n",
      "Iteration 79, loss = 0.49941302\n",
      "Iteration 80, loss = 0.49614096\n",
      "Iteration 81, loss = 0.49291284\n",
      "Iteration 82, loss = 0.48972788\n",
      "Iteration 83, loss = 0.48658538\n",
      "Iteration 84, loss = 0.48348461\n",
      "Iteration 85, loss = 0.48042487\n",
      "Iteration 86, loss = 0.47740547\n",
      "Iteration 87, loss = 0.47442574\n",
      "Iteration 88, loss = 0.47148503\n",
      "Iteration 89, loss = 0.46858266\n",
      "Iteration 90, loss = 0.46571803\n",
      "Iteration 91, loss = 0.46289049\n",
      "Iteration 92, loss = 0.46009943\n",
      "Iteration 93, loss = 0.45734426\n",
      "Iteration 94, loss = 0.45462440\n",
      "Iteration 95, loss = 0.45193925\n",
      "Iteration 96, loss = 0.44928825\n",
      "Iteration 97, loss = 0.44667085\n",
      "Iteration 98, loss = 0.44408651\n",
      "Iteration 99, loss = 0.44153468\n",
      "Iteration 100, loss = 0.43901484\n",
      "Iteration 101, loss = 0.43652649\n",
      "Iteration 102, loss = 0.43406911\n",
      "Iteration 103, loss = 0.43164220\n",
      "Iteration 104, loss = 0.42924529\n",
      "Iteration 105, loss = 0.42687789\n",
      "Iteration 106, loss = 0.42453954\n",
      "Iteration 107, loss = 0.42222978\n",
      "Iteration 108, loss = 0.41994815\n",
      "Iteration 109, loss = 0.41769421\n",
      "Iteration 110, loss = 0.41546754\n",
      "Iteration 111, loss = 0.41326769\n",
      "Iteration 112, loss = 0.41109427\n",
      "Iteration 113, loss = 0.40894684\n",
      "Iteration 114, loss = 0.40682502\n",
      "Iteration 115, loss = 0.40472840\n",
      "Iteration 116, loss = 0.40265660\n",
      "Iteration 117, loss = 0.40060923\n",
      "Iteration 118, loss = 0.39858593\n",
      "Iteration 119, loss = 0.39658632\n",
      "Iteration 120, loss = 0.39461004\n",
      "Iteration 121, loss = 0.39265674\n",
      "Iteration 122, loss = 0.39072607\n",
      "Iteration 123, loss = 0.38881768\n",
      "Iteration 124, loss = 0.38693125\n",
      "Iteration 125, loss = 0.38506645\n",
      "Iteration 126, loss = 0.38322294\n",
      "Iteration 127, loss = 0.38140041\n",
      "Iteration 128, loss = 0.37959855\n",
      "Iteration 129, loss = 0.37781706\n",
      "Iteration 130, loss = 0.37605562\n",
      "Iteration 131, loss = 0.37431395\n",
      "Iteration 132, loss = 0.37259176\n",
      "Iteration 133, loss = 0.37088875\n",
      "Iteration 134, loss = 0.36920465\n",
      "Iteration 135, loss = 0.36753918\n",
      "Iteration 136, loss = 0.36589208\n",
      "Iteration 137, loss = 0.36426307\n",
      "Iteration 138, loss = 0.36265189\n",
      "Iteration 139, loss = 0.36105829\n",
      "Iteration 140, loss = 0.35948202\n",
      "Iteration 141, loss = 0.35792282\n",
      "Iteration 142, loss = 0.35638045\n",
      "Iteration 143, loss = 0.35485467\n",
      "Iteration 144, loss = 0.35334525\n",
      "Iteration 145, loss = 0.35185196\n",
      "Iteration 146, loss = 0.35037456\n",
      "Iteration 147, loss = 0.34891284\n",
      "Iteration 148, loss = 0.34746657\n",
      "Iteration 149, loss = 0.34603553\n",
      "Iteration 150, loss = 0.34461952\n",
      "Iteration 151, loss = 0.34321833\n",
      "Iteration 152, loss = 0.34183175\n",
      "Iteration 153, loss = 0.34045957\n",
      "Iteration 154, loss = 0.33910160\n",
      "Iteration 155, loss = 0.33775765\n",
      "Iteration 156, loss = 0.33642751\n",
      "Iteration 157, loss = 0.33511101\n",
      "Iteration 158, loss = 0.33380796\n",
      "Iteration 159, loss = 0.33251816\n",
      "Iteration 160, loss = 0.33124146\n",
      "Iteration 161, loss = 0.32997765\n",
      "Iteration 162, loss = 0.32872659\n",
      "Iteration 163, loss = 0.32748808\n",
      "Iteration 164, loss = 0.32626197\n",
      "Iteration 165, loss = 0.32504808\n",
      "Iteration 166, loss = 0.32384626\n",
      "Iteration 167, loss = 0.32265634\n",
      "Iteration 168, loss = 0.32147818\n",
      "Iteration 169, loss = 0.32031160\n",
      "Iteration 170, loss = 0.31915646\n",
      "Iteration 171, loss = 0.31801261\n",
      "Iteration 172, loss = 0.31687991\n",
      "Iteration 173, loss = 0.31575819\n",
      "Iteration 174, loss = 0.31464734\n",
      "Iteration 175, loss = 0.31354719\n",
      "Iteration 176, loss = 0.31245761\n",
      "Iteration 177, loss = 0.31137848\n",
      "Iteration 178, loss = 0.31030964\n",
      "Iteration 179, loss = 0.30925098\n",
      "Iteration 180, loss = 0.30820235\n",
      "Iteration 181, loss = 0.30716364\n",
      "Iteration 182, loss = 0.30613471\n",
      "Iteration 183, loss = 0.30511545\n",
      "Iteration 184, loss = 0.30410572\n",
      "Iteration 185, loss = 0.30310542\n",
      "Iteration 186, loss = 0.30211441\n",
      "Iteration 187, loss = 0.30113259\n",
      "Iteration 188, loss = 0.30015984\n",
      "Iteration 189, loss = 0.29919604\n",
      "Iteration 190, loss = 0.29824109\n",
      "Iteration 191, loss = 0.29729487\n",
      "Iteration 192, loss = 0.29635728\n",
      "Iteration 193, loss = 0.29542822\n",
      "Iteration 194, loss = 0.29450756\n",
      "Iteration 195, loss = 0.29359522\n",
      "Iteration 196, loss = 0.29269110\n",
      "Iteration 197, loss = 0.29179508\n",
      "Iteration 198, loss = 0.29090707\n",
      "Iteration 199, loss = 0.29002699\n",
      "Iteration 200, loss = 0.28915472\n",
      "Iteration 201, loss = 0.28829017\n",
      "Iteration 202, loss = 0.28743326\n",
      "Iteration 203, loss = 0.28658389\n",
      "Iteration 204, loss = 0.28574197\n",
      "Iteration 205, loss = 0.28490742\n",
      "Iteration 206, loss = 0.28408014\n",
      "Iteration 207, loss = 0.28326004\n",
      "Iteration 208, loss = 0.28244706\n",
      "Iteration 209, loss = 0.28164109\n",
      "Iteration 210, loss = 0.28084205\n",
      "Iteration 211, loss = 0.28004988\n",
      "Iteration 212, loss = 0.27926448\n",
      "Iteration 213, loss = 0.27848578\n",
      "Iteration 214, loss = 0.27771369\n",
      "Iteration 215, loss = 0.27694815\n",
      "Iteration 216, loss = 0.27618908\n",
      "Iteration 217, loss = 0.27543640\n",
      "Iteration 218, loss = 0.27469004\n",
      "Iteration 219, loss = 0.27394992\n",
      "Iteration 220, loss = 0.27321598\n",
      "Iteration 221, loss = 0.27248814\n",
      "Iteration 222, loss = 0.27176634\n",
      "Iteration 223, loss = 0.27105051\n",
      "Iteration 224, loss = 0.27034057\n",
      "Iteration 225, loss = 0.26963647\n",
      "Iteration 226, loss = 0.26893814\n",
      "Iteration 227, loss = 0.26824551\n",
      "Iteration 228, loss = 0.26755851\n",
      "Iteration 229, loss = 0.26687710\n",
      "Iteration 230, loss = 0.26620120\n",
      "Iteration 231, loss = 0.26553075\n",
      "Iteration 232, loss = 0.26486569\n",
      "Iteration 233, loss = 0.26420597\n",
      "Iteration 234, loss = 0.26355152\n",
      "Iteration 235, loss = 0.26290228\n",
      "Iteration 236, loss = 0.26225821\n",
      "Iteration 237, loss = 0.26161924\n",
      "Iteration 238, loss = 0.26098532\n",
      "Iteration 239, loss = 0.26035639\n",
      "Iteration 240, loss = 0.25973240\n",
      "Iteration 241, loss = 0.25911329\n",
      "Iteration 242, loss = 0.25849902\n",
      "Iteration 243, loss = 0.25788953\n",
      "Iteration 244, loss = 0.25728477\n",
      "Iteration 245, loss = 0.25668469\n",
      "Iteration 246, loss = 0.25608923\n",
      "Iteration 247, loss = 0.25549836\n",
      "Iteration 248, loss = 0.25491202\n",
      "Iteration 249, loss = 0.25433016\n",
      "Iteration 250, loss = 0.25375274\n",
      "Iteration 251, loss = 0.25317971\n",
      "Iteration 252, loss = 0.25261103\n",
      "Iteration 253, loss = 0.25204664\n",
      "Iteration 254, loss = 0.25148651\n",
      "Iteration 255, loss = 0.25093058\n",
      "Iteration 256, loss = 0.25037882\n",
      "Iteration 257, loss = 0.24983119\n",
      "Iteration 258, loss = 0.24928763\n",
      "Iteration 259, loss = 0.24874812\n",
      "Iteration 260, loss = 0.24821260\n",
      "Iteration 261, loss = 0.24768103\n",
      "Iteration 262, loss = 0.24715338\n",
      "Iteration 263, loss = 0.24662961\n",
      "Iteration 264, loss = 0.24610967\n",
      "Iteration 265, loss = 0.24559353\n",
      "Iteration 266, loss = 0.24508115\n",
      "Iteration 267, loss = 0.24457249\n",
      "Iteration 268, loss = 0.24406751\n",
      "Iteration 269, loss = 0.24356618\n",
      "Iteration 270, loss = 0.24306846\n",
      "Iteration 271, loss = 0.24257432\n",
      "Iteration 272, loss = 0.24208371\n",
      "Iteration 273, loss = 0.24159661\n",
      "Iteration 274, loss = 0.24111297\n",
      "Iteration 275, loss = 0.24063277\n",
      "Iteration 276, loss = 0.24015597\n",
      "Iteration 277, loss = 0.23968254\n",
      "Iteration 278, loss = 0.23921244\n",
      "Iteration 279, loss = 0.23874564\n",
      "Iteration 280, loss = 0.23828211\n",
      "Iteration 281, loss = 0.23782182\n",
      "Iteration 282, loss = 0.23736474\n",
      "Iteration 283, loss = 0.23691083\n",
      "Iteration 284, loss = 0.23646007\n",
      "Iteration 285, loss = 0.23601242\n",
      "Iteration 286, loss = 0.23556785\n",
      "Iteration 287, loss = 0.23512634\n",
      "Iteration 288, loss = 0.23468785\n",
      "Iteration 289, loss = 0.23425236\n",
      "Iteration 290, loss = 0.23381984\n",
      "Iteration 291, loss = 0.23339026\n",
      "Iteration 292, loss = 0.23296359\n",
      "Iteration 293, loss = 0.23253981\n",
      "Iteration 294, loss = 0.23211888\n",
      "Iteration 295, loss = 0.23170079\n",
      "Iteration 296, loss = 0.23128549\n",
      "Iteration 297, loss = 0.23087298\n",
      "Iteration 298, loss = 0.23046322\n",
      "Iteration 299, loss = 0.23005618\n",
      "Iteration 300, loss = 0.22965185\n",
      "Iteration 301, loss = 0.22925019\n",
      "Iteration 302, loss = 0.22885118\n",
      "Iteration 303, loss = 0.22845480\n",
      "Iteration 304, loss = 0.22806102\n",
      "Iteration 305, loss = 0.22766982\n",
      "Iteration 306, loss = 0.22728118\n",
      "Iteration 307, loss = 0.22689506\n",
      "Iteration 308, loss = 0.22651146\n",
      "Iteration 309, loss = 0.22613034\n",
      "Iteration 310, loss = 0.22575169\n",
      "Iteration 311, loss = 0.22537547\n",
      "Iteration 312, loss = 0.22500168\n",
      "Iteration 313, loss = 0.22463028\n",
      "Iteration 314, loss = 0.22426126\n",
      "Iteration 315, loss = 0.22389460\n",
      "Iteration 316, loss = 0.22353027\n",
      "Iteration 317, loss = 0.22316825\n",
      "Iteration 318, loss = 0.22280852\n",
      "Iteration 319, loss = 0.22245106\n",
      "Iteration 320, loss = 0.22209586\n",
      "Iteration 321, loss = 0.22174289\n",
      "Iteration 322, loss = 0.22139213\n",
      "Iteration 323, loss = 0.22104356\n",
      "Iteration 324, loss = 0.22069717\n",
      "Iteration 325, loss = 0.22035293\n",
      "Iteration 326, loss = 0.22001082\n",
      "Iteration 327, loss = 0.21967084\n",
      "Iteration 328, loss = 0.21933295\n",
      "Iteration 329, loss = 0.21899714\n",
      "Iteration 330, loss = 0.21866340\n",
      "Iteration 331, loss = 0.21833170\n",
      "Iteration 332, loss = 0.21800203\n",
      "Iteration 333, loss = 0.21767436\n",
      "Iteration 334, loss = 0.21734869\n",
      "Iteration 335, loss = 0.21702499\n",
      "Iteration 336, loss = 0.21670326\n",
      "Iteration 337, loss = 0.21638346\n",
      "Iteration 338, loss = 0.21606559\n",
      "Iteration 339, loss = 0.21574963\n",
      "Iteration 340, loss = 0.21543556\n",
      "Iteration 341, loss = 0.21512337\n",
      "Iteration 342, loss = 0.21481304\n",
      "Iteration 343, loss = 0.21450455\n",
      "Iteration 344, loss = 0.21419790\n",
      "Iteration 345, loss = 0.21389306\n",
      "Iteration 346, loss = 0.21359002\n",
      "Iteration 347, loss = 0.21328876\n",
      "Iteration 348, loss = 0.21298927\n",
      "Iteration 349, loss = 0.21269154\n",
      "Iteration 350, loss = 0.21239555\n",
      "Iteration 351, loss = 0.21210129\n",
      "Iteration 352, loss = 0.21180873\n",
      "Iteration 353, loss = 0.21151788\n",
      "Iteration 354, loss = 0.21122870\n",
      "Iteration 355, loss = 0.21094120\n",
      "Iteration 356, loss = 0.21065536\n",
      "Iteration 357, loss = 0.21037116\n",
      "Iteration 358, loss = 0.21008858\n",
      "Iteration 359, loss = 0.20980763\n",
      "Iteration 360, loss = 0.20952827\n",
      "Iteration 361, loss = 0.20925051\n",
      "Iteration 362, loss = 0.20897433\n",
      "Iteration 363, loss = 0.20869970\n",
      "Iteration 364, loss = 0.20842664\n",
      "Iteration 365, loss = 0.20815511\n",
      "Iteration 366, loss = 0.20788511\n",
      "Iteration 367, loss = 0.20761662\n",
      "Iteration 368, loss = 0.20734964\n",
      "Iteration 369, loss = 0.20708415\n",
      "Iteration 370, loss = 0.20682014\n",
      "Iteration 371, loss = 0.20655759\n",
      "Iteration 372, loss = 0.20629651\n",
      "Iteration 373, loss = 0.20603686\n",
      "Iteration 374, loss = 0.20577865\n",
      "Iteration 375, loss = 0.20552187\n",
      "Iteration 376, loss = 0.20526649\n",
      "Iteration 377, loss = 0.20501251\n",
      "Iteration 378, loss = 0.20475993\n",
      "Iteration 379, loss = 0.20450872\n",
      "Iteration 380, loss = 0.20425888\n",
      "Iteration 381, loss = 0.20401039\n",
      "Iteration 382, loss = 0.20376325\n",
      "Iteration 383, loss = 0.20351745\n",
      "Iteration 384, loss = 0.20327297\n",
      "Iteration 385, loss = 0.20302981\n",
      "Iteration 386, loss = 0.20278796\n",
      "Iteration 387, loss = 0.20254740\n",
      "Iteration 388, loss = 0.20230812\n",
      "Iteration 389, loss = 0.20207012\n",
      "Iteration 390, loss = 0.20183339\n",
      "Iteration 391, loss = 0.20159791\n",
      "Iteration 392, loss = 0.20136369\n",
      "Iteration 393, loss = 0.20113069\n",
      "Iteration 394, loss = 0.20089893\n",
      "Iteration 395, loss = 0.20066839\n",
      "Iteration 396, loss = 0.20043905\n",
      "Iteration 397, loss = 0.20021092\n",
      "Iteration 398, loss = 0.19998397\n",
      "Iteration 399, loss = 0.19975821\n",
      "Iteration 400, loss = 0.19953363\n",
      "Iteration 401, loss = 0.19931021\n",
      "Iteration 402, loss = 0.19908794\n",
      "Iteration 403, loss = 0.19886682\n",
      "Iteration 404, loss = 0.19864685\n",
      "Iteration 405, loss = 0.19842800\n",
      "Iteration 406, loss = 0.19821028\n",
      "Iteration 407, loss = 0.19799367\n",
      "Iteration 408, loss = 0.19777817\n",
      "Iteration 409, loss = 0.19756376\n",
      "Iteration 410, loss = 0.19735045\n",
      "Iteration 411, loss = 0.19713822\n",
      "Iteration 412, loss = 0.19692706\n",
      "Iteration 413, loss = 0.19671696\n",
      "Iteration 414, loss = 0.19650793\n",
      "Iteration 415, loss = 0.19629995\n",
      "Iteration 416, loss = 0.19609301\n",
      "Iteration 417, loss = 0.19588711\n",
      "Iteration 418, loss = 0.19568223\n",
      "Iteration 419, loss = 0.19547838\n",
      "Iteration 420, loss = 0.19527554\n",
      "Iteration 421, loss = 0.19507371\n",
      "Iteration 422, loss = 0.19487287\n",
      "Iteration 423, loss = 0.19467303\n",
      "Iteration 424, loss = 0.19447417\n",
      "Iteration 425, loss = 0.19427629\n",
      "Iteration 426, loss = 0.19407938\n",
      "Iteration 427, loss = 0.19388344\n",
      "Iteration 428, loss = 0.19368845\n",
      "Iteration 429, loss = 0.19349442\n",
      "Iteration 430, loss = 0.19330133\n",
      "Iteration 431, loss = 0.19310917\n",
      "Iteration 432, loss = 0.19291795\n",
      "Iteration 433, loss = 0.19272764\n",
      "Iteration 434, loss = 0.19253826\n",
      "Iteration 435, loss = 0.19234979\n",
      "Iteration 436, loss = 0.19216222\n",
      "Iteration 437, loss = 0.19197556\n",
      "Iteration 438, loss = 0.19178978\n",
      "Iteration 439, loss = 0.19160489\n",
      "Iteration 440, loss = 0.19142088\n",
      "Iteration 441, loss = 0.19123774\n",
      "Iteration 442, loss = 0.19105548\n",
      "Iteration 443, loss = 0.19087407\n",
      "Iteration 444, loss = 0.19069352\n",
      "Iteration 445, loss = 0.19051382\n",
      "Iteration 446, loss = 0.19033497\n",
      "Iteration 447, loss = 0.19015695\n",
      "Iteration 448, loss = 0.18997977\n",
      "Iteration 449, loss = 0.18980341\n",
      "Iteration 450, loss = 0.18962787\n",
      "Iteration 451, loss = 0.18945315\n",
      "Iteration 452, loss = 0.18927924\n",
      "Iteration 453, loss = 0.18910614\n",
      "Iteration 454, loss = 0.18893384\n",
      "Iteration 455, loss = 0.18876232\n",
      "Iteration 456, loss = 0.18859160\n",
      "Iteration 457, loss = 0.18842166\n",
      "Iteration 458, loss = 0.18825250\n",
      "Iteration 459, loss = 0.18808412\n",
      "Iteration 460, loss = 0.18791650\n",
      "Iteration 461, loss = 0.18774964\n",
      "Iteration 462, loss = 0.18758354\n",
      "Iteration 463, loss = 0.18741819\n",
      "Iteration 464, loss = 0.18725360\n",
      "Iteration 465, loss = 0.18708974\n",
      "Iteration 466, loss = 0.18692662\n",
      "Iteration 467, loss = 0.18676423\n",
      "Iteration 468, loss = 0.18660258\n",
      "Iteration 469, loss = 0.18644164\n",
      "Iteration 470, loss = 0.18628142\n",
      "Iteration 471, loss = 0.18612192\n",
      "Iteration 472, loss = 0.18596313\n",
      "Iteration 473, loss = 0.18580504\n",
      "Iteration 474, loss = 0.18564765\n",
      "Iteration 475, loss = 0.18549096\n",
      "Iteration 476, loss = 0.18533495\n",
      "Iteration 477, loss = 0.18517963\n",
      "Iteration 478, loss = 0.18502500\n",
      "Iteration 479, loss = 0.18487104\n",
      "Iteration 480, loss = 0.18471776\n",
      "Iteration 481, loss = 0.18456514\n",
      "Iteration 482, loss = 0.18441319\n",
      "Iteration 483, loss = 0.18426190\n",
      "Iteration 484, loss = 0.18411126\n",
      "Iteration 485, loss = 0.18396127\n",
      "Iteration 486, loss = 0.18381194\n",
      "Iteration 487, loss = 0.18366324\n",
      "Iteration 488, loss = 0.18351519\n",
      "Iteration 489, loss = 0.18336777\n",
      "Iteration 490, loss = 0.18322098\n",
      "Iteration 491, loss = 0.18307482\n",
      "Iteration 492, loss = 0.18292928\n",
      "Iteration 493, loss = 0.18278436\n",
      "Iteration 494, loss = 0.18264006\n",
      "Iteration 495, loss = 0.18249636\n",
      "Iteration 496, loss = 0.18235328\n",
      "Iteration 497, loss = 0.18221080\n",
      "Iteration 498, loss = 0.18206892\n",
      "Iteration 499, loss = 0.18192763\n",
      "Iteration 500, loss = 0.18178694\n",
      "Iteration 501, loss = 0.18164683\n",
      "Iteration 502, loss = 0.18150731\n",
      "Iteration 503, loss = 0.18136838\n",
      "Iteration 504, loss = 0.18123002\n",
      "Iteration 505, loss = 0.18109223\n",
      "Iteration 506, loss = 0.18095501\n",
      "Iteration 507, loss = 0.18081837\n",
      "Iteration 508, loss = 0.18068228\n",
      "Iteration 509, loss = 0.18054676\n",
      "Iteration 510, loss = 0.18041179\n",
      "Iteration 511, loss = 0.18027737\n",
      "Iteration 512, loss = 0.18014351\n",
      "Iteration 513, loss = 0.18001019\n",
      "Iteration 514, loss = 0.17987741\n",
      "Iteration 515, loss = 0.17974517\n",
      "Iteration 516, loss = 0.17961347\n",
      "Iteration 517, loss = 0.17948231\n",
      "Iteration 518, loss = 0.17935167\n",
      "Iteration 519, loss = 0.17922156\n",
      "Iteration 520, loss = 0.17909197\n",
      "Iteration 521, loss = 0.17896291\n",
      "Iteration 522, loss = 0.17883436\n",
      "Iteration 523, loss = 0.17870632\n",
      "Iteration 524, loss = 0.17857880\n",
      "Iteration 525, loss = 0.17845178\n",
      "Iteration 526, loss = 0.17832527\n",
      "Iteration 527, loss = 0.17819926\n",
      "Iteration 528, loss = 0.17807375\n",
      "Iteration 529, loss = 0.17794873\n",
      "Iteration 530, loss = 0.17782421\n",
      "Iteration 531, loss = 0.17770018\n",
      "Iteration 532, loss = 0.17757663\n",
      "Iteration 533, loss = 0.17745357\n",
      "Iteration 534, loss = 0.17733099\n",
      "Iteration 535, loss = 0.17720889\n",
      "Iteration 536, loss = 0.17708726\n",
      "Iteration 537, loss = 0.17696611\n",
      "Iteration 538, loss = 0.17684542\n",
      "Iteration 539, loss = 0.17672520\n",
      "Iteration 540, loss = 0.17660545\n",
      "Iteration 541, loss = 0.17648616\n",
      "Iteration 542, loss = 0.17636732\n",
      "Iteration 543, loss = 0.17624895\n",
      "Iteration 544, loss = 0.17613102\n",
      "Iteration 545, loss = 0.17601355\n",
      "Iteration 546, loss = 0.17589653\n",
      "Iteration 547, loss = 0.17577995\n",
      "Iteration 548, loss = 0.17566381\n",
      "Iteration 549, loss = 0.17554811\n",
      "Iteration 550, loss = 0.17543286\n",
      "Iteration 551, loss = 0.17531803\n",
      "Iteration 552, loss = 0.17520364\n",
      "Iteration 553, loss = 0.17508968\n",
      "Iteration 554, loss = 0.17497615\n",
      "Iteration 555, loss = 0.17486304\n",
      "Iteration 556, loss = 0.17475036\n",
      "Iteration 557, loss = 0.17463810\n",
      "Iteration 558, loss = 0.17452625\n",
      "Iteration 559, loss = 0.17441482\n",
      "Iteration 560, loss = 0.17430380\n",
      "Iteration 561, loss = 0.17419320\n",
      "Iteration 562, loss = 0.17408300\n",
      "Iteration 563, loss = 0.17397321\n",
      "Iteration 564, loss = 0.17386382\n",
      "Iteration 565, loss = 0.17375483\n",
      "Iteration 566, loss = 0.17364624\n",
      "Iteration 567, loss = 0.17353805\n",
      "Iteration 568, loss = 0.17343026\n",
      "Iteration 569, loss = 0.17332285\n",
      "Iteration 570, loss = 0.17321584\n",
      "Iteration 571, loss = 0.17310921\n",
      "Iteration 572, loss = 0.17300297\n",
      "Iteration 573, loss = 0.17289712\n",
      "Iteration 574, loss = 0.17279164\n",
      "Iteration 575, loss = 0.17268655\n",
      "Iteration 576, loss = 0.17258183\n",
      "Iteration 577, loss = 0.17247749\n",
      "Iteration 578, loss = 0.17237352\n",
      "Iteration 579, loss = 0.17226992\n",
      "Iteration 580, loss = 0.17216669\n",
      "Iteration 581, loss = 0.17206382\n",
      "Iteration 582, loss = 0.17196132\n",
      "Iteration 583, loss = 0.17185919\n",
      "Iteration 584, loss = 0.17175741\n",
      "Iteration 585, loss = 0.17165599\n",
      "Iteration 586, loss = 0.17155493\n",
      "Iteration 587, loss = 0.17145423\n",
      "Iteration 588, loss = 0.17135388\n",
      "Iteration 589, loss = 0.17125387\n",
      "Iteration 590, loss = 0.17115422\n",
      "Iteration 591, loss = 0.17105491\n",
      "Iteration 592, loss = 0.17095595\n",
      "Iteration 593, loss = 0.17085733\n",
      "Iteration 594, loss = 0.17075906\n",
      "Iteration 595, loss = 0.17066112\n",
      "Iteration 596, loss = 0.17056352\n",
      "Iteration 597, loss = 0.17046625\n",
      "Iteration 598, loss = 0.17036932\n",
      "Iteration 599, loss = 0.17027272\n",
      "Iteration 600, loss = 0.17017645\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Y.shape[1]\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,4),activation='tanh',solver='sgd',alpha=0,learning_rate_init=0.01,max_iter=2000,batch_size=m,shuffle=False,momentum=0,verbose=True)\n",
    "mlp.fit(X.T,Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights and biases\n",
      "W1: [[ 0.75192146  0.73648437  0.40574287]\n",
      " [ 0.35974089 -0.25625228  0.09145413]\n",
      " [-0.88722822 -0.83121018 -0.30095148]\n",
      " [-0.4673581   0.67789668  0.78962813]]\n",
      "b1: [[-0.08111665]\n",
      " [ 0.04187014]\n",
      " [ 0.15467396]\n",
      " [ 0.2383913 ]]\n",
      "W2: [[-0.5817917  -0.05550567 -0.26736525 -0.14597262]\n",
      " [-0.27966009  0.13048394 -0.14432222 -0.53108032]\n",
      " [ 0.08234017  0.48243911  0.44408451  0.05788336]\n",
      " [-0.37600303 -0.33525537  0.2884989  -0.23753565]]\n",
      "b2: [[ 0.74337523]\n",
      " [-0.15046751]\n",
      " [-0.66369201]\n",
      " [-0.17715084]]\n",
      "W3: [[-1.1562686  -0.258861    0.31821525  0.49027796]]\n",
      "b3: [[-1.41599055]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",mlp.coefs_[0].T)\n",
    "print(\"b1:\",mlp.intercepts_[0].reshape(-1,1))\n",
    "print(\"W2:\",mlp.coefs_[1].T)\n",
    "print(\"b2:\",mlp.intercepts_[1].reshape(-1,1))\n",
    "print(\"W3:\",mlp.coefs_[2].T)\n",
    "print(\"b3:\",mlp.intercepts_[2].reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training a Deep Neural Network Using Backpropagation\n",
    "The following code implements backpropagation to train a deep neural network of 2 hidden layers with 4 neurons in each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the paramaters of the neural network with Xavier initialisation\n",
    "W = [np.random.randn(4,3)*np.sqrt(1/3),np.random.randn(4,4)*np.sqrt(1/4),np.random.randn(1,4)*np.sqrt(1/4)]\n",
    "b = [np.zeros((4,1)),np.zeros((4,1)),np.zeros((1,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last iteration: 647\n",
      "Losses: [0.15031277 0.15023881 0.15016498 0.1500913  0.15001776 0.14994436\n",
      " 0.1498711  0.14979797 0.14972499 0.14965213]\n",
      "\n",
      "Weights and biases\n",
      "W1: [[-0.47212131 -0.16068759  0.14794861]\n",
      " [ 0.01109504 -0.12347557  0.40612833]\n",
      " [-0.69849706 -0.08003476  0.14600144]\n",
      " [-0.30272018 -0.50594741 -0.29904912]]\n",
      "b1: [[ 0.42484187]\n",
      " [ 0.05130814]\n",
      " [-0.34046101]\n",
      " [ 0.28858312]]\n",
      "W2: [[ 0.0373258   0.47558472 -0.73968589 -0.39146117]\n",
      " [-0.14578094 -0.55690607 -0.00375964  0.31473865]\n",
      " [-0.93130409 -0.66318398  1.1130561  -0.33913283]\n",
      " [-0.28125399  0.35492742  0.43370201 -0.15200551]]\n",
      "b2: [[-0.22822084]\n",
      " [ 0.2067205 ]\n",
      " [-0.56204863]\n",
      " [-0.23300232]]\n",
      "W3: [[ 0.25365679 -0.30719803  1.64647414  0.43117253]]\n",
      "b3: [[-0.8606525]]\n"
     ]
    }
   ],
   "source": [
    "# Updating parameters using gradient descent\n",
    "iter = 647\n",
    "lr = 0.01\n",
    "loss = np.array([])\n",
    "L = 3\n",
    "\n",
    "for i in np.arange(iter):\n",
    "    # Forward propagation\n",
    "    Z = []\n",
    "    A = []\n",
    "    for l in np.arange(L):\n",
    "        Z.append(W[l]@A[l-1]+b[l] if l> 0 else W[l]@X+b[l])\n",
    "        A.append(1/(1+np.exp(-Z[l])) if l==L-1 else np.tanh(Z[l]))\n",
    "\n",
    "    # Back propagation\n",
    "    dZ = [0]*L\n",
    "    dA = [0]*L\n",
    "    dW = [0]*L\n",
    "    db = [0]*L\n",
    "    \n",
    "    for l in np.arange(L-1,-1,-1):\n",
    "        dZ[l] = A[l]-Y if l==L-1 else dA[l]*(1-np.tanh(Z[l])**2)\n",
    "        dA[l-1] = W[l].T@dZ[l] if l> 0 else 0\n",
    "        dW[l] = 1/m*dZ[l]@A[l-1].T if l>0 else 1/m*dZ[l]@X.T\n",
    "        db[l] = 1/m*np.sum(dZ[l],axis=1,keepdims=True)\n",
    "        W[l] -= lr*dW[l]\n",
    "        b[l] -= lr*db[l]\n",
    "\n",
    "    current_loss = -1/m*(Y@np.log(A[L-1]).T+(1-Y)@np.log(1-A[L-1]).T)\n",
    "    loss = np.append(loss,current_loss)\n",
    "\n",
    "print(\"Last iteration:\",i+1)\n",
    "print(\"Losses:\",loss[-10:])\n",
    "\n",
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",W[0])\n",
    "print(\"b1:\",b[0])\n",
    "print(\"W2:\",W[1])\n",
    "print(\"b2:\",b[1])\n",
    "print(\"W3:\",W[2])\n",
    "print(\"b3:\",b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the cost function against the number of iterations\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = plt.figure(figsize=[7,7])\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(np.arange(iter)+1,loss)\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_xlabel('Iterations')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gradient checking to verify the custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00426023,  0.02755832,  0.00387759,  0.00124478,  0.00244373,\n",
       "        0.00347004,  0.0011717 , -0.02027639,  0.00163784, -0.00445076,\n",
       "        0.0084092 , -0.00181105, -0.00728694,  0.01000257,  0.00604911,\n",
       "       -0.01423322,  0.00471166,  0.00220914, -0.00391723,  0.00592337,\n",
       "       -0.00583757, -0.00313233,  0.00361917, -0.00690205,  0.00628534,\n",
       "        0.00356521, -0.00176716,  0.01466623,  0.00729127,  0.00413946,\n",
       "       -0.00414748,  0.008284  ,  0.01145364, -0.01226758,  0.007371  ,\n",
       "        0.01483166, -0.00367235,  0.00841552, -0.04344287, -0.02049016,\n",
       "        0.04478106])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the gradient vector of the cost function\n",
    "dtheta = np.array([])\n",
    "for l in range(L):\n",
    "    dtheta = np.append(dtheta,dW[l].flatten())\n",
    "    dtheta = np.append(dtheta,db[l].flatten())\n",
    "dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.47212131, -0.16068759,  0.14794861],\n",
      "       [ 0.01109504, -0.12347557,  0.40612833],\n",
      "       [-0.69849706, -0.08003476,  0.14600144],\n",
      "       [-0.30272018, -0.50594741, -0.29904912]]), array([[ 0.42484187],\n",
      "       [ 0.05130814],\n",
      "       [-0.34046101],\n",
      "       [ 0.28858312]]), array([[ 0.0373258 ,  0.47558472, -0.73968589, -0.39146117],\n",
      "       [-0.14578094, -0.55690607, -0.00375964,  0.31473865],\n",
      "       [-0.93130409, -0.66318398,  1.1130561 , -0.33913283],\n",
      "       [-0.28125399,  0.35492742,  0.43370201, -0.15200551]]), array([[-0.22822084],\n",
      "       [ 0.2067205 ],\n",
      "       [-0.56204863],\n",
      "       [-0.23300232]]), array([[ 0.25365679, -0.30719803,  1.64647414,  0.43117253]]), array([[-0.8606525]])]\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of all parameters\n",
    "P = b.copy()\n",
    "for l in range(L):\n",
    "    P.insert(2*l,W[l])\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, (3, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function to index an element in the list of parameters\n",
    "def loc(index,P):\n",
    "    i = 0\n",
    "    for p in P:\n",
    "        if index<p.size:\n",
    "            return i,np.unravel_index(index,p.shape)\n",
    "        else:\n",
    "            index -= p.size\n",
    "            i += 1\n",
    "\n",
    "loc(10,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14957942]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function to calculate the cost function\n",
    "\n",
    "def cost(index,epsilon,X,Y,P,L):\n",
    "    i = loc(index,P)\n",
    "    P[i[0]][i[1]] += epsilon\n",
    "\n",
    "    W = [P[i] for i in range(0,L*2,2)]\n",
    "    b = [P[i] for i in range(1,L*2,2)]\n",
    "\n",
    "    Z = []\n",
    "    A = []\n",
    "    for l in np.arange(L):\n",
    "        Z.append(W[l]@A[l-1]+b[l] if l> 0 else W[l]@X+b[l])\n",
    "        A.append(1/(1+np.exp(-Z[l])) if l==L-1 else np.tanh(Z[l]))\n",
    "    \n",
    "    P[i[0]][i[1]] -= epsilon\n",
    "    return -1/m*(Y@np.log(A[L-1]).T+(1-Y)@np.log(1-A[L-1]).T)\n",
    "\n",
    "cost(10,1e-7,X,Y,P,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the partial derivative of the cost function with respect to a parameter\n",
    "\n",
    "def partial(index,epsilon,X,Y,P,L):\n",
    "    return (cost(index,epsilon,X,Y,P,L)-cost(index,-1*epsilon,X,Y,P,L))/(2*epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00424866,  0.02758993,  0.00387583,  0.00124777,  0.00246551,\n",
       "        0.00346638,  0.00116509, -0.02031098,  0.00162978, -0.00444397,\n",
       "        0.00841751, -0.00181287, -0.00724725,  0.01000625,  0.00601047,\n",
       "       -0.01421209,  0.00471   ,  0.00220715, -0.00391221,  0.00592254,\n",
       "       -0.00583596, -0.00313003,  0.00361431, -0.00690132,  0.00628907,\n",
       "        0.00356982, -0.00174518,  0.01467497,  0.00728865,  0.00413611,\n",
       "       -0.00414115,  0.008284  ,  0.01143585, -0.01224858,  0.00731984,\n",
       "        0.01480648, -0.00368096,  0.00841289, -0.04338572, -0.02047281,\n",
       "        0.04470332])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the approximate gradient vector of the cost function\n",
    "\n",
    "dtheta_approx = np.array([])\n",
    "for index in range(41):\n",
    "    dtheta_approx = np.append(dtheta_approx,partial(index,1e-7,X,Y,P,L))\n",
    "\n",
    "dtheta_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008483014605131798"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = np.linalg.norm(dtheta_approx - dtheta)/(np.linalg.norm(dtheta_approx) + np.linalg.norm(dtheta))\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.15691930e-05,  3.16061659e-05, -1.76333410e-06,  2.98943489e-06,\n",
       "        2.17772372e-05, -3.65654020e-06, -6.60553359e-06, -3.45867926e-05,\n",
       "       -8.05460939e-06,  6.79184259e-06,  8.30972127e-06, -1.81434012e-06,\n",
       "        3.96887189e-05,  3.67969658e-06, -3.86372233e-05,  2.11339176e-05,\n",
       "       -1.66461664e-06, -1.98704447e-06,  5.01934568e-06, -8.23341342e-07,\n",
       "        1.61811395e-06,  2.29365001e-06, -4.86683294e-06,  7.22525185e-07,\n",
       "        3.73197318e-06,  4.61378388e-06,  2.19859454e-05,  8.74455618e-06,\n",
       "       -2.61945414e-06, -3.35132098e-06,  6.33355206e-06,  4.03851003e-09,\n",
       "       -1.77855776e-05,  1.89954999e-05, -5.11588709e-05, -2.51815520e-05,\n",
       "       -8.60432434e-06, -2.63179667e-06,  5.71479601e-05,  1.73542644e-05,\n",
       "       -7.77376414e-05])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtheta_approx - dtheta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusion\n",
    "Similar values of loss from 2 & 3 for the same number of iterations indicates that the custom gradient descent implementation is correct. The weights and biases are different because the 2 models are randomly initialised during training and the loss function of the shallow neural network has multiple maximia and minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
