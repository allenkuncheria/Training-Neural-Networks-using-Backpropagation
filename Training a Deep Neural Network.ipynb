{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Deep Neural Network\n",
    "The following code implements a deep neural network of 2 hidden layers with backpropagation using low-level libraries and compares it with a model generated by Scikit-learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Loading & Cleaning\n",
    "The data set contains credit card debt information about 10,000 customers and whether they defaulted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income\n",
       "0      No      No   729.526495  44361.625074\n",
       "1      No     Yes   817.180407  12106.134700\n",
       "2      No      No  1073.549164  31767.138947\n",
       "3      No      No   529.250605  35704.493935\n",
       "4      No      No   785.655883  38463.495879"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('Default.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and converting to NumPy arrays\n",
    "df['default']=df['default'].apply(lambda x: 0 if x=='No' else 1)\n",
    "df['student']=df['student'].apply(lambda x: 0 if x=='No' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "0        0        0   729.526495  44361.625074\n",
       "1        0        1   817.180407  12106.134700\n",
       "2        0        0  1073.549164  31767.138947\n",
       "3        0        0   529.250605  35704.493935\n",
       "4        0        0   785.655883  38463.495879"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   default  10000 non-null  int64  \n",
      " 1   student  10000 non-null  int64  \n",
      " 2   balance  10000 non-null  float64\n",
      " 3   income   10000 non-null  float64\n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 312.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.218835</td>\n",
       "      <td>0.813187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.037616</td>\n",
       "      <td>-1.605496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>-0.131212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.632893</td>\n",
       "      <td>0.164031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>0.370915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.255990</td>\n",
       "      <td>1.460366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.160044</td>\n",
       "      <td>-1.039014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>1.883565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.516742</td>\n",
       "      <td>0.236363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.311691</td>\n",
       "      <td>-1.248805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      default  student   balance    income\n",
       "0           0        0 -0.218835  0.813187\n",
       "1           0        1 -0.037616 -1.605496\n",
       "2           0        0  0.492410 -0.131212\n",
       "3           0        0 -0.632893  0.164031\n",
       "4           0        0 -0.102791  0.370915\n",
       "...       ...      ...       ...       ...\n",
       "9995        0        0 -0.255990  1.460366\n",
       "9996        0        0 -0.160044 -1.039014\n",
       "9997        0        0  0.020751  1.883565\n",
       "9998        0        0  1.516742  0.236363\n",
       "9999        0        1 -1.311691 -1.248805\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['balance','income']] = scaler.fit_transform(df[['balance','income']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['default'].to_numpy().reshape(-1,1)\n",
    "X = df.drop(columns=['default']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (10000, 1)\n",
      "Shape of X: (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 10000)\n",
      "Shape of X: (3, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = X.T\n",
    "Y = Y.T\n",
    "\n",
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training a Deep Neural Network Using Scikit-learn\n",
    "The following code trains a deep neural network of 2 hidden layers with 4 neurons in each hidden layer using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1118: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.77699467\n",
      "Iteration 2, loss = 0.77127488\n",
      "Iteration 3, loss = 0.76560155\n",
      "Iteration 4, loss = 0.75997452\n",
      "Iteration 5, loss = 0.75439366\n",
      "Iteration 6, loss = 0.74885881\n",
      "Iteration 7, loss = 0.74336984\n",
      "Iteration 8, loss = 0.73792658\n",
      "Iteration 9, loss = 0.73252887\n",
      "Iteration 10, loss = 0.72717657\n",
      "Iteration 11, loss = 0.72186949\n",
      "Iteration 12, loss = 0.71660747\n",
      "Iteration 13, loss = 0.71139034\n",
      "Iteration 14, loss = 0.70621792\n",
      "Iteration 15, loss = 0.70109002\n",
      "Iteration 16, loss = 0.69600648\n",
      "Iteration 17, loss = 0.69096709\n",
      "Iteration 18, loss = 0.68597168\n",
      "Iteration 19, loss = 0.68102003\n",
      "Iteration 20, loss = 0.67611196\n",
      "Iteration 21, loss = 0.67124727\n",
      "Iteration 22, loss = 0.66642574\n",
      "Iteration 23, loss = 0.66164718\n",
      "Iteration 24, loss = 0.65691137\n",
      "Iteration 25, loss = 0.65221810\n",
      "Iteration 26, loss = 0.64756714\n",
      "Iteration 27, loss = 0.64295829\n",
      "Iteration 28, loss = 0.63839131\n",
      "Iteration 29, loss = 0.63386599\n",
      "Iteration 30, loss = 0.62938209\n",
      "Iteration 31, loss = 0.62493938\n",
      "Iteration 32, loss = 0.62053762\n",
      "Iteration 33, loss = 0.61617659\n",
      "Iteration 34, loss = 0.61185604\n",
      "Iteration 35, loss = 0.60757573\n",
      "Iteration 36, loss = 0.60333542\n",
      "Iteration 37, loss = 0.59913486\n",
      "Iteration 38, loss = 0.59497381\n",
      "Iteration 39, loss = 0.59085201\n",
      "Iteration 40, loss = 0.58676921\n",
      "Iteration 41, loss = 0.58272516\n",
      "Iteration 42, loss = 0.57871960\n",
      "Iteration 43, loss = 0.57475227\n",
      "Iteration 44, loss = 0.57082293\n",
      "Iteration 45, loss = 0.56693129\n",
      "Iteration 46, loss = 0.56307712\n",
      "Iteration 47, loss = 0.55926013\n",
      "Iteration 48, loss = 0.55548007\n",
      "Iteration 49, loss = 0.55173667\n",
      "Iteration 50, loss = 0.54802966\n",
      "Iteration 51, loss = 0.54435878\n",
      "Iteration 52, loss = 0.54072376\n",
      "Iteration 53, loss = 0.53712432\n",
      "Iteration 54, loss = 0.53356020\n",
      "Iteration 55, loss = 0.53003112\n",
      "Iteration 56, loss = 0.52653682\n",
      "Iteration 57, loss = 0.52307702\n",
      "Iteration 58, loss = 0.51965144\n",
      "Iteration 59, loss = 0.51625982\n",
      "Iteration 60, loss = 0.51290188\n",
      "Iteration 61, loss = 0.50957735\n",
      "Iteration 62, loss = 0.50628595\n",
      "Iteration 63, loss = 0.50302741\n",
      "Iteration 64, loss = 0.49980145\n",
      "Iteration 65, loss = 0.49660781\n",
      "Iteration 66, loss = 0.49344619\n",
      "Iteration 67, loss = 0.49031634\n",
      "Iteration 68, loss = 0.48721798\n",
      "Iteration 69, loss = 0.48415084\n",
      "Iteration 70, loss = 0.48111463\n",
      "Iteration 71, loss = 0.47810910\n",
      "Iteration 72, loss = 0.47513396\n",
      "Iteration 73, loss = 0.47218894\n",
      "Iteration 74, loss = 0.46927379\n",
      "Iteration 75, loss = 0.46638821\n",
      "Iteration 76, loss = 0.46353195\n",
      "Iteration 77, loss = 0.46070473\n",
      "Iteration 78, loss = 0.45790630\n",
      "Iteration 79, loss = 0.45513637\n",
      "Iteration 80, loss = 0.45239469\n",
      "Iteration 81, loss = 0.44968098\n",
      "Iteration 82, loss = 0.44699499\n",
      "Iteration 83, loss = 0.44433645\n",
      "Iteration 84, loss = 0.44170510\n",
      "Iteration 85, loss = 0.43910068\n",
      "Iteration 86, loss = 0.43652292\n",
      "Iteration 87, loss = 0.43397158\n",
      "Iteration 88, loss = 0.43144638\n",
      "Iteration 89, loss = 0.42894709\n",
      "Iteration 90, loss = 0.42647343\n",
      "Iteration 91, loss = 0.42402516\n",
      "Iteration 92, loss = 0.42160203\n",
      "Iteration 93, loss = 0.41920378\n",
      "Iteration 94, loss = 0.41683017\n",
      "Iteration 95, loss = 0.41448095\n",
      "Iteration 96, loss = 0.41215587\n",
      "Iteration 97, loss = 0.40985468\n",
      "Iteration 98, loss = 0.40757715\n",
      "Iteration 99, loss = 0.40532304\n",
      "Iteration 100, loss = 0.40309209\n",
      "Iteration 101, loss = 0.40088408\n",
      "Iteration 102, loss = 0.39869877\n",
      "Iteration 103, loss = 0.39653593\n",
      "Iteration 104, loss = 0.39439531\n",
      "Iteration 105, loss = 0.39227669\n",
      "Iteration 106, loss = 0.39017984\n",
      "Iteration 107, loss = 0.38810453\n",
      "Iteration 108, loss = 0.38605053\n",
      "Iteration 109, loss = 0.38401762\n",
      "Iteration 110, loss = 0.38200558\n",
      "Iteration 111, loss = 0.38001418\n",
      "Iteration 112, loss = 0.37804321\n",
      "Iteration 113, loss = 0.37609244\n",
      "Iteration 114, loss = 0.37416167\n",
      "Iteration 115, loss = 0.37225067\n",
      "Iteration 116, loss = 0.37035924\n",
      "Iteration 117, loss = 0.36848717\n",
      "Iteration 118, loss = 0.36663424\n",
      "Iteration 119, loss = 0.36480025\n",
      "Iteration 120, loss = 0.36298500\n",
      "Iteration 121, loss = 0.36118828\n",
      "Iteration 122, loss = 0.35940988\n",
      "Iteration 123, loss = 0.35764962\n",
      "Iteration 124, loss = 0.35590730\n",
      "Iteration 125, loss = 0.35418271\n",
      "Iteration 126, loss = 0.35247566\n",
      "Iteration 127, loss = 0.35078596\n",
      "Iteration 128, loss = 0.34911343\n",
      "Iteration 129, loss = 0.34745787\n",
      "Iteration 130, loss = 0.34581909\n",
      "Iteration 131, loss = 0.34419691\n",
      "Iteration 132, loss = 0.34259115\n",
      "Iteration 133, loss = 0.34100163\n",
      "Iteration 134, loss = 0.33942817\n",
      "Iteration 135, loss = 0.33787058\n",
      "Iteration 136, loss = 0.33632869\n",
      "Iteration 137, loss = 0.33480233\n",
      "Iteration 138, loss = 0.33329133\n",
      "Iteration 139, loss = 0.33179552\n",
      "Iteration 140, loss = 0.33031472\n",
      "Iteration 141, loss = 0.32884877\n",
      "Iteration 142, loss = 0.32739750\n",
      "Iteration 143, loss = 0.32596076\n",
      "Iteration 144, loss = 0.32453837\n",
      "Iteration 145, loss = 0.32313018\n",
      "Iteration 146, loss = 0.32173602\n",
      "Iteration 147, loss = 0.32035575\n",
      "Iteration 148, loss = 0.31898921\n",
      "Iteration 149, loss = 0.31763624\n",
      "Iteration 150, loss = 0.31629669\n",
      "Iteration 151, loss = 0.31497041\n",
      "Iteration 152, loss = 0.31365725\n",
      "Iteration 153, loss = 0.31235707\n",
      "Iteration 154, loss = 0.31106972\n",
      "Iteration 155, loss = 0.30979505\n",
      "Iteration 156, loss = 0.30853292\n",
      "Iteration 157, loss = 0.30728320\n",
      "Iteration 158, loss = 0.30604574\n",
      "Iteration 159, loss = 0.30482041\n",
      "Iteration 160, loss = 0.30360707\n",
      "Iteration 161, loss = 0.30240558\n",
      "Iteration 162, loss = 0.30121581\n",
      "Iteration 163, loss = 0.30003764\n",
      "Iteration 164, loss = 0.29887093\n",
      "Iteration 165, loss = 0.29771554\n",
      "Iteration 166, loss = 0.29657136\n",
      "Iteration 167, loss = 0.29543826\n",
      "Iteration 168, loss = 0.29431612\n",
      "Iteration 169, loss = 0.29320481\n",
      "Iteration 170, loss = 0.29210421\n",
      "Iteration 171, loss = 0.29101419\n",
      "Iteration 172, loss = 0.28993465\n",
      "Iteration 173, loss = 0.28886547\n",
      "Iteration 174, loss = 0.28780652\n",
      "Iteration 175, loss = 0.28675770\n",
      "Iteration 176, loss = 0.28571889\n",
      "Iteration 177, loss = 0.28468997\n",
      "Iteration 178, loss = 0.28367085\n",
      "Iteration 179, loss = 0.28266140\n",
      "Iteration 180, loss = 0.28166153\n",
      "Iteration 181, loss = 0.28067111\n",
      "Iteration 182, loss = 0.27969006\n",
      "Iteration 183, loss = 0.27871826\n",
      "Iteration 184, loss = 0.27775561\n",
      "Iteration 185, loss = 0.27680201\n",
      "Iteration 186, loss = 0.27585736\n",
      "Iteration 187, loss = 0.27492155\n",
      "Iteration 188, loss = 0.27399450\n",
      "Iteration 189, loss = 0.27307610\n",
      "Iteration 190, loss = 0.27216626\n",
      "Iteration 191, loss = 0.27126487\n",
      "Iteration 192, loss = 0.27037186\n",
      "Iteration 193, loss = 0.26948713\n",
      "Iteration 194, loss = 0.26861057\n",
      "Iteration 195, loss = 0.26774211\n",
      "Iteration 196, loss = 0.26688166\n",
      "Iteration 197, loss = 0.26602912\n",
      "Iteration 198, loss = 0.26518441\n",
      "Iteration 199, loss = 0.26434744\n",
      "Iteration 200, loss = 0.26351813\n",
      "Iteration 201, loss = 0.26269640\n",
      "Iteration 202, loss = 0.26188215\n",
      "Iteration 203, loss = 0.26107531\n",
      "Iteration 204, loss = 0.26027579\n",
      "Iteration 205, loss = 0.25948353\n",
      "Iteration 206, loss = 0.25869843\n",
      "Iteration 207, loss = 0.25792041\n",
      "Iteration 208, loss = 0.25714941\n",
      "Iteration 209, loss = 0.25638535\n",
      "Iteration 210, loss = 0.25562814\n",
      "Iteration 211, loss = 0.25487772\n",
      "Iteration 212, loss = 0.25413400\n",
      "Iteration 213, loss = 0.25339693\n",
      "Iteration 214, loss = 0.25266642\n",
      "Iteration 215, loss = 0.25194240\n",
      "Iteration 216, loss = 0.25122481\n",
      "Iteration 217, loss = 0.25051358\n",
      "Iteration 218, loss = 0.24980863\n",
      "Iteration 219, loss = 0.24910989\n",
      "Iteration 220, loss = 0.24841731\n",
      "Iteration 221, loss = 0.24773082\n",
      "Iteration 222, loss = 0.24705034\n",
      "Iteration 223, loss = 0.24637582\n",
      "Iteration 224, loss = 0.24570719\n",
      "Iteration 225, loss = 0.24504439\n",
      "Iteration 226, loss = 0.24438735\n",
      "Iteration 227, loss = 0.24373601\n",
      "Iteration 228, loss = 0.24309032\n",
      "Iteration 229, loss = 0.24245022\n",
      "Iteration 230, loss = 0.24181563\n",
      "Iteration 231, loss = 0.24118651\n",
      "Iteration 232, loss = 0.24056280\n",
      "Iteration 233, loss = 0.23994444\n",
      "Iteration 234, loss = 0.23933137\n",
      "Iteration 235, loss = 0.23872354\n",
      "Iteration 236, loss = 0.23812089\n",
      "Iteration 237, loss = 0.23752336\n",
      "Iteration 238, loss = 0.23693091\n",
      "Iteration 239, loss = 0.23634348\n",
      "Iteration 240, loss = 0.23576101\n",
      "Iteration 241, loss = 0.23518346\n",
      "Iteration 242, loss = 0.23461077\n",
      "Iteration 243, loss = 0.23404290\n",
      "Iteration 244, loss = 0.23347979\n",
      "Iteration 245, loss = 0.23292139\n",
      "Iteration 246, loss = 0.23236765\n",
      "Iteration 247, loss = 0.23181853\n",
      "Iteration 248, loss = 0.23127397\n",
      "Iteration 249, loss = 0.23073393\n",
      "Iteration 250, loss = 0.23019837\n",
      "Iteration 251, loss = 0.22966723\n",
      "Iteration 252, loss = 0.22914048\n",
      "Iteration 253, loss = 0.22861806\n",
      "Iteration 254, loss = 0.22809993\n",
      "Iteration 255, loss = 0.22758605\n",
      "Iteration 256, loss = 0.22707637\n",
      "Iteration 257, loss = 0.22657085\n",
      "Iteration 258, loss = 0.22606944\n",
      "Iteration 259, loss = 0.22557212\n",
      "Iteration 260, loss = 0.22507882\n",
      "Iteration 261, loss = 0.22458952\n",
      "Iteration 262, loss = 0.22410418\n",
      "Iteration 263, loss = 0.22362274\n",
      "Iteration 264, loss = 0.22314518\n",
      "Iteration 265, loss = 0.22267145\n",
      "Iteration 266, loss = 0.22220151\n",
      "Iteration 267, loss = 0.22173533\n",
      "Iteration 268, loss = 0.22127287\n",
      "Iteration 269, loss = 0.22081408\n",
      "Iteration 270, loss = 0.22035895\n",
      "Iteration 271, loss = 0.21990741\n",
      "Iteration 272, loss = 0.21945945\n",
      "Iteration 273, loss = 0.21901503\n",
      "Iteration 274, loss = 0.21857410\n",
      "Iteration 275, loss = 0.21813664\n",
      "Iteration 276, loss = 0.21770261\n",
      "Iteration 277, loss = 0.21727198\n",
      "Iteration 278, loss = 0.21684471\n",
      "Iteration 279, loss = 0.21642076\n",
      "Iteration 280, loss = 0.21600012\n",
      "Iteration 281, loss = 0.21558273\n",
      "Iteration 282, loss = 0.21516858\n",
      "Iteration 283, loss = 0.21475763\n",
      "Iteration 284, loss = 0.21434985\n",
      "Iteration 285, loss = 0.21394520\n",
      "Iteration 286, loss = 0.21354366\n",
      "Iteration 287, loss = 0.21314520\n",
      "Iteration 288, loss = 0.21274978\n",
      "Iteration 289, loss = 0.21235738\n",
      "Iteration 290, loss = 0.21196796\n",
      "Iteration 291, loss = 0.21158150\n",
      "Iteration 292, loss = 0.21119797\n",
      "Iteration 293, loss = 0.21081734\n",
      "Iteration 294, loss = 0.21043958\n",
      "Iteration 295, loss = 0.21006467\n",
      "Iteration 296, loss = 0.20969258\n",
      "Iteration 297, loss = 0.20932327\n",
      "Iteration 298, loss = 0.20895673\n",
      "Iteration 299, loss = 0.20859292\n",
      "Iteration 300, loss = 0.20823183\n",
      "Iteration 301, loss = 0.20787341\n",
      "Iteration 302, loss = 0.20751766\n",
      "Iteration 303, loss = 0.20716454\n",
      "Iteration 304, loss = 0.20681403\n",
      "Iteration 305, loss = 0.20646610\n",
      "Iteration 306, loss = 0.20612073\n",
      "Iteration 307, loss = 0.20577790\n",
      "Iteration 308, loss = 0.20543757\n",
      "Iteration 309, loss = 0.20509974\n",
      "Iteration 310, loss = 0.20476436\n",
      "Iteration 311, loss = 0.20443143\n",
      "Iteration 312, loss = 0.20410091\n",
      "Iteration 313, loss = 0.20377279\n",
      "Iteration 314, loss = 0.20344704\n",
      "Iteration 315, loss = 0.20312364\n",
      "Iteration 316, loss = 0.20280256\n",
      "Iteration 317, loss = 0.20248380\n",
      "Iteration 318, loss = 0.20216731\n",
      "Iteration 319, loss = 0.20185309\n",
      "Iteration 320, loss = 0.20154112\n",
      "Iteration 321, loss = 0.20123136\n",
      "Iteration 322, loss = 0.20092381\n",
      "Iteration 323, loss = 0.20061843\n",
      "Iteration 324, loss = 0.20031522\n",
      "Iteration 325, loss = 0.20001414\n",
      "Iteration 326, loss = 0.19971519\n",
      "Iteration 327, loss = 0.19941834\n",
      "Iteration 328, loss = 0.19912357\n",
      "Iteration 329, loss = 0.19883086\n",
      "Iteration 330, loss = 0.19854020\n",
      "Iteration 331, loss = 0.19825156\n",
      "Iteration 332, loss = 0.19796493\n",
      "Iteration 333, loss = 0.19768029\n",
      "Iteration 334, loss = 0.19739762\n",
      "Iteration 335, loss = 0.19711690\n",
      "Iteration 336, loss = 0.19683811\n",
      "Iteration 337, loss = 0.19656125\n",
      "Iteration 338, loss = 0.19628628\n",
      "Iteration 339, loss = 0.19601320\n",
      "Iteration 340, loss = 0.19574198\n",
      "Iteration 341, loss = 0.19547261\n",
      "Iteration 342, loss = 0.19520508\n",
      "Iteration 343, loss = 0.19493936\n",
      "Iteration 344, loss = 0.19467544\n",
      "Iteration 345, loss = 0.19441331\n",
      "Iteration 346, loss = 0.19415294\n",
      "Iteration 347, loss = 0.19389433\n",
      "Iteration 348, loss = 0.19363745\n",
      "Iteration 349, loss = 0.19338230\n",
      "Iteration 350, loss = 0.19312885\n",
      "Iteration 351, loss = 0.19287709\n",
      "Iteration 352, loss = 0.19262700\n",
      "Iteration 353, loss = 0.19237858\n",
      "Iteration 354, loss = 0.19213181\n",
      "Iteration 355, loss = 0.19188667\n",
      "Iteration 356, loss = 0.19164314\n",
      "Iteration 357, loss = 0.19140122\n",
      "Iteration 358, loss = 0.19116089\n",
      "Iteration 359, loss = 0.19092213\n",
      "Iteration 360, loss = 0.19068494\n",
      "Iteration 361, loss = 0.19044930\n",
      "Iteration 362, loss = 0.19021519\n",
      "Iteration 363, loss = 0.18998260\n",
      "Iteration 364, loss = 0.18975152\n",
      "Iteration 365, loss = 0.18952193\n",
      "Iteration 366, loss = 0.18929383\n",
      "Iteration 367, loss = 0.18906720\n",
      "Iteration 368, loss = 0.18884202\n",
      "Iteration 369, loss = 0.18861829\n",
      "Iteration 370, loss = 0.18839599\n",
      "Iteration 371, loss = 0.18817511\n",
      "Iteration 372, loss = 0.18795564\n",
      "Iteration 373, loss = 0.18773757\n",
      "Iteration 374, loss = 0.18752087\n",
      "Iteration 375, loss = 0.18730555\n",
      "Iteration 376, loss = 0.18709159\n",
      "Iteration 377, loss = 0.18687898\n",
      "Iteration 378, loss = 0.18666771\n",
      "Iteration 379, loss = 0.18645776\n",
      "Iteration 380, loss = 0.18624912\n",
      "Iteration 381, loss = 0.18604179\n",
      "Iteration 382, loss = 0.18583575\n",
      "Iteration 383, loss = 0.18563100\n",
      "Iteration 384, loss = 0.18542751\n",
      "Iteration 385, loss = 0.18522529\n",
      "Iteration 386, loss = 0.18502431\n",
      "Iteration 387, loss = 0.18482457\n",
      "Iteration 388, loss = 0.18462607\n",
      "Iteration 389, loss = 0.18442878\n",
      "Iteration 390, loss = 0.18423270\n",
      "Iteration 391, loss = 0.18403781\n",
      "Iteration 392, loss = 0.18384412\n",
      "Iteration 393, loss = 0.18365161\n",
      "Iteration 394, loss = 0.18346026\n",
      "Iteration 395, loss = 0.18327007\n",
      "Iteration 396, loss = 0.18308103\n",
      "Iteration 397, loss = 0.18289313\n",
      "Iteration 398, loss = 0.18270637\n",
      "Iteration 399, loss = 0.18252072\n",
      "Iteration 400, loss = 0.18233619\n",
      "Iteration 401, loss = 0.18215276\n",
      "Iteration 402, loss = 0.18197042\n",
      "Iteration 403, loss = 0.18178917\n",
      "Iteration 404, loss = 0.18160899\n",
      "Iteration 405, loss = 0.18142988\n",
      "Iteration 406, loss = 0.18125183\n",
      "Iteration 407, loss = 0.18107483\n",
      "Iteration 408, loss = 0.18089887\n",
      "Iteration 409, loss = 0.18072395\n",
      "Iteration 410, loss = 0.18055005\n",
      "Iteration 411, loss = 0.18037717\n",
      "Iteration 412, loss = 0.18020529\n",
      "Iteration 413, loss = 0.18003442\n",
      "Iteration 414, loss = 0.17986453\n",
      "Iteration 415, loss = 0.17969563\n",
      "Iteration 416, loss = 0.17952771\n",
      "Iteration 417, loss = 0.17936076\n",
      "Iteration 418, loss = 0.17919476\n",
      "Iteration 419, loss = 0.17902972\n",
      "Iteration 420, loss = 0.17886563\n",
      "Iteration 421, loss = 0.17870247\n",
      "Iteration 422, loss = 0.17854024\n",
      "Iteration 423, loss = 0.17837894\n",
      "Iteration 424, loss = 0.17821855\n",
      "Iteration 425, loss = 0.17805907\n",
      "Iteration 426, loss = 0.17790049\n",
      "Iteration 427, loss = 0.17774281\n",
      "Iteration 428, loss = 0.17758601\n",
      "Iteration 429, loss = 0.17743009\n",
      "Iteration 430, loss = 0.17727505\n",
      "Iteration 431, loss = 0.17712087\n",
      "Iteration 432, loss = 0.17696755\n",
      "Iteration 433, loss = 0.17681508\n",
      "Iteration 434, loss = 0.17666346\n",
      "Iteration 435, loss = 0.17651268\n",
      "Iteration 436, loss = 0.17636274\n",
      "Iteration 437, loss = 0.17621362\n",
      "Iteration 438, loss = 0.17606531\n",
      "Iteration 439, loss = 0.17591783\n",
      "Iteration 440, loss = 0.17577115\n",
      "Iteration 441, loss = 0.17562527\n",
      "Iteration 442, loss = 0.17548018\n",
      "Iteration 443, loss = 0.17533589\n",
      "Iteration 444, loss = 0.17519237\n",
      "Iteration 445, loss = 0.17504964\n",
      "Iteration 446, loss = 0.17490767\n",
      "Iteration 447, loss = 0.17476647\n",
      "Iteration 448, loss = 0.17462602\n",
      "Iteration 449, loss = 0.17448633\n",
      "Iteration 450, loss = 0.17434739\n",
      "Iteration 451, loss = 0.17420919\n",
      "Iteration 452, loss = 0.17407173\n",
      "Iteration 453, loss = 0.17393499\n",
      "Iteration 454, loss = 0.17379898\n",
      "Iteration 455, loss = 0.17366369\n",
      "Iteration 456, loss = 0.17352911\n",
      "Iteration 457, loss = 0.17339524\n",
      "Iteration 458, loss = 0.17326207\n",
      "Iteration 459, loss = 0.17312960\n",
      "Iteration 460, loss = 0.17299782\n",
      "Iteration 461, loss = 0.17286673\n",
      "Iteration 462, loss = 0.17273633\n",
      "Iteration 463, loss = 0.17260659\n",
      "Iteration 464, loss = 0.17247753\n",
      "Iteration 465, loss = 0.17234914\n",
      "Iteration 466, loss = 0.17222141\n",
      "Iteration 467, loss = 0.17209433\n",
      "Iteration 468, loss = 0.17196791\n",
      "Iteration 469, loss = 0.17184213\n",
      "Iteration 470, loss = 0.17171700\n",
      "Iteration 471, loss = 0.17159250\n",
      "Iteration 472, loss = 0.17146863\n",
      "Iteration 473, loss = 0.17134539\n",
      "Iteration 474, loss = 0.17122278\n",
      "Iteration 475, loss = 0.17110078\n",
      "Iteration 476, loss = 0.17097940\n",
      "Iteration 477, loss = 0.17085863\n",
      "Iteration 478, loss = 0.17073846\n",
      "Iteration 479, loss = 0.17061889\n",
      "Iteration 480, loss = 0.17049992\n",
      "Iteration 481, loss = 0.17038154\n",
      "Iteration 482, loss = 0.17026374\n",
      "Iteration 483, loss = 0.17014653\n",
      "Iteration 484, loss = 0.17002990\n",
      "Iteration 485, loss = 0.16991384\n",
      "Iteration 486, loss = 0.16979836\n",
      "Iteration 487, loss = 0.16968343\n",
      "Iteration 488, loss = 0.16956907\n",
      "Iteration 489, loss = 0.16945527\n",
      "Iteration 490, loss = 0.16934202\n",
      "Iteration 491, loss = 0.16922932\n",
      "Iteration 492, loss = 0.16911717\n",
      "Iteration 493, loss = 0.16900556\n",
      "Iteration 494, loss = 0.16889448\n",
      "Iteration 495, loss = 0.16878394\n",
      "Iteration 496, loss = 0.16867393\n",
      "Iteration 497, loss = 0.16856445\n",
      "Iteration 498, loss = 0.16845549\n",
      "Iteration 499, loss = 0.16834704\n",
      "Iteration 500, loss = 0.16823912\n",
      "Iteration 501, loss = 0.16813170\n",
      "Iteration 502, loss = 0.16802479\n",
      "Iteration 503, loss = 0.16791838\n",
      "Iteration 504, loss = 0.16781248\n",
      "Iteration 505, loss = 0.16770707\n",
      "Iteration 506, loss = 0.16760215\n",
      "Iteration 507, loss = 0.16749772\n",
      "Iteration 508, loss = 0.16739378\n",
      "Iteration 509, loss = 0.16729033\n",
      "Iteration 510, loss = 0.16718735\n",
      "Iteration 511, loss = 0.16708484\n",
      "Iteration 512, loss = 0.16698281\n",
      "Iteration 513, loss = 0.16688125\n",
      "Iteration 514, loss = 0.16678015\n",
      "Iteration 515, loss = 0.16667952\n",
      "Iteration 516, loss = 0.16657934\n",
      "Iteration 517, loss = 0.16647962\n",
      "Iteration 518, loss = 0.16638036\n",
      "Iteration 519, loss = 0.16628154\n",
      "Iteration 520, loss = 0.16618317\n",
      "Iteration 521, loss = 0.16608524\n",
      "Iteration 522, loss = 0.16598775\n",
      "Iteration 523, loss = 0.16589070\n",
      "Iteration 524, loss = 0.16579409\n",
      "Iteration 525, loss = 0.16569790\n",
      "Iteration 526, loss = 0.16560214\n",
      "Iteration 527, loss = 0.16550681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Y.shape[1]\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,4),activation='tanh',solver='sgd',alpha=0,learning_rate_init=0.01,max_iter=2000,batch_size=m,shuffle=False,momentum=0,verbose=True)\n",
    "mlp.fit(X.T,Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights and biases\n",
      "W1: [[-0.16905105  0.49850713 -0.75603018]\n",
      " [ 0.53120166  0.07185733 -0.07866865]\n",
      " [ 0.38506698  0.63479874  0.70192537]\n",
      " [ 0.40849687  0.28333679  0.4195484 ]]\n",
      "b1: [[ 0.65264976]\n",
      " [-0.26108415]\n",
      " [ 0.60626433]\n",
      " [ 0.75449845]]\n",
      "W2: [[ 0.24677249 -0.70763733  0.33229445 -0.67727186]\n",
      " [-0.13311203  0.87323899  0.68842082  0.0540427 ]\n",
      " [-0.73199369 -0.36985573 -0.5191542  -0.41344406]\n",
      " [-0.01479345 -0.25828787 -0.43335966 -0.85312403]]\n",
      "b2: [[ 0.67948802]\n",
      " [-0.99871148]\n",
      " [-0.63788959]\n",
      " [-0.75277169]]\n",
      "W3: [[-0.44145447  0.74204117  0.74567222  1.42869179]]\n",
      "b3: [[0.03478877]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",mlp.coefs_[0].T)\n",
    "print(\"b1:\",mlp.intercepts_[0].reshape(-1,1))\n",
    "print(\"W2:\",mlp.coefs_[1].T)\n",
    "print(\"b2:\",mlp.intercepts_[1].reshape(-1,1))\n",
    "print(\"W3:\",mlp.coefs_[2].T)\n",
    "print(\"b3:\",mlp.intercepts_[2].reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training a Deep Neural Network Using Backpropagation\n",
    "The following code implements backpropagation to train a deep neural network of 2 hidden layers with 4 neurons in each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the paramaters of the neural network with Xavier initialisation\n",
    "W = [np.random.randn(4,3)*np.sqrt(1/3),np.random.randn(4,4)*np.sqrt(1/4),np.random.randn(1,4)*np.sqrt(1/4)]\n",
    "b = [np.zeros((4,1)),np.zeros((4,1)),np.zeros((1,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last iteration: 402\n",
      "Losses: [0.19007655 0.18985948 0.1896436  0.1894289  0.18921537 0.18900301\n",
      " 0.18879179 0.18858172 0.18837279 0.18816499]\n",
      "\n",
      "Weights and biases\n",
      "W1: [[ 0.23225049  0.70235851 -0.60641509]\n",
      " [-0.84215143 -1.0047139  -0.69345399]\n",
      " [-0.58152449  0.32689847  0.91284491]\n",
      " [ 0.21799331  0.15903783  0.21828128]]\n",
      "b1: [[-0.14280446]\n",
      " [-0.26534776]\n",
      " [-0.12652547]\n",
      " [ 0.31429823]]\n",
      "W2: [[-0.33655407 -0.21378306 -0.34068044  0.07657408]\n",
      " [-0.11607443  0.22145416 -0.32453692 -0.42593727]\n",
      " [-0.18609074  0.21684395 -0.05565453 -0.38643371]\n",
      " [-0.26426254 -0.47198044 -0.35136714  0.51700934]]\n",
      "b2: [[ 0.46182213]\n",
      " [-0.18407148]\n",
      " [ 0.10574289]\n",
      " [ 0.5555595 ]]\n",
      "W3: [[-0.83802602  0.28875408 -0.17562209 -1.09307335]]\n",
      "b3: [[-0.77906532]]\n"
     ]
    }
   ],
   "source": [
    "# Updating parameters using gradient descent\n",
    "iter = 402\n",
    "lr = 0.01\n",
    "loss = np.array([])\n",
    "L = 3\n",
    "\n",
    "for i in np.arange(iter):\n",
    "    # Forward propagation\n",
    "    Z = []\n",
    "    A = []\n",
    "    for l in np.arange(L):\n",
    "        Z.append(W[l]@A[l-1]+b[l] if l> 0 else W[l]@X+b[l])\n",
    "        A.append(1/(1+np.exp(-Z[l])) if l==L-1 else np.tanh(Z[l]))\n",
    "\n",
    "    # Back propagation\n",
    "    dZ = [0]*L\n",
    "    dA = [0]*L\n",
    "    dW = [0]*L\n",
    "    db = [0]*L\n",
    "    \n",
    "    for l in np.arange(L-1,-1,-1):\n",
    "        dZ[l] = A[l]-Y if l==L-1 else dA[l]*(1-np.tanh(Z[l])**2)\n",
    "        dA[l-1] = W[l].T@dZ[l] if l> 0 else 0\n",
    "        dW[l] = 1/m*dZ[l]@A[l-1].T if l>0 else 1/m*dZ[l]@X.T\n",
    "        db[l] = 1/m*np.sum(dZ[l],axis=1,keepdims=True)\n",
    "        W[l] -= lr*dW[l]\n",
    "        b[l] -= lr*db[l]\n",
    "\n",
    "    current_loss = -1/m*(Y@np.log(A[L-1]).T+(1-Y)@np.log(1-A[L-1]).T)\n",
    "    loss = np.append(loss,current_loss)\n",
    "\n",
    "print(\"Last iteration:\",i+1)\n",
    "print(\"Losses:\",loss[-10:])\n",
    "\n",
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",W[0])\n",
    "print(\"b1:\",b[0])\n",
    "print(\"W2:\",W[1])\n",
    "print(\"b2:\",b[1])\n",
    "print(\"W3:\",W[2])\n",
    "print(\"b3:\",b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the cost function against the number of iterations\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = plt.figure(figsize=[7,7])\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(np.arange(iter)+1,loss)\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_xlabel('Iterations')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gradient checking to verify the custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.28414166e-03, -8.43529511e-03, -6.04443375e-03,  7.01436459e-03,\n",
       "       -1.15402180e-02, -3.17387358e-03,  1.21086958e-05, -1.31220292e-02,\n",
       "        5.36237556e-03, -8.30168294e-03,  2.40940731e-02,  6.08759732e-03,\n",
       "        1.76472182e-02,  2.39591145e-02,  1.44864396e-02, -3.01126534e-02,\n",
       "        1.73593553e-02, -5.86381416e-03,  1.29212013e-02, -8.97761378e-03,\n",
       "       -5.95508749e-03,  2.57877441e-03, -6.57740769e-03,  4.65958516e-03,\n",
       "        4.48420919e-03, -7.81764432e-04,  3.30740317e-03, -3.21764256e-03,\n",
       "        1.75339285e-02, -1.18728215e-02,  1.71179299e-02, -9.13493086e-03,\n",
       "       -4.33258887e-02,  2.07646327e-02, -1.33972165e-02, -4.76317081e-02,\n",
       "        4.65066803e-02, -9.27794457e-03,  9.29245029e-03,  5.15747376e-02,\n",
       "        7.85470592e-02])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the gradient vector of the cost function\n",
    "dtheta = np.array([])\n",
    "for l in range(L):\n",
    "    dtheta = np.append(dtheta,dW[l].flatten())\n",
    "    dtheta = np.append(dtheta,db[l].flatten())\n",
    "dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.23225049,  0.70235851, -0.60641509],\n",
      "       [-0.84215143, -1.0047139 , -0.69345399],\n",
      "       [-0.58152449,  0.32689847,  0.91284491],\n",
      "       [ 0.21799331,  0.15903783,  0.21828128]]), array([[-0.14280446],\n",
      "       [-0.26534776],\n",
      "       [-0.12652547],\n",
      "       [ 0.31429823]]), array([[-0.33655407, -0.21378306, -0.34068044,  0.07657408],\n",
      "       [-0.11607443,  0.22145416, -0.32453692, -0.42593727],\n",
      "       [-0.18609074,  0.21684395, -0.05565453, -0.38643371],\n",
      "       [-0.26426254, -0.47198044, -0.35136714,  0.51700934]]), array([[ 0.46182213],\n",
      "       [-0.18407148],\n",
      "       [ 0.10574289],\n",
      "       [ 0.5555595 ]]), array([[-0.83802602,  0.28875408, -0.17562209, -1.09307335]]), array([[-0.77906532]])]\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of all parameters\n",
    "P = b.copy()\n",
    "for l in range(L):\n",
    "    P.insert(2*l,W[l])\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, (3, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function to index an element in the list of parameters\n",
    "def loc(index,P):\n",
    "    i = 0\n",
    "    for p in P:\n",
    "        if index<p.size:\n",
    "            return i,np.unravel_index(index,p.shape)\n",
    "        else:\n",
    "            index -= p.size\n",
    "            i += 1\n",
    "\n",
    "loc(10,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02404599]]\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to calculate the partial derivative of the cost function with respect to a parameter\n",
    "\n",
    "def partial(index,epsilon,X,Y,P,L):\n",
    "    Q = P.copy()\n",
    "    i = loc(index,Q)\n",
    "    Q[i[0]][i[1]] += epsilon\n",
    "\n",
    "    W = [Q[i] for i in range(0,L*2,2)]\n",
    "    b = [Q[i] for i in range(1,L*2,2)]\n",
    "\n",
    "    Z = []\n",
    "    A = []\n",
    "    for l in np.arange(L):\n",
    "        Z.append(W[l]@A[l-1]+b[l] if l> 0 else W[l]@X+b[l])\n",
    "        A.append(1/(1+np.exp(-Z[l])) if l==L-1 else np.tanh(Z[l]))\n",
    "    \n",
    "    U = -1/m*(Y@np.log(A[L-1]).T+(1-Y)@np.log(1-A[L-1]).T)\n",
    "\n",
    "    Q[i[0]][i[1]] -= 2*epsilon\n",
    "    \n",
    "    W = [Q[i] for i in range(0,L*2,2)]\n",
    "    b = [Q[i] for i in range(1,L*2,2)]\n",
    "\n",
    "    Z = []\n",
    "    A = []\n",
    "    for l in np.arange(L):\n",
    "        Z.append(W[l]@A[l-1]+b[l] if l> 0 else W[l]@X+b[l])\n",
    "        A.append(1/(1+np.exp(-Z[l])) if l==L-1 else np.tanh(Z[l]))\n",
    "    \n",
    "    L = -1/m*(Y@np.log(A[L-1]).T+(1-Y)@np.log(1-A[L-1]).T)\n",
    "\n",
    "    return (U-L)/(2*epsilon)\n",
    "\n",
    "print(partial(10,1e-7,X,Y,P,L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.27320965e-03, -8.42551723e-03, -6.03798692e-03,  6.99271058e-03,\n",
       "       -1.15054202e-02, -3.16608378e-03,  8.65876815e-06, -1.31064083e-02,\n",
       "        5.35474179e-03, -8.27393390e-03,  2.40459948e-02,  6.07176642e-03,\n",
       "        1.76037614e-02,  2.38845321e-02,  1.44422192e-02, -3.00144415e-02,\n",
       "        1.73506544e-02, -5.86995827e-03,  1.29004678e-02, -8.95302971e-03,\n",
       "       -5.95248781e-03,  2.57772872e-03, -6.56692922e-03,  4.65433053e-03,\n",
       "        4.48411169e-03, -7.83445669e-04,  3.30328612e-03, -3.21390983e-03,\n",
       "        1.75035220e-02, -1.18431456e-02,  1.70761334e-02, -9.11517972e-03,\n",
       "       -4.31762530e-02,  2.07131318e-02, -1.33663720e-02, -4.74561676e-02,\n",
       "        4.64186144e-02, -9.25931637e-03,  9.29023386e-03,  5.14646326e-02,\n",
       "        7.83137726e-02])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the approximate gradient vector of the cost function\n",
    "dtheta_approx = np.array([])\n",
    "for index in range(41):\n",
    "    dtheta_approx = np.append(dtheta_approx,partial(index,1e-7,X,Y,P,L))\n",
    "\n",
    "dtheta_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001398288651323276"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = np.linalg.norm(dtheta_approx - dtheta)/(np.linalg.norm(dtheta_approx) + np.linalg.norm(dtheta))\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.09320164e-05,  9.77788376e-06,  6.44683739e-06, -2.16540161e-05,\n",
       "        3.47977600e-05,  7.78979224e-06, -3.44992768e-06,  1.56208419e-05,\n",
       "       -7.63377339e-06,  2.77490458e-05, -4.80782886e-05, -1.58308929e-05,\n",
       "       -4.34568343e-05, -7.45823963e-05, -4.42204124e-05,  9.82118803e-05,\n",
       "       -8.70090781e-06, -6.14410963e-06, -2.07335185e-05,  2.45840734e-05,\n",
       "        2.59968673e-06, -1.04568191e-06,  1.04784664e-05, -5.25462907e-06,\n",
       "       -9.74926656e-08, -1.68123771e-06, -4.11705541e-06,  3.73272688e-06,\n",
       "       -3.04065566e-05,  2.96758804e-05, -4.17964888e-05,  1.97511414e-05,\n",
       "        1.49635675e-04, -5.15008868e-05,  3.08444644e-05,  1.75540459e-04,\n",
       "       -8.80658965e-05,  1.86281969e-05, -2.21643176e-06, -1.10105084e-04,\n",
       "       -2.33286610e-04])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtheta_approx - dtheta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusion\n",
    "Similar values of loss from 2 & 3 for the same number of iterations indicates that the custom gradient descent implementation is correct. The weights and biases are different because the 2 models are randomly initialised during training and the loss function of the shallow neural network has multiple maximia and minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
