{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Deep Neural Network\n",
    "The following code implements a deep neural network of 2 hidden layers with backpropagation using low-level libraries and compares it with a model generated by Scikit-learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Loading & Cleaning\n",
    "The data set contains credit card debt information about 10,000 customers and whether they defaulted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income\n",
       "0      No      No   729.526495  44361.625074\n",
       "1      No     Yes   817.180407  12106.134700\n",
       "2      No      No  1073.549164  31767.138947\n",
       "3      No      No   529.250605  35704.493935\n",
       "4      No      No   785.655883  38463.495879"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('Default.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and converting to NumPy arrays\n",
    "df['default']=df['default'].apply(lambda x: 0 if x=='No' else 1)\n",
    "df['student']=df['student'].apply(lambda x: 0 if x=='No' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "0        0        0   729.526495  44361.625074\n",
       "1        0        1   817.180407  12106.134700\n",
       "2        0        0  1073.549164  31767.138947\n",
       "3        0        0   529.250605  35704.493935\n",
       "4        0        0   785.655883  38463.495879"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   default  10000 non-null  int64  \n",
      " 1   student  10000 non-null  int64  \n",
      " 2   balance  10000 non-null  float64\n",
      " 3   income   10000 non-null  float64\n",
      "dtypes: float64(2), int64(2)\n",
      "memory usage: 312.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.218835</td>\n",
       "      <td>0.813187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.037616</td>\n",
       "      <td>-1.605496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>-0.131212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.632893</td>\n",
       "      <td>0.164031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.102791</td>\n",
       "      <td>0.370915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.255990</td>\n",
       "      <td>1.460366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.160044</td>\n",
       "      <td>-1.039014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>1.883565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.516742</td>\n",
       "      <td>0.236363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.311691</td>\n",
       "      <td>-1.248805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      default  student   balance    income\n",
       "0           0        0 -0.218835  0.813187\n",
       "1           0        1 -0.037616 -1.605496\n",
       "2           0        0  0.492410 -0.131212\n",
       "3           0        0 -0.632893  0.164031\n",
       "4           0        0 -0.102791  0.370915\n",
       "...       ...      ...       ...       ...\n",
       "9995        0        0 -0.255990  1.460366\n",
       "9996        0        0 -0.160044 -1.039014\n",
       "9997        0        0  0.020751  1.883565\n",
       "9998        0        0  1.516742  0.236363\n",
       "9999        0        1 -1.311691 -1.248805\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['balance','income']] = scaler.fit_transform(df[['balance','income']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['default'].to_numpy().reshape(-1,1)\n",
    "X = df.drop(columns=['default']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (10000, 1)\n",
      "Shape of X: (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Y: (1, 10000)\n",
      "Shape of X: (3, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = X.T\n",
    "Y = Y.T\n",
    "\n",
    "print(\"Shape of Y:\",Y.shape)\n",
    "print(\"Shape of X:\",X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training a Deep Neural Network Using Scikit-learn\n",
    "The following code trains a deep neural network of 2 hidden layers with 4 neurons in each hidden layer using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allen/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1118: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12991523\n",
      "Iteration 2, loss = 1.10932788\n",
      "Iteration 3, loss = 1.08910561\n",
      "Iteration 4, loss = 1.06924971\n",
      "Iteration 5, loss = 1.04976094\n",
      "Iteration 6, loss = 1.03063957\n",
      "Iteration 7, loss = 1.01188540\n",
      "Iteration 8, loss = 0.99349777\n",
      "Iteration 9, loss = 0.97547556\n",
      "Iteration 10, loss = 0.95781724\n",
      "Iteration 11, loss = 0.94052089\n",
      "Iteration 12, loss = 0.92358417\n",
      "Iteration 13, loss = 0.90700442\n",
      "Iteration 14, loss = 0.89077864\n",
      "Iteration 15, loss = 0.87490349\n",
      "Iteration 16, loss = 0.85937539\n",
      "Iteration 17, loss = 0.84419044\n",
      "Iteration 18, loss = 0.82934454\n",
      "Iteration 19, loss = 0.81483334\n",
      "Iteration 20, loss = 0.80065232\n",
      "Iteration 21, loss = 0.78679674\n",
      "Iteration 22, loss = 0.77326173\n",
      "Iteration 23, loss = 0.76004227\n",
      "Iteration 24, loss = 0.74713322\n",
      "Iteration 25, loss = 0.73452931\n",
      "Iteration 26, loss = 0.72222521\n",
      "Iteration 27, loss = 0.71021550\n",
      "Iteration 28, loss = 0.69849470\n",
      "Iteration 29, loss = 0.68705728\n",
      "Iteration 30, loss = 0.67589768\n",
      "Iteration 31, loss = 0.66501030\n",
      "Iteration 32, loss = 0.65438956\n",
      "Iteration 33, loss = 0.64402985\n",
      "Iteration 34, loss = 0.63392558\n",
      "Iteration 35, loss = 0.62407118\n",
      "Iteration 36, loss = 0.61446108\n",
      "Iteration 37, loss = 0.60508978\n",
      "Iteration 38, loss = 0.59595180\n",
      "Iteration 39, loss = 0.58704171\n",
      "Iteration 40, loss = 0.57835412\n",
      "Iteration 41, loss = 0.56988373\n",
      "Iteration 42, loss = 0.56162526\n",
      "Iteration 43, loss = 0.55357353\n",
      "Iteration 44, loss = 0.54572343\n",
      "Iteration 45, loss = 0.53806990\n",
      "Iteration 46, loss = 0.53060799\n",
      "Iteration 47, loss = 0.52333280\n",
      "Iteration 48, loss = 0.51623954\n",
      "Iteration 49, loss = 0.50932350\n",
      "Iteration 50, loss = 0.50258004\n",
      "Iteration 51, loss = 0.49600463\n",
      "Iteration 52, loss = 0.48959282\n",
      "Iteration 53, loss = 0.48334025\n",
      "Iteration 54, loss = 0.47724266\n",
      "Iteration 55, loss = 0.47129587\n",
      "Iteration 56, loss = 0.46549582\n",
      "Iteration 57, loss = 0.45983850\n",
      "Iteration 58, loss = 0.45432003\n",
      "Iteration 59, loss = 0.44893660\n",
      "Iteration 60, loss = 0.44368450\n",
      "Iteration 61, loss = 0.43856012\n",
      "Iteration 62, loss = 0.43355992\n",
      "Iteration 63, loss = 0.42868047\n",
      "Iteration 64, loss = 0.42391842\n",
      "Iteration 65, loss = 0.41927049\n",
      "Iteration 66, loss = 0.41473352\n",
      "Iteration 67, loss = 0.41030441\n",
      "Iteration 68, loss = 0.40598016\n",
      "Iteration 69, loss = 0.40175783\n",
      "Iteration 70, loss = 0.39763457\n",
      "Iteration 71, loss = 0.39360763\n",
      "Iteration 72, loss = 0.38967432\n",
      "Iteration 73, loss = 0.38583201\n",
      "Iteration 74, loss = 0.38207817\n",
      "Iteration 75, loss = 0.37841033\n",
      "Iteration 76, loss = 0.37482610\n",
      "Iteration 77, loss = 0.37132315\n",
      "Iteration 78, loss = 0.36789922\n",
      "Iteration 79, loss = 0.36455212\n",
      "Iteration 80, loss = 0.36127972\n",
      "Iteration 81, loss = 0.35807996\n",
      "Iteration 82, loss = 0.35495084\n",
      "Iteration 83, loss = 0.35189040\n",
      "Iteration 84, loss = 0.34889677\n",
      "Iteration 85, loss = 0.34596811\n",
      "Iteration 86, loss = 0.34310264\n",
      "Iteration 87, loss = 0.34029866\n",
      "Iteration 88, loss = 0.33755448\n",
      "Iteration 89, loss = 0.33486848\n",
      "Iteration 90, loss = 0.33223910\n",
      "Iteration 91, loss = 0.32966481\n",
      "Iteration 92, loss = 0.32714414\n",
      "Iteration 93, loss = 0.32467564\n",
      "Iteration 94, loss = 0.32225794\n",
      "Iteration 95, loss = 0.31988968\n",
      "Iteration 96, loss = 0.31756956\n",
      "Iteration 97, loss = 0.31529630\n",
      "Iteration 98, loss = 0.31306869\n",
      "Iteration 99, loss = 0.31088552\n",
      "Iteration 100, loss = 0.30874564\n",
      "Iteration 101, loss = 0.30664794\n",
      "Iteration 102, loss = 0.30459131\n",
      "Iteration 103, loss = 0.30257471\n",
      "Iteration 104, loss = 0.30059711\n",
      "Iteration 105, loss = 0.29865752\n",
      "Iteration 106, loss = 0.29675498\n",
      "Iteration 107, loss = 0.29488855\n",
      "Iteration 108, loss = 0.29305732\n",
      "Iteration 109, loss = 0.29126041\n",
      "Iteration 110, loss = 0.28949698\n",
      "Iteration 111, loss = 0.28776618\n",
      "Iteration 112, loss = 0.28606722\n",
      "Iteration 113, loss = 0.28439932\n",
      "Iteration 114, loss = 0.28276171\n",
      "Iteration 115, loss = 0.28115367\n",
      "Iteration 116, loss = 0.27957447\n",
      "Iteration 117, loss = 0.27802342\n",
      "Iteration 118, loss = 0.27649985\n",
      "Iteration 119, loss = 0.27500310\n",
      "Iteration 120, loss = 0.27353255\n",
      "Iteration 121, loss = 0.27208756\n",
      "Iteration 122, loss = 0.27066755\n",
      "Iteration 123, loss = 0.26927192\n",
      "Iteration 124, loss = 0.26790012\n",
      "Iteration 125, loss = 0.26655159\n",
      "Iteration 126, loss = 0.26522579\n",
      "Iteration 127, loss = 0.26392221\n",
      "Iteration 128, loss = 0.26264035\n",
      "Iteration 129, loss = 0.26137971\n",
      "Iteration 130, loss = 0.26013980\n",
      "Iteration 131, loss = 0.25892018\n",
      "Iteration 132, loss = 0.25772039\n",
      "Iteration 133, loss = 0.25653998\n",
      "Iteration 134, loss = 0.25537853\n",
      "Iteration 135, loss = 0.25423563\n",
      "Iteration 136, loss = 0.25311086\n",
      "Iteration 137, loss = 0.25200385\n",
      "Iteration 138, loss = 0.25091419\n",
      "Iteration 139, loss = 0.24984153\n",
      "Iteration 140, loss = 0.24878549\n",
      "Iteration 141, loss = 0.24774573\n",
      "Iteration 142, loss = 0.24672189\n",
      "Iteration 143, loss = 0.24571366\n",
      "Iteration 144, loss = 0.24472069\n",
      "Iteration 145, loss = 0.24374267\n",
      "Iteration 146, loss = 0.24277929\n",
      "Iteration 147, loss = 0.24183025\n",
      "Iteration 148, loss = 0.24089526\n",
      "Iteration 149, loss = 0.23997403\n",
      "Iteration 150, loss = 0.23906627\n",
      "Iteration 151, loss = 0.23817173\n",
      "Iteration 152, loss = 0.23729013\n",
      "Iteration 153, loss = 0.23642122\n",
      "Iteration 154, loss = 0.23556474\n",
      "Iteration 155, loss = 0.23472044\n",
      "Iteration 156, loss = 0.23388810\n",
      "Iteration 157, loss = 0.23306747\n",
      "Iteration 158, loss = 0.23225833\n",
      "Iteration 159, loss = 0.23146045\n",
      "Iteration 160, loss = 0.23067361\n",
      "Iteration 161, loss = 0.22989762\n",
      "Iteration 162, loss = 0.22913225\n",
      "Iteration 163, loss = 0.22837731\n",
      "Iteration 164, loss = 0.22763260\n",
      "Iteration 165, loss = 0.22689793\n",
      "Iteration 166, loss = 0.22617310\n",
      "Iteration 167, loss = 0.22545795\n",
      "Iteration 168, loss = 0.22475229\n",
      "Iteration 169, loss = 0.22405594\n",
      "Iteration 170, loss = 0.22336873\n",
      "Iteration 171, loss = 0.22269050\n",
      "Iteration 172, loss = 0.22202109\n",
      "Iteration 173, loss = 0.22136033\n",
      "Iteration 174, loss = 0.22070807\n",
      "Iteration 175, loss = 0.22006416\n",
      "Iteration 176, loss = 0.21942844\n",
      "Iteration 177, loss = 0.21880079\n",
      "Iteration 178, loss = 0.21818104\n",
      "Iteration 179, loss = 0.21756907\n",
      "Iteration 180, loss = 0.21696473\n",
      "Iteration 181, loss = 0.21636791\n",
      "Iteration 182, loss = 0.21577846\n",
      "Iteration 183, loss = 0.21519625\n",
      "Iteration 184, loss = 0.21462118\n",
      "Iteration 185, loss = 0.21405310\n",
      "Iteration 186, loss = 0.21349191\n",
      "Iteration 187, loss = 0.21293749\n",
      "Iteration 188, loss = 0.21238973\n",
      "Iteration 189, loss = 0.21184851\n",
      "Iteration 190, loss = 0.21131372\n",
      "Iteration 191, loss = 0.21078527\n",
      "Iteration 192, loss = 0.21026303\n",
      "Iteration 193, loss = 0.20974692\n",
      "Iteration 194, loss = 0.20923684\n",
      "Iteration 195, loss = 0.20873267\n",
      "Iteration 196, loss = 0.20823434\n",
      "Iteration 197, loss = 0.20774174\n",
      "Iteration 198, loss = 0.20725478\n",
      "Iteration 199, loss = 0.20677337\n",
      "Iteration 200, loss = 0.20629743\n",
      "Iteration 201, loss = 0.20582687\n",
      "Iteration 202, loss = 0.20536160\n",
      "Iteration 203, loss = 0.20490154\n",
      "Iteration 204, loss = 0.20444660\n",
      "Iteration 205, loss = 0.20399672\n",
      "Iteration 206, loss = 0.20355181\n",
      "Iteration 207, loss = 0.20311179\n",
      "Iteration 208, loss = 0.20267659\n",
      "Iteration 209, loss = 0.20224614\n",
      "Iteration 210, loss = 0.20182036\n",
      "Iteration 211, loss = 0.20139918\n",
      "Iteration 212, loss = 0.20098253\n",
      "Iteration 213, loss = 0.20057035\n",
      "Iteration 214, loss = 0.20016257\n",
      "Iteration 215, loss = 0.19975912\n",
      "Iteration 216, loss = 0.19935994\n",
      "Iteration 217, loss = 0.19896496\n",
      "Iteration 218, loss = 0.19857413\n",
      "Iteration 219, loss = 0.19818738\n",
      "Iteration 220, loss = 0.19780466\n",
      "Iteration 221, loss = 0.19742590\n",
      "Iteration 222, loss = 0.19705104\n",
      "Iteration 223, loss = 0.19668004\n",
      "Iteration 224, loss = 0.19631284\n",
      "Iteration 225, loss = 0.19594937\n",
      "Iteration 226, loss = 0.19558960\n",
      "Iteration 227, loss = 0.19523346\n",
      "Iteration 228, loss = 0.19488090\n",
      "Iteration 229, loss = 0.19453188\n",
      "Iteration 230, loss = 0.19418635\n",
      "Iteration 231, loss = 0.19384425\n",
      "Iteration 232, loss = 0.19350554\n",
      "Iteration 233, loss = 0.19317017\n",
      "Iteration 234, loss = 0.19283810\n",
      "Iteration 235, loss = 0.19250927\n",
      "Iteration 236, loss = 0.19218366\n",
      "Iteration 237, loss = 0.19186120\n",
      "Iteration 238, loss = 0.19154187\n",
      "Iteration 239, loss = 0.19122561\n",
      "Iteration 240, loss = 0.19091239\n",
      "Iteration 241, loss = 0.19060216\n",
      "Iteration 242, loss = 0.19029489\n",
      "Iteration 243, loss = 0.18999053\n",
      "Iteration 244, loss = 0.18968905\n",
      "Iteration 245, loss = 0.18939041\n",
      "Iteration 246, loss = 0.18909457\n",
      "Iteration 247, loss = 0.18880150\n",
      "Iteration 248, loss = 0.18851115\n",
      "Iteration 249, loss = 0.18822350\n",
      "Iteration 250, loss = 0.18793850\n",
      "Iteration 251, loss = 0.18765613\n",
      "Iteration 252, loss = 0.18737635\n",
      "Iteration 253, loss = 0.18709913\n",
      "Iteration 254, loss = 0.18682443\n",
      "Iteration 255, loss = 0.18655222\n",
      "Iteration 256, loss = 0.18628247\n",
      "Iteration 257, loss = 0.18601515\n",
      "Iteration 258, loss = 0.18575023\n",
      "Iteration 259, loss = 0.18548767\n",
      "Iteration 260, loss = 0.18522746\n",
      "Iteration 261, loss = 0.18496955\n",
      "Iteration 262, loss = 0.18471393\n",
      "Iteration 263, loss = 0.18446056\n",
      "Iteration 264, loss = 0.18420941\n",
      "Iteration 265, loss = 0.18396046\n",
      "Iteration 266, loss = 0.18371368\n",
      "Iteration 267, loss = 0.18346904\n",
      "Iteration 268, loss = 0.18322652\n",
      "Iteration 269, loss = 0.18298610\n",
      "Iteration 270, loss = 0.18274774\n",
      "Iteration 271, loss = 0.18251142\n",
      "Iteration 272, loss = 0.18227712\n",
      "Iteration 273, loss = 0.18204481\n",
      "Iteration 274, loss = 0.18181447\n",
      "Iteration 275, loss = 0.18158607\n",
      "Iteration 276, loss = 0.18135960\n",
      "Iteration 277, loss = 0.18113503\n",
      "Iteration 278, loss = 0.18091234\n",
      "Iteration 279, loss = 0.18069150\n",
      "Iteration 280, loss = 0.18047250\n",
      "Iteration 281, loss = 0.18025530\n",
      "Iteration 282, loss = 0.18003990\n",
      "Iteration 283, loss = 0.17982627\n",
      "Iteration 284, loss = 0.17961439\n",
      "Iteration 285, loss = 0.17940424\n",
      "Iteration 286, loss = 0.17919580\n",
      "Iteration 287, loss = 0.17898905\n",
      "Iteration 288, loss = 0.17878397\n",
      "Iteration 289, loss = 0.17858055\n",
      "Iteration 290, loss = 0.17837875\n",
      "Iteration 291, loss = 0.17817857\n",
      "Iteration 292, loss = 0.17797999\n",
      "Iteration 293, loss = 0.17778299\n",
      "Iteration 294, loss = 0.17758755\n",
      "Iteration 295, loss = 0.17739365\n",
      "Iteration 296, loss = 0.17720127\n",
      "Iteration 297, loss = 0.17701041\n",
      "Iteration 298, loss = 0.17682104\n",
      "Iteration 299, loss = 0.17663314\n",
      "Iteration 300, loss = 0.17644671\n",
      "Iteration 301, loss = 0.17626172\n",
      "Iteration 302, loss = 0.17607816\n",
      "Iteration 303, loss = 0.17589601\n",
      "Iteration 304, loss = 0.17571525\n",
      "Iteration 305, loss = 0.17553588\n",
      "Iteration 306, loss = 0.17535788\n",
      "Iteration 307, loss = 0.17518122\n",
      "Iteration 308, loss = 0.17500591\n",
      "Iteration 309, loss = 0.17483192\n",
      "Iteration 310, loss = 0.17465924\n",
      "Iteration 311, loss = 0.17448785\n",
      "Iteration 312, loss = 0.17431774\n",
      "Iteration 313, loss = 0.17414891\n",
      "Iteration 314, loss = 0.17398132\n",
      "Iteration 315, loss = 0.17381498\n",
      "Iteration 316, loss = 0.17364987\n",
      "Iteration 317, loss = 0.17348597\n",
      "Iteration 318, loss = 0.17332327\n",
      "Iteration 319, loss = 0.17316177\n",
      "Iteration 320, loss = 0.17300144\n",
      "Iteration 321, loss = 0.17284227\n",
      "Iteration 322, loss = 0.17268426\n",
      "Iteration 323, loss = 0.17252739\n",
      "Iteration 324, loss = 0.17237165\n",
      "Iteration 325, loss = 0.17221703\n",
      "Iteration 326, loss = 0.17206352\n",
      "Iteration 327, loss = 0.17191110\n",
      "Iteration 328, loss = 0.17175976\n",
      "Iteration 329, loss = 0.17160950\n",
      "Iteration 330, loss = 0.17146030\n",
      "Iteration 331, loss = 0.17131215\n",
      "Iteration 332, loss = 0.17116504\n",
      "Iteration 333, loss = 0.17101896\n",
      "Iteration 334, loss = 0.17087390\n",
      "Iteration 335, loss = 0.17072985\n",
      "Iteration 336, loss = 0.17058680\n",
      "Iteration 337, loss = 0.17044475\n",
      "Iteration 338, loss = 0.17030367\n",
      "Iteration 339, loss = 0.17016356\n",
      "Iteration 340, loss = 0.17002441\n",
      "Iteration 341, loss = 0.16988621\n",
      "Iteration 342, loss = 0.16974895\n",
      "Iteration 343, loss = 0.16961263\n",
      "Iteration 344, loss = 0.16947723\n",
      "Iteration 345, loss = 0.16934274\n",
      "Iteration 346, loss = 0.16920916\n",
      "Iteration 347, loss = 0.16907647\n",
      "Iteration 348, loss = 0.16894468\n",
      "Iteration 349, loss = 0.16881376\n",
      "Iteration 350, loss = 0.16868371\n",
      "Iteration 351, loss = 0.16855453\n",
      "Iteration 352, loss = 0.16842620\n",
      "Iteration 353, loss = 0.16829871\n",
      "Iteration 354, loss = 0.16817206\n",
      "Iteration 355, loss = 0.16804625\n",
      "Iteration 356, loss = 0.16792125\n",
      "Iteration 357, loss = 0.16779707\n",
      "Iteration 358, loss = 0.16767369\n",
      "Iteration 359, loss = 0.16755112\n",
      "Iteration 360, loss = 0.16742933\n",
      "Iteration 361, loss = 0.16730833\n",
      "Iteration 362, loss = 0.16718810\n",
      "Iteration 363, loss = 0.16706864\n",
      "Iteration 364, loss = 0.16694995\n",
      "Iteration 365, loss = 0.16683201\n",
      "Iteration 366, loss = 0.16671481\n",
      "Iteration 367, loss = 0.16659836\n",
      "Iteration 368, loss = 0.16648264\n",
      "Iteration 369, loss = 0.16636765\n",
      "Iteration 370, loss = 0.16625337\n",
      "Iteration 371, loss = 0.16613982\n",
      "Iteration 372, loss = 0.16602696\n",
      "Iteration 373, loss = 0.16591481\n",
      "Iteration 374, loss = 0.16580335\n",
      "Iteration 375, loss = 0.16569258\n",
      "Iteration 376, loss = 0.16558249\n",
      "Iteration 377, loss = 0.16547308\n",
      "Iteration 378, loss = 0.16536434\n",
      "Iteration 379, loss = 0.16525625\n",
      "Iteration 380, loss = 0.16514883\n",
      "Iteration 381, loss = 0.16504206\n",
      "Iteration 382, loss = 0.16493593\n",
      "Iteration 383, loss = 0.16483044\n",
      "Iteration 384, loss = 0.16472558\n",
      "Iteration 385, loss = 0.16462135\n",
      "Iteration 386, loss = 0.16451774\n",
      "Iteration 387, loss = 0.16441475\n",
      "Iteration 388, loss = 0.16431237\n",
      "Iteration 389, loss = 0.16421060\n",
      "Iteration 390, loss = 0.16410943\n",
      "Iteration 391, loss = 0.16400885\n",
      "Iteration 392, loss = 0.16390886\n",
      "Iteration 393, loss = 0.16380945\n",
      "Iteration 394, loss = 0.16371063\n",
      "Iteration 395, loss = 0.16361238\n",
      "Iteration 396, loss = 0.16351469\n",
      "Iteration 397, loss = 0.16341757\n",
      "Iteration 398, loss = 0.16332101\n",
      "Iteration 399, loss = 0.16322501\n",
      "Iteration 400, loss = 0.16312955\n",
      "Iteration 401, loss = 0.16303464\n",
      "Iteration 402, loss = 0.16294026\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0, batch_size=10000,\n",
       "              hidden_layer_sizes=(4, 4), learning_rate_init=0.01, max_iter=2000,\n",
       "              momentum=0, shuffle=False, solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Y.shape[1]\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,4),activation='tanh',solver='sgd',alpha=0,learning_rate_init=0.01,max_iter=2000,batch_size=m,shuffle=False,momentum=0,verbose=True)\n",
    "mlp.fit(X.T,Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights and biases\n",
      "W1: [[-0.71463019 -0.57655304 -0.06590277]\n",
      " [ 0.02376012  0.08793267 -0.022375  ]\n",
      " [-0.24629548 -0.09607573 -0.1657701 ]\n",
      " [-0.46148914  0.18801803  0.19881984]]\n",
      "b1: [[-1.13916944]\n",
      " [-0.23047559]\n",
      " [ 0.4056156 ]\n",
      " [-1.17710392]]\n",
      "W2: [[-0.45038575 -0.18968503 -0.01368623 -0.30938031]\n",
      " [-0.59923418 -0.93279314 -0.14422182 -0.93799234]\n",
      " [-0.55140271 -0.14558229  0.07554677  0.3972282 ]\n",
      " [-0.47213049  0.65578757  0.0098236  -1.21240234]]\n",
      "b2: [[ 0.92865696]\n",
      " [-0.10587238]\n",
      " [-0.26617069]\n",
      " [-0.17628227]]\n",
      "W3: [[-1.12575363 -1.14635024 -0.1397987  -1.24219472]]\n",
      "b3: [[0.45849985]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",mlp.coefs_[0].T)\n",
    "print(\"b1:\",mlp.intercepts_[0].reshape(-1,1))\n",
    "print(\"W2:\",mlp.coefs_[1].T)\n",
    "print(\"b2:\",mlp.intercepts_[1].reshape(-1,1))\n",
    "print(\"W3:\",mlp.coefs_[2].T)\n",
    "print(\"b3:\",mlp.intercepts_[2].reshape(-1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training a Deep Neural Network Using Backpropagation\n",
    "The following code implements backpropagation to train a deep neural network of 2 hidden layers with 4 neurons in each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the paramaters of the neural network\n",
    "W = [np.random.rand(4,3),np.random.rand(4,4),np.random.rand(1,4)]\n",
    "b = [np.zeros((4,1)),np.zeros((4,1)),np.zeros((1,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last iteration: 402\n",
      "Losses: [0.18791801 0.187687   0.18745753 0.18722959 0.18700315 0.18677822\n",
      " 0.18655477 0.1863328  0.18611229 0.18589323]\n",
      "\n",
      "Weights and biases\n",
      "W1: [[ 0.18350402  0.06106043  0.16089564]\n",
      " [ 0.14008441  0.00853907  0.90328437]\n",
      " [-0.07998509 -0.0903867   0.07160842]\n",
      " [-0.01942756  0.36761766  0.66993764]]\n",
      "b1: [[-0.42209151]\n",
      " [-0.15729594]\n",
      " [-0.525868  ]\n",
      " [-0.25821328]]\n",
      "W2: [[ 0.60674666  0.48584253  0.8893756   0.82787761]\n",
      " [ 0.03799913  0.0919417   0.72888361 -0.03253264]\n",
      " [ 0.6760222  -0.04103128  0.19930486  0.08545966]\n",
      " [ 0.53991338  0.61439598  0.99447289  0.19597078]]\n",
      "b2: [[-0.34006464]\n",
      " [-0.31962694]\n",
      " [-0.37682823]\n",
      " [ 0.00467849]]\n",
      "W3: [[0.81152421 0.57336916 0.6764238  0.04450193]]\n",
      "b3: [[-0.84748953]]\n"
     ]
    }
   ],
   "source": [
    "# Updating parameters using gradient descent\n",
    "iter = 402\n",
    "lr = 0.01\n",
    "loss = np.arange(10,20)\n",
    "L = 3\n",
    "\n",
    "for i in np.arange(iter):\n",
    "    # Forward propagation\n",
    "    Z = []\n",
    "    A = []\n",
    "    for l in np.arange(L):\n",
    "        Z.append(W[l]@A[l-1]+b[l] if l> 0 else W[l]@X+b[l])\n",
    "        A.append(1/(1+np.exp(-Z[l])) if l==L-1 else np.tanh(Z[l]))\n",
    "\n",
    "    # Back propagation\n",
    "    dZ = [0]*L\n",
    "    dA = [0]*L\n",
    "    dW = [0]*L\n",
    "    db = [0]*L\n",
    "    \n",
    "    for l in np.arange(L-1,-1,-1):\n",
    "        dZ[l] = A[l]-Y if l==L-1 else dA[l]*(1-np.tanh(Z[l])**2)\n",
    "        dA[l-1] = W[l].T@dZ[l] if l> 0 else 0\n",
    "        dW[l] = 1/m*dZ[l]@A[l-1].T\n",
    "        db[l] = 1/m*np.sum(dZ[l],axis=1,keepdims=True)\n",
    "        W[l] -= lr*dW[l]\n",
    "        b[l] -= lr*db[l]\n",
    "\n",
    "    current_loss = -1/m*(Y@np.log(A[L-1]).T+(1-Y)@np.log(1-A[L-1]).T)\n",
    "    loss = np.append(loss,current_loss)\n",
    "    loss = np.delete(loss,0)\n",
    "\n",
    "print(\"Last iteration:\",i+1)\n",
    "print(\"Losses:\",loss)\n",
    "\n",
    "print(\"\\nWeights and biases\")\n",
    "print(\"W1:\",W[0])\n",
    "print(\"b1:\",b[0])\n",
    "print(\"W2:\",W[1])\n",
    "print(\"b2:\",b[1])\n",
    "print(\"W3:\",W[2])\n",
    "print(\"b3:\",b[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusion\n",
    "Similar values of loss from 2 & 3 for the same number of iterations indicates that the custom gradient descent implementation is correct. The weights and biases are different because the 2 models are randomly initialised during training and the loss function of the shallow neural network has multiple maximia and minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
